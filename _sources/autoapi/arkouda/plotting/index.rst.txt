arkouda.plotting
================

.. py:module:: arkouda.plotting


Classes
-------

.. autoapisummary::

   arkouda.plotting.DataFrame
   arkouda.plotting.Datetime
   arkouda.plotting.GroupBy
   arkouda.plotting.Timedelta


Functions
---------

.. autoapisummary::

   arkouda.plotting.arange
   arkouda.plotting.date_range
   arkouda.plotting.hist_all
   arkouda.plotting.histogram
   arkouda.plotting.isnan
   arkouda.plotting.plot_dist
   arkouda.plotting.skew
   arkouda.plotting.timedelta_range


Module Contents
---------------

.. py:class:: DataFrame(dict=None, /, **kwargs)

   Bases: :py:obj:`collections.UserDict`


   A DataFrame structure based on arkouda arrays.

   :param initialdata: Each list/dictionary entry corresponds to one column of the data and
                       should be a homogenous type. Different columns may have different
                       types. If using a dictionary, keys should be strings.
   :type initialdata: List or dictionary of lists, tuples, or pdarrays
   :param index: Index for the resulting frame. Defaults to an integer range.
   :type index: Index, pdarray, or Strings
   :param columns: Column labels to use if the data does not include them. Elements must
                   be strings. Defaults to an stringified integer range.
   :type columns: List, tuple, pdarray, or Strings

   .. rubric:: Examples

   Create an empty DataFrame and add a column of data:

   >>> import arkouda as ak
   >>> ak.connect()
   >>> df = ak.DataFrame()
   >>> df['a'] = ak.array([1,2,3])
   >>> display(df)

   +----+-----+
   |    |   a |
   +====+=====+
   |  0 |   1 |
   +----+-----+
   |  1 |   2 |
   +----+-----+
   |  2 |   3 |
   +----+-----+

   Create a new DataFrame using a dictionary of data:

   >>> userName = ak.array(['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice'])
   >>> userID = ak.array([111, 222, 111, 333, 222, 111])
   >>> item = ak.array([0, 0, 1, 1, 2, 0])
   >>> day = ak.array([5, 5, 6, 5, 6, 6])
   >>> amount = ak.array([0.5, 0.6, 1.1, 1.2, 4.3, 0.6])
   >>> df = ak.DataFrame({'userName': userName, 'userID': userID,
   >>>            'item': item, 'day': day, 'amount': amount})
   >>> display(df)

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Alice      |      111 |      0 |     5 |      0.5 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  3 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  4 | Bob        |      222 |      2 |     6 |      4.3 |
   +----+------------+----------+--------+-------+----------+
   |  5 | Alice      |      111 |      0 |     6 |      0.6 |
   +----+------------+----------+--------+-------+----------+

   Indexing works slightly differently than with pandas:

   >>> df[0]

   +------------+----------+
   | keys       |   values |
   +============+==========+
   | userName   |    Alice |
   +------------+----------+
   |userID      |      111 |
   +------------+----------+
   | item       |      0   |
   +------------+----------+
   | day        |      5   |
   +------------+----------+
   | amount     |     0.5  |
   +------------+----------+

   >>> df['userID']
   array([111, 222, 111, 333, 222, 111])

   >>> df['userName']
   array(['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice'])

   >>> df[ak.array([1,3,5])]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Alice      |      111 |      0 |     6 |      0.6 |
   +----+------------+----------+--------+-------+----------+

   Compute the stride:

   >>> df[1:5:1]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  3 | Bob        |      222 |      2 |     6 |      4.3 |
   +----+------------+----------+--------+-------+----------+

   >>> df[ak.array([1,2,3])]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+

   >>> df[['userID', 'day']]

   +----+----------+-------+
   |    |   userID |   day |
   +====+==========+=======+
   |  0 |      111 |     5 |
   +----+----------+-------+
   |  1 |      222 |     5 |
   +----+----------+-------+
   |  2 |      111 |     6 |
   +----+----------+-------+
   |  3 |      333 |     5 |
   +----+----------+-------+
   |  4 |      222 |     6 |
   +----+----------+-------+
   |  5 |      111 |     6 |
   +----+----------+-------+


   .. py:method:: GroupBy(keys, use_series=False, as_index=True, dropna=True)

      Group the dataframe by a column or a list of columns.

      :param keys: An (ordered) list of column names or a single string to group by.
      :type keys: str or list of str
      :param use_series: If True, returns an arkouda.dataframe.DataFrameGroupBy object.
                         Otherwise an arkouda.groupbyclass.GroupBy object.
      :type use_series: bool, default=False
      :param as_index: If True, groupby columns will be set as index
                       otherwise, the groupby columns will be treated as DataFrame columns.
      :type as_index: bool, default=True
      :param dropna: If True, and the groupby keys contain NaN values,
                     the NaN values together with the corresponding row will be dropped.
                     Otherwise, the rows corresponding to NaN values will be kept.
      :type dropna: bool, default=True

      :returns: If use_series = True, returns an arkouda.dataframe.DataFrameGroupBy object.
                Otherwise returns an arkouda.groupbyclass.GroupBy object.
      :rtype: arkouda.dataframe.DataFrameGroupBy or arkouda.groupbyclass.GroupBy

      .. seealso:: :obj:`arkouda.GroupBy`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.0, 1.0, 2.0, np.nan], 'col2': [4, 5, 6, 7]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      1 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |    nan |      7 |
      +----+--------+--------+

      >>> df.GroupBy("col1")
      <arkouda.groupbyclass.GroupBy at 0x7f2cf23e10c0>
      >>> df.GroupBy("col1").size()
      (array([1.00000000000000000 2.00000000000000000]), array([2 1]))

      >>> df.GroupBy("col1",use_series=True)
      col1
      1.0    2
      2.0    1
      dtype: int64
      >>> df.GroupBy("col1",use_series=True, as_index = False).size()

      +----+--------+--------+
      |    |   col1 |   size |
      +====+========+========+
      |  0 |      1 |      2 |
      +----+--------+--------+
      |  1 |      2 |      1 |
      +----+--------+--------+



   .. py:method:: all(axis=0) -> Union[Series, bool]

      Return whether all elements are True, potentially over an axis.

      Returns True unless there at least one element along a Dataframe axis that is False.

      Currently, will ignore any columns that are not type bool.
      This is equivalent to the pandas option bool_only=True.

      :param axis: Indicate which axis or axes should be reduced.

                   0 / ‘index’ : reduce the index, return a Series whose index is the original column labels.

                   1 / ‘columns’ : reduce the columns, return a Series whose index is the original index.

                   None : reduce all axes, return a scalar.
      :type axis: {0 or ‘index’, 1 or ‘columns’, None}, default = 0

      :rtype: arkouda.series.Series or bool

      :raises ValueError: Raised if axis does not have a value in {0 or ‘index’, 1 or ‘columns’, None}.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[True,True,True,False],"B":[True,True,True,False],
      ...          "C":[True,False,True,False],"D":[True,True,True,True]})

      +----+---------+---------+---------+--------+
      |    |   A     |   B     |   C     |   D    |
      +====+=========+=========+=========+========+
      |  0 |   True  |   True  |   True  |   True |
      +----+---------+---------+---------+--------+
      |  1 |   True  |   True  |   False |   True |
      +----+---------+---------+---------+--------+
      |  2 |   True  |   True  |   True  |   True |
      +----+---------+---------+---------+--------+
      |  3 |   False |   False |   False |   True |
      +----+---------+---------+---------+--------+

      >>> df.all(axis=0)
      A    False
      B    False
      C    False
      D     True
      dtype: bool
      >>> df.all(axis=1)
      0     True
      1    False
      2     True
      3    False
      dtype: bool
      >>> df.all(axis=None)
      False



   .. py:method:: any(axis=0) -> Union[Series, bool]

      Return whether any element is True, potentially over an axis.

      Returns False unless there is at least one element along a Dataframe axis that is True.

      Currently, will ignore any columns that are not type bool.
      This is equivalent to the pandas option bool_only=True.

      :param axis: Indicate which axis or axes should be reduced.

                   0 / ‘index’ : reduce the index, return a Series whose index is the original column labels.

                   1 / ‘columns’ : reduce the columns, return a Series whose index is the original index.

                   None : reduce all axes, return a scalar.
      :type axis: {0 or ‘index’, 1 or ‘columns’, None}, default = 0

      :rtype: arkouda.series.Series or bool

      :raises ValueError: Raised if axis does not have a value in {0 or ‘index’, 1 or ‘columns’, None}.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[True,True,True,False],"B":[True,True,True,False],
      ...          "C":[True,False,True,False],"D":[False,False,False,False]})

      +----+---------+---------+---------+---------+
      |    |   A     |   B     |   C     |   D     |
      +====+=========+=========+=========+=========+
      |  0 |   True  |   True  |   True  |   False |
      +----+---------+---------+---------+---------+
      |  1 |   True  |   True  |   False |   False |
      +----+---------+---------+---------+---------+
      |  2 |   True  |   True  |   True  |   False |
      +----+---------+---------+---------+---------+
      |  3 |   False |   False |   False |   False |
      +----+---------+---------+---------+---------+

      >>> df.any(axis=0)
      A     True
      B     True
      C     True
      D    False
      dtype: bool
      >>> df.any(axis=1)
      0     True
      1     True
      2     True
      3    False
      dtype: bool
      >>> df.any(axis=None)
      True



   .. py:method:: append(other, ordered=True)

      Concatenate data from 'other' onto the end of this DataFrame, in place.

      Explicitly, use the arkouda concatenate function to append the data
      from each column in other to the end of self. This operation is done
      in place, in the sense that the underlying pdarrays are updated from
      the result of the arkouda concatenate function, rather than returning
      a new DataFrame object containing the result.

      :param other: The DataFrame object whose data will be appended to this DataFrame.
      :type other: DataFrame
      :param ordered: If False, allow rows to be interleaved for better performance (but
                      data within a row remains together). By default, append all rows
                      to the end, in input order.
      :type ordered: bool, default=True

      :returns: Appending occurs in-place, but result is returned for compatibility.
      :rtype: self

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df1 = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df2 = ak.DataFrame({'col1': [3], 'col2': [5]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      3 |      5 |
      +----+--------+--------+

      >>> df1.append(df2)
      >>> df1

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+
      |  2 |      3 |      5 |
      +----+--------+--------+



   .. py:method:: apply_permutation(perm)

      Apply a permutation to an entire DataFrame.  The operation is done in
      place and the original DataFrame will be modified.

      This may be useful if you want to unsort an DataFrame, or even to
      apply an arbitrary permutation such as the inverse of a sorting
      permutation.

      :param perm: A permutation array. Should be the same size as the data
                   arrays, and should consist of the integers [0,size-1] in
                   some order. Very minimal testing is done to ensure this
                   is a permutation.
      :type perm: pdarray

      :rtype: None

      .. seealso:: :obj:`sort`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> perm_arry = ak.array([0, 2, 1])
      >>> df.apply_permutation(perm_arry)
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      3 |      6 |
      +----+--------+--------+
      |  2 |      2 |      5 |
      +----+--------+--------+



   .. py:method:: argsort(key, ascending=True)

      Return the permutation that sorts the dataframe by `key`.

      :param key: The key to sort on.
      :type key: str
      :param ascending: If true, sort the key in ascending order.
                        Otherwise, sort the key in descending order.
      :type ascending: bool, default = True

      :returns: The permutation array that sorts the data on `key`.
      :rtype: arkouda.pdarrayclass.pdarray

      .. seealso:: :obj:`coargsort`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.1, 3.1, 2.1], 'col2': [6, 5, 4]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    2.1 |      4 |
      +----+--------+--------+

      >>> df.argsort('col1')
      array([0 2 1])
      >>> sorted_df1 = df[df.argsort('col1')]
      >>> display(sorted_df1)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    2.1 |      4 |
      +----+--------+--------+
      |  2 |    3.1 |      5 |
      +----+--------+--------+

      >>> df.argsort('col2')
      array([2 1 0])
      >>> sorted_df2 = df[df.argsort('col2')]
      >>> display(sorted_df2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    2.1 |      4 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    1.1 |      6 |
      +----+--------+--------+



   .. py:method:: attach(user_defined_name: str) -> DataFrame

      Function to return a DataFrame object attached to the registered name in the
      arkouda server which was registered using register().

      :param user_defined_name: user defined name which DataFrame object was registered under.
      :type user_defined_name: str

      :returns: The DataFrame object created by re-attaching to the corresponding server components.
      :rtype: arkouda.dataframe.DataFrame

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: coargsort(keys, ascending=True)

      Return the permutation that sorts the dataframe by `keys`.

      Note: Sorting using Strings may not yield correct sort order.

      :param keys: The keys to sort on.
      :type keys: list of str

      :returns: The permutation array that sorts the data on `keys`.
      :rtype: arkouda.pdarrayclass.pdarray

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [2, 2, 1], 'col2': [3, 4, 3], 'col3':[5, 6, 7]})
      >>> display(df)

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  1 |      2 |      4 |      6 |
      +----+--------+--------+--------+
      |  2 |      1 |      3 |      7 |
      +----+--------+--------+--------+

      >>> df.coargsort(['col1', 'col2'])
      array([2 0 1])
      >>>



   .. py:property:: columns
      An Index where the values are the column names of the dataframe.

      :returns: The values of the index are the column names of the dataframe.
      :rtype: arkouda.index.Index

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df.columns
      Index(array(['col1', 'col2']), dtype='<U0')


   .. py:method:: concat(items, ordered=True)

      Essentially an append, but different formatting.





   .. py:method:: corr() -> DataFrame

      Return new DataFrame with pairwise correlation of columns.

      :returns: Arkouda DataFrame containing correlation matrix of all columns.
      :rtype: arkouda.dataframe.DataFrame

      :raises RuntimeError: Raised if there's a server-side error thrown.

      .. seealso:: :obj:`pdarray.corr`

      .. rubric:: Notes

      Generates the correlation matrix using Pearson R for all columns.

      Attempts to convert to numeric values where possible for inclusion in the matrix.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [-1, -2]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |     -1 |
      +----+--------+--------+
      |  1 |      2 |     -2 |
      +----+--------+--------+

      >>> corr = df.corr()

      +------+--------+--------+
      |      |   col1 |   col2 |
      +======+========+========+
      | col1 |      1 |     -1 |
      +------+--------+--------+
      | col2 |     -1 |      1 |
      +------+--------+--------+



   .. py:method:: count(axis: Union[int, str] = 0, numeric_only=False) -> Series

      Count non-NA cells for each column or row.

      The values np.NaN are considered NA.

      :param axis: If 0 or ‘index’ counts are generated for each column.
                   If 1 or ‘columns’ counts are generated for each row.
      :type axis: {0 or 'index', 1 or 'columns'}, default 0
      :param numeric_only: Include only float, int or boolean data.
      :type numeric_only: bool = False

      :returns: For each column/row the number of non-NA/null entries.
      :rtype: arkouda.series.Series

      :raises ValueError: Raised if axis is not 0, 1, 'index', or 'columns'.

      .. seealso:: :obj:`GroupBy.count`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({'col_A': ak.array([7, np.nan]), 'col_B':ak.array([1, 9])})
      >>> display(df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       7 |       1 |
      +----+---------+---------+
      |  1 |     nan |       9 |
      +----+---------+---------+

      >>> df.count()
      col_A    1
      col_B    2
      dtype: int64

      >>> df = ak.DataFrame({'col_A': ak.array(["a","b","c"]), 'col_B':ak.array([1, np.nan, np.nan])})
      >>> display(df)

      +----+---------+---------+
      |    | col_A   |   col_B |
      +====+=========+=========+
      |  0 | a       |       1 |
      +----+---------+---------+
      |  1 | b       |     nan |
      +----+---------+---------+
      |  2 | c       |     nan |
      +----+---------+---------+

      >>> df.count()
      col_A    3
      col_B    1
      dtype: int64

      >>> df.count(numeric_only=True)
      col_B    1
      dtype: int64

      >>> df.count(axis=1)
      0    2
      1    1
      2    1
      dtype: int64



   .. py:method:: drop(keys: Union[str, int, List[Union[str, int]]], axis: Union[str, int] = 0, inplace: bool = False) -> Union[None, DataFrame]

      Drop column/s or row/s from the dataframe.

      :param keys: The labels to be dropped on the given axis.
      :type keys: str, int or list
      :param axis: The axis on which to drop from. 0/'index' - drop rows, 1/'columns' - drop columns.
      :type axis: int or str
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`
      :rtype: arkouda.dataframe.DataFrame or None

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      Drop column

      >>> df.drop('col1', axis = 1)

      +----+--------+
      |    |   col2 |
      +====+========+
      |  0 |      3 |
      +----+--------+
      |  1 |      4 |
      +----+--------+

      Drop row

      >>> df.drop(0, axis = 0)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      2 |      4 |
      +----+--------+--------+



   .. py:method:: drop_duplicates(subset=None, keep='first')

      Drops duplcated rows and returns resulting DataFrame.

      If a subset of the columns are provided then only one instance of each
      duplicated row will be returned (keep determines which row).

      :param subset: Iterable of column names to use to dedupe.
      :type subset: Iterable
      :param keep: Determines which duplicates (if any) to keep.
      :type keep: {'first', 'last'}, default='first'

      :returns: DataFrame with duplicates removed.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 2, 3], 'col2': [4, 5, 5, 6]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      2 |      5 |
      +----+--------+--------+
      |  3 |      3 |      6 |
      +----+--------+--------+

      >>> df.drop_duplicates()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+



   .. py:method:: dropna(axis: Union[int, str] = 0, how: Optional[str] = None, thresh: Optional[int] = None, ignore_index: bool = False) -> DataFrame

      Remove missing values.

      :param axis: Determine if rows or columns which contain missing values are removed.

                   0, or 'index': Drop rows which contain missing values.

                   1, or 'columns': Drop columns which contain missing value.

                   Only a single axis is allowed.
      :type axis: {0 or 'index', 1 or 'columns'}, default = 0
      :param how: Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.

                  'any': If any NA values are present, drop that row or column.

                  'all': If all values are NA, drop that row or column.
      :type how: {'any', 'all'}, default='any'
      :param thresh: Require that many non - NA values.Cannot be combined with how.
      :type thresh: int, optional
      :param ignore_index: If ``True``, the resulting axis will be labeled 0, 1, …, n - 1.
      :type ignore_index: bool, default ``False``

      :returns: DataFrame with NA entries dropped from it.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame(
          {
              "A": [True, True, True, True],
              "B": [1, np.nan, 2, np.nan],
              "C": [1, 2, 3, np.nan],
              "D": [False, False, False, False],
              "E": [1, 2, 3, 4],
              "F": ["a", "b", "c", "d"],
              "G": [1, 2, 3, 4],
          }
         )

      >>> display(df)

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True | nan |   2 | False |   2 | b   |   2 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  2 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  3 | True | nan | nan | False |   4 | d   |   4 |
      +----+------+-----+-----+-------+-----+-----+-----+

      >>> df.dropna()

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+

      >>> df.dropna(axis=1)

      +----+------+-------+-----+-----+-----+
      |    | A    | D     |   E | F   |   G |
      +====+======+=======+=====+=====+=====+
      |  0 | True | False |   1 | a   |   1 |
      +----+------+-------+-----+-----+-----+
      |  1 | True | False |   2 | b   |   2 |
      +----+------+-------+-----+-----+-----+
      |  2 | True | False |   3 | c   |   3 |
      +----+------+-------+-----+-----+-----+
      |  3 | True | False |   4 | d   |   4 |
      +----+------+-------+-----+-----+-----+

      >>> df.dropna(axis=1, thresh=3)

      +----+------+-----+-------+-----+-----+-----+
      |    | A    |   C | D     |   E | F   |   G |
      +====+======+=====+=======+=====+=====+=====+
      |  0 | True |   1 | False |   1 | a   |   1 |
      +----+------+-----+-------+-----+-----+-----+
      |  1 | True |   2 | False |   2 | b   |   2 |
      +----+------+-----+-------+-----+-----+-----+
      |  2 | True |   3 | False |   3 | c   |   3 |
      +----+------+-----+-------+-----+-----+-----+
      |  3 | True | nan | False |   4 | d   |   4 |
      +----+------+-----+-------+-----+-----+-----+

      >>> df.dropna(axis=1, how="all")

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True | nan |   2 | False |   2 | b   |   2 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  2 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  3 | True | nan | nan | False |   4 | d   |   4 |
      +----+------+-----+-----+-------+-----+-----+-----+



   .. py:property:: dtypes
      :type: DataFrame

      The dtypes of the dataframe.

      :returns: **dtypes** -- The dtypes of the dataframe.
      :rtype: arkouda.row.Row

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': ["a", "b"]})
      >>> df

      +----+--------+--------+
      |    |   col1 | col2   |
      +====+========+========+
      |  0 |      1 | a      |
      +----+--------+--------+
      |  1 |      2 | b      |
      +----+--------+--------+

      >>> df.dtypes

      +----+--------+
      |keys| values |
      +====+========+
      |col1|  int64 |
      +----+--------+
      |col2|    str |
      +----+--------+


   .. py:property:: empty
      :type: DataFrame

      Whether the dataframe is empty.

      :returns: True if the dataframe is empty, otherwise False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({})
      >>> df
       0 rows x 0 columns
      >>> df.empty
      True


   .. py:method:: filter_by_range(keys, low=1, high=None)

      Find all rows where the value count of the items in a given set of
      columns (keys) is within the range [low, high].

      To filter by a specific value, set low == high.

      :param keys: The names of the columns to group by.
      :type keys: str or list of str
      :param low: The lowest value count.
      :type low: int, default=1
      :param high: The highest value count, default to unlimited.
      :type high: int, default=None

      :returns: An array of boolean values for qualified rows in this DataFrame.
      :rtype: arkouda.pdarrayclass.pdarray

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 2, 2, 3, 3], 'col2': [4, 5, 6, 7, 8, 9]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |      2 |      7 |
      +----+--------+--------+
      |  4 |      3 |      8 |
      +----+--------+--------+
      |  5 |      3 |      9 |
      +----+--------+--------+

      >>> df.filter_by_range("col1", low=1, high=2)
      array([True False False False True True])

      >>> filtered_df = df[df.filter_by_range("col1", low=1, high=2)]
      >>> display(filtered_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      3 |      8 |
      +----+--------+--------+
      |  2 |      3 |      9 |
      +----+--------+--------+



   .. py:method:: from_pandas(pd_df)

      Copy the data from a pandas DataFrame into a new arkouda.dataframe.DataFrame.

      :param pd_df: A pandas DataFrame to convert.
      :type pd_df: pandas.DataFrame

      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import pandas as pd
      >>> pd_df = pd.DataFrame({"A":[1,2],"B":[3,4]})
      >>> type(pd_df)
      pandas.core.frame.DataFrame
      >>> display(pd_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+

      >>> ak_df = DataFrame.from_pandas(pd_df)
      >>> type(ak_df)
      arkouda.dataframe.DataFrame
      >>> display(ak_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: from_return_msg(rep_msg)

      Creates a DataFrame object from an arkouda server response message.

      :param rep_msg: Server response message used to create a DataFrame.
      :type rep_msg: string

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: groupby(keys, use_series=True, as_index=True, dropna=True)

      Group the dataframe by a column or a list of columns.  Alias for GroupBy.

      :param keys: An (ordered) list of column names or a single string to group by.
      :type keys: str or list of str
      :param use_series: If True, returns an arkouda.dataframe.DataFrameGroupBy object.
                         Otherwise an arkouda.groupbyclass.GroupBy object.
      :type use_series: bool, default=True
      :param as_index: If True, groupby columns will be set as index
                       otherwise, the groupby columns will be treated as DataFrame columns.
      :type as_index: bool, default=True
      :param dropna: If True, and the groupby keys contain NaN values,
                     the NaN values together with the corresponding row will be dropped.
                     Otherwise, the rows corresponding to NaN values will be kept.
      :type dropna: bool, default=True

      :returns: If use_series = True, returns an arkouda.dataframe.DataFrameGroupBy object.
                Otherwise returns an arkouda.groupbyclass.GroupBy object.
      :rtype: arkouda.dataframe.DataFrameGroupBy or arkouda.groupbyclass.GroupBy

      .. seealso:: :obj:`arkouda.GroupBy`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.0, 1.0, 2.0, np.nan], 'col2': [4, 5, 6, 7]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      1 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |    nan |      7 |
      +----+--------+--------+

      >>> df.GroupBy("col1")
      <arkouda.groupbyclass.GroupBy at 0x7f2cf23e10c0>
      >>> df.GroupBy("col1").size()
      (array([1.00000000000000000 2.00000000000000000]), array([2 1]))

      >>> df.GroupBy("col1",use_series=True)
      col1
      1.0    2
      2.0    1
      dtype: int64
      >>> df.GroupBy("col1",use_series=True, as_index = False).size()

      +----+--------+--------+
      |    |   col1 |   size |
      +====+========+========+
      |  0 |      1 |      2 |
      +----+--------+--------+
      |  1 |      2 |      1 |
      +----+--------+--------+



   .. py:method:: head(n=5)

      Return the first `n` rows.

      This function returns the first `n` rows of the the dataframe. It is
      useful for quickly verifying data, for example, after sorting or
      appending rows.

      :param n: Number of rows to select.
      :type n: int, default = 5

      :returns: The first `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`tail`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(10), 'col2': -1 * ak.arange(10)})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+
      |  5 |      5 |     -5 |
      +----+--------+--------+
      |  6 |      6 |     -6 |
      +----+--------+--------+
      |  7 |      7 |     -7 |
      +----+--------+--------+
      |  8 |      8 |     -8 |
      +----+--------+--------+
      |  9 |      9 |     -9 |
      +----+--------+--------+

      >>> df.head()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+

      >>> df.head(n=2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+



   .. py:property:: index
      The index of the dataframe.

      :returns: The index of the dataframe.
      :rtype: arkouda.index.Index or arkouda.index.MultiIndex

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df.index
      Index(array([0 1]), dtype='int64')


   .. py:property:: info
      Returns a summary string of this dataframe.

      :returns: A summary string of this dataframe.
      :rtype: str

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': ["a", "b"]})
      >>> df

      +----+--------+--------+
      |    |   col1 | col2   |
      +====+========+========+
      |  0 |      1 | a      |
      +----+--------+--------+
      |  1 |      2 | b      |
      +----+--------+--------+

      >>> df.info
      "DataFrame(['col1', 'col2'], 2 rows, 20 B)"


   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry.

      :returns: Indicates if the object is contained in the registry.
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components.

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_dataframe_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: isin(values: Union[pdarray, Dict, Series, DataFrame]) -> DataFrame

      Determine whether each element in the DataFrame is contained in values.

      :param values: The values to check for in DataFrame. Series can only have a single index.
      :type values: pdarray, dict, Series, or DataFrame

      :returns: Arkouda DataFrame of booleans showing whether each element in the DataFrame is
                contained in values.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`ak.Series.isin`

      .. rubric:: Notes

      - Pandas supports values being an iterable type. In arkouda, we replace this with pdarray.
      - Pandas supports ~ operations. Currently, ak.DataFrame does not support this.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col_A': ak.array([7, 3]), 'col_B':ak.array([1, 9])})
      >>> display(df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       7 |       1 |
      +----+---------+---------+
      |  1 |       3 |       9 |
      +----+---------+---------+

      When `values` is a pdarray, check every value in the DataFrame to determine if
      it exists in values.

      >>> df.isin(ak.array([0, 1]))

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       1 |
      +----+---------+---------+
      |  1 |       0 |       0 |
      +----+---------+---------+

      When `values` is a dict, the values in the dict are passed to check the column
      indicated by the key.

      >>> df.isin({'col_A': ak.array([0, 3])})

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       0 |
      +----+---------+---------+
      |  1 |       1 |       0 |
      +----+---------+---------+

      When `values` is a Series, each column is checked if values is present positionally.
      This means that for `True` to be returned, the indexes must be the same.

      >>> i = ak.Index(ak.arange(2))
      >>> s = ak.Series(data=[3, 9], index=i)
      >>> df.isin(s)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       0 |
      +----+---------+---------+
      |  1 |       0 |       1 |
      +----+---------+---------+

      When `values` is a DataFrame, the index and column must match.
      Note that 9 is not found because the column name does not match.

      >>> other_df = ak.DataFrame({'col_A':ak.array([7, 3]), 'col_C':ak.array([0, 9])})
      >>> df.isin(other_df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       1 |       0 |
      +----+---------+---------+
      |  1 |       1 |       0 |
      +----+---------+---------+



   .. py:method:: isna() -> DataFrame

      Detect missing values.

      Return a boolean same-sized object indicating if the values are NA.
      numpy.NaN values get mapped to True values.
      Everything else gets mapped to False values.

      :returns: Mask of bool values for each element in DataFrame
                that indicates whether an element is an NA value.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({"A": [np.nan, 2, 2, 3], "B": [3, np.nan, 5, 6],
      ...          "C": [1, np.nan, 2, np.nan], "D":["a","b","c","d"]})
      >>> display(df)

      +----+-----+-----+-----+-----+
      |    |   A |   B |   C | D   |
      +====+=====+=====+=====+=====+
      |  0 | nan |   3 |   1 | a   |
      +----+-----+-----+-----+-----+
      |  1 |   2 | nan | nan | b   |
      +----+-----+-----+-----+-----+
      |  2 |   2 |   5 |   2 | c   |
      +----+-----+-----+-----+-----+
      |  3 |   3 |   6 | nan | d   |
      +----+-----+-----+-----+-----+

      >>> df.isna()
             A      B      C      D
      0   True  False  False  False
      1  False   True   True  False
      2  False  False  False  False
      3  False  False   True  False (4 rows x 4 columns)



   .. py:method:: load(prefix_path, file_format='INFER')

      Load dataframe from file.
      file_format needed for consistency with other load functions.

      :param prefix_path: The prefix path for the data.
      :type prefix_path: str
      :param file_format:
      :type file_format: string, default = "INFER"

      :returns: A dataframe loaded from the prefix_path.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      To store data in <my_dir>/my_data_LOCALE0000,
      use "<my_dir>/my_data" as the prefix.

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf5_output','my_data')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)
      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> df.save(my_path, file_type="distribute")
      >>> df.load(my_path)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+



   .. py:method:: memory_usage(index=True, unit='B') -> Series

      Return the memory usage of each column in bytes.

      The memory usage can optionally include the contribution of
      the index.

      :param index: Specifies whether to include the memory usage of the DataFrame's
                    index in returned Series. If ``index=True``, the memory usage of
                    the index is the first item in the output.
      :type index: bool, default True
      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: A Series whose index is the original column names and whose values
                is the memory usage of each column in bytes.
      :rtype: Series

      .. seealso:: :obj:`arkouda.pdarrayclass.nbytes`, :obj:`arkouda.index.Index.memory_usage`, :obj:`arkouda.index.MultiIndex.memory_usage`, :obj:`arkouda.series.Series.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> dtypes = [ak.int64, ak.float64,  ak.bool]
      >>> data = dict([(str(t), ak.ones(5000, dtype=ak.int64).astype(t)) for t in dtypes])
      >>> df = ak.DataFrame(data)
      >>> display(df.head())

      +----+---------+-----------+--------+
      |    |   int64 |   float64 | bool   |
      +====+=========+===========+========+
      |  0 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  1 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  2 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  3 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  4 |       1 |         1 | True   |
      +----+---------+-----------+--------+

      >>> df.memory_usage()

      +---------+-------+
      |         |     0 |
      +=========+=======+
      | Index   | 40000 |
      +---------+-------+
      | int64   | 40000 |
      +---------+-------+
      | float64 | 40000 |
      +---------+-------+
      | bool    |  5000 |
      +---------+-------+

      >>> df.memory_usage(index=False)

      +---------+-------+
      |         |     0 |
      +=========+=======+
      | int64   | 40000 |
      +---------+-------+
      | float64 | 40000 |
      +---------+-------+
      | bool    |  5000 |
      +---------+-------+

      >>> df.memory_usage(unit="KB")

      +---------+----------+
      |         |        0 |
      +=========+==========+
      | Index   | 39.0625  |
      +---------+----------+
      | int64   | 39.0625  |
      +---------+----------+
      | float64 | 39.0625  |
      +---------+----------+
      | bool    |  4.88281 |
      +---------+----------+

      To get the approximate total memory usage:

      >>>  df.memory_usage(index=True).sum()



   .. py:method:: memory_usage_info(unit='GB')

      A formatted string representation of the size of this DataFrame.

      :param unit: Unit to return. One of {'KB', 'MB', 'GB'}.
      :type unit: str, default = "GB"

      :returns: A string representation of the number of bytes used by this DataFrame in [unit]s.
      :rtype: str

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(1000), 'col2': ak.arange(1000)})
      >>> df.memory_usage_info()
      '0.00 GB'

      >>> df.memory_usage_info(unit="KB")
      '15 KB'



   .. py:method:: merge(right: DataFrame, on: Optional[Union[str, List[str]]] = None, how: str = 'inner', left_suffix: str = '_x', right_suffix: str = '_y', convert_ints: bool = True, sort: bool = True) -> DataFrame

      Merge Arkouda DataFrames with a database-style join.
      The resulting dataframe contains rows from both DataFrames as specified by
      the merge condition (based on the "how" and "on" parameters).

      Based on pandas merge functionality.
      https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html

      :param right: The Right DataFrame to be joined.
      :type right: DataFrame
      :param on: The name or list of names of the DataFrame column(s) to join on.
                 If on is None, this defaults to the intersection of the columns in both DataFrames.
      :type on: Optional[Union[str, List[str]]] = None
      :param how: The merge condition.
                  Must be "inner", "left", or "right".
      :type how: {"inner", "left", "right}, default = "inner"
      :param left_suffix: A string indicating the suffix to add to columns from the left dataframe for overlapping
                          column names in both left and right. Defaults to "_x". Only used when how is "inner".
      :type left_suffix: str, default = "_x"
      :param right_suffix: A string indicating the suffix to add to columns from the right dataframe for overlapping
                           column names in both left and right. Defaults to "_y". Only used when how is "inner".
      :type right_suffix: str, default = "_y"
      :param convert_ints: If True, convert columns with missing int values (due to the join) to float64.
                           This is to match pandas.
                           If False, do not convert the column dtypes.
                           This has no effect when how = "inner".
      :type convert_ints: bool = True
      :param sort: If True, DataFrame is returned sorted by "on".
                   Otherwise, the DataFrame is not sorted.
      :type sort: bool = True

      :returns: Joined Arkouda DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. note:: Multiple column joins are only supported for integer columns.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> left_df = ak.DataFrame({'col1': ak.arange(5), 'col2': -1 * ak.arange(5)})
      >>> display(left_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+

      >>> right_df = ak.DataFrame({'col1': 2 * ak.arange(5), 'col2': 2 * ak.arange(5)})
      >>> display(right_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      2 |      2 |
      +----+--------+--------+
      |  2 |      4 |      4 |
      +----+--------+--------+
      |  3 |      6 |      6 |
      +----+--------+--------+
      |  4 |      8 |      8 |
      +----+--------+--------+

      >>> left_df.merge(right_df, on = "col1")

      +----+--------+----------+----------+
      |    |   col1 |   col2_x |   col2_y |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      2 |       -2 |        2 |
      +----+--------+----------+----------+
      |  2 |      4 |       -4 |        4 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "left")

      +----+--------+----------+----------+
      |    |   col1 |   col2_y |   col2_x |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      1 |      nan |       -1 |
      +----+--------+----------+----------+
      |  2 |      2 |        2 |       -2 |
      +----+--------+----------+----------+
      |  3 |      3 |      nan |       -3 |
      +----+--------+----------+----------+
      |  4 |      4 |        4 |       -4 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "right")

      +----+--------+----------+----------+
      |    |   col1 |   col2_x |   col2_y |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      2 |       -2 |        2 |
      +----+--------+----------+----------+
      |  2 |      4 |       -4 |        4 |
      +----+--------+----------+----------+
      |  3 |      6 |      nan |        6 |
      +----+--------+----------+----------+
      |  4 |      8 |      nan |        8 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "outer")

      +----+--------+----------+----------+
      |    |   col1 |   col2_y |   col2_x |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      1 |      nan |       -1 |
      +----+--------+----------+----------+
      |  2 |      2 |        2 |       -2 |
      +----+--------+----------+----------+
      |  3 |      3 |      nan |       -3 |
      +----+--------+----------+----------+
      |  4 |      4 |        4 |       -4 |
      +----+--------+----------+----------+
      |  5 |      6 |        6 |      nan |
      +----+--------+----------+----------+
      |  6 |      8 |        8 |      nan |
      +----+--------+----------+----------+



   .. py:method:: notna() -> DataFrame

      Detect existing (non-missing) values.

      Return a boolean same-sized object indicating if the values are not NA.
      numpy.NaN values get mapped to False values.

      :returns: Mask of bool values for each element in DataFrame
                that indicates whether an element is not an NA value.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({"A": [np.nan, 2, 2, 3], "B": [3, np.nan, 5, 6],
      ...          "C": [1, np.nan, 2, np.nan], "D":["a","b","c","d"]})
      >>> display(df)

      +----+-----+-----+-----+-----+
      |    |   A |   B |   C | D   |
      +====+=====+=====+=====+=====+
      |  0 | nan |   3 |   1 | a   |
      +----+-----+-----+-----+-----+
      |  1 |   2 | nan | nan | b   |
      +----+-----+-----+-----+-----+
      |  2 |   2 |   5 |   2 | c   |
      +----+-----+-----+-----+-----+
      |  3 |   3 |   6 | nan | d   |
      +----+-----+-----+-----+-----+

      >>> df.notna()
             A      B      C     D
      0  False   True   True  True
      1   True  False  False  True
      2   True   True   True  True
      3   True   True  False  True (4 rows x 4 columns)



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: read_csv(filename: str, col_delim: str = ',')

      Read the columns of a CSV file into an Arkouda DataFrame.
      If the file contains the appropriately formatted header, typed data will be returned.
      Otherwise, all data will be returned as a Strings objects.

      :param filename: Filename to read data from.
      :type filename: str
      :param col_delim: The delimiter for columns within the data.
      :type col_delim: str, default=","

      :returns: Arkouda DataFrame containing the columns from the CSV file.
      :rtype: arkouda.dataframe.DataFrame

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.

      .. seealso:: :obj:`to_csv`

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations.
      - The column delimiter is expected to be the same for column names and data.
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline ("\\n") at this time.
      - Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'csv_output','my_data')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_csv(my_path)
      >>> df2 = DataFrame.read_csv(my_path + "_LOCALE0000")
      >>> display(df2)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: register(user_defined_name: str) -> DataFrame

      Register this DataFrame object and underlying components with the Arkouda server.

      :param user_defined_name: User defined name the DataFrame is to be registered under.
                                This will be the root name for underlying components.
      :type user_defined_name: str

      :returns: The same DataFrame which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different DataFrames with the same name.
      :rtype: arkouda.dataframe.DataFrame

      :raises TypeError: Raised if user_defined_name is not a str.
      :raises RegistrationError: If the server was unable to register the DataFrame with the user_defined_name.

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_dataframe_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      Any changes made to a DataFrame object after registering with the server may not be reflected
      in attached copies.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: rename(mapper: Optional[Union[Callable, Dict]] = None, index: Optional[Union[Callable, Dict]] = None, column: Optional[Union[Callable, Dict]] = None, axis: Union[str, int] = 0, inplace: bool = False) -> Optional[DataFrame]

      Rename indexes or columns according to a mapping.

      :param mapper: Function or dictionary mapping existing values to new values.
                     Nonexistent names will not raise an error.
                     Uses the value of axis to determine if renaming column or index
      :type mapper: callable or dict-like, Optional
      :param column: Function or dictionary mapping existing column names to
                     new column names. Nonexistent names will not raise an
                     error.
                     When this is set, axis is ignored.
      :type column: callable or dict-like, Optional
      :param index: Function or dictionary mapping existing index names to
                    new index names. Nonexistent names will not raise an
                    error.
                    When this is set, axis is ignored.
      :type index: callable or dict-like, Optional
      :param axis: Indicates which axis to perform the rename.
                   0/"index" - Indexes
                   1/"column" - Columns
      :type axis: int or str, default=0
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`.
      :rtype: arkouda.dataframe.DataFrame or None

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A": ak.array([1, 2, 3]), "B": ak.array([4, 5, 6])})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename columns using a mapping:

      >>> df.rename(column={'A':'a', 'B':'c'})

      +----+-----+-----+
      |    |   a |   c |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename indexes using a mapping:

      >>> df.rename(index={0:99, 2:11})

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename using an axis style parameter:

      >>> df.rename(str.lower, axis='column')

      +----+-----+-----+
      |    |   a |   b |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+



   .. py:method:: reset_index(size: Optional[int] = None, inplace: bool = False) -> Union[None, DataFrame]

      Set the index to an integer range.

      Useful if this dataframe is the result of a slice operation from
      another dataframe, or if you have permuted the rows and no longer need
      to keep that ordering on the rows.

      :param size: If size is passed, do not attempt to determine size based on
                   existing column sizes. Assume caller handles consistency correctly.
      :type size: int, optional
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`.
      :rtype: arkouda.dataframe.DataFrame or None

      .. note::

         Pandas adds a column 'index' to indicate the original index. Arkouda does not currently
         support this behavior.

      .. rubric:: Example

      >>> df = ak.DataFrame({"A": ak.array([1, 2, 3]), "B": ak.array([4, 5, 6])})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      >>> perm_df = df[ak.array([0,2,1])]
      >>> display(perm_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   3 |   6 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+

      >>> perm_df.reset_index()

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   3 |   6 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+



   .. py:method:: sample(n=5)

      Return a random sample of `n` rows.

      :param n: Number of rows to return.
      :type n: int, default=5

      :returns: The sampled `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Example

      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+

      Random output of size 3:

      >>> df.sample(n=3)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   4 |  -4 |
      +----+-----+-----+



   .. py:method:: save(path, index=False, columns=None, file_format='HDF5', file_type='distribute', compression: Optional[str] = None)

      DEPRECATED
      Save DataFrame to disk, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: list, default=None
      :param file_format: 'HDF5' or 'Parquet'. Defaults to 'HDF5'
      :type file_format: str, default='HDF5'
      :param file_type: "single" or "distribute"
                        If single, will right a single file to locale 0.
      :type file_type: str, default=distribute
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Compression type. Only used for Parquet
      :type compression: str (Optional)

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf5_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> df.save(my_path + '/my_data', file_type="single")
      >>> df.load(my_path + '/my_data')

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+



   .. py:property:: shape
      The shape of the dataframe.

      :returns: Tuple of array dimensions.
      :rtype: tuple of int

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> df.shape
      (3, 2)


   .. py:property:: size
      Returns the number of bytes on the arkouda server.

      :returns: The number of bytes on the arkouda server.
      :rtype: int

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> df.size
      6


   .. py:method:: sort_index(ascending=True)

      Sort the DataFrame by indexed columns.

      Note: Fails on sort order of arkouda.strings.Strings columns when multiple columns being sorted.

      :param ascending: Sort values in ascending (default) or descending order.
      :type ascending: bool, default = True

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1.1, 3.1, 2.1], 'col2': [6, 5, 4]},
      ...          index = Index(ak.array([2,0,1]), name="idx"))

      >>> display(df)

      +----+--------+--------+
      | idx|   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    2.1 |      4 |
      +----+--------+--------+

      >>> df.sort_index()

      +----+--------+--------+
      | idx|   col1 |   col2 |
      +====+========+========+
      |  0 |    3.1 |      5 |
      +----+--------+--------+
      |  1 |    2.1 |      4 |
      +----+--------+--------+
      |  2 |    1.1 |      6 |
      +----+--------+--------+



   .. py:method:: sort_values(by=None, ascending=True)

      Sort the DataFrame by one or more columns.

      If no column is specified, all columns are used.

      Note: Fails on order of arkouda.strings.Strings columns when multiple columns being sorted.

      :param by: The name(s) of the column(s) to sort by.
      :type by: str or list/tuple of str, default = None
      :param ascending: Sort values in ascending (default) or descending order.
      :type ascending: bool, default = True

      .. seealso:: :obj:`apply_permutation`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [2, 2, 1], 'col2': [3, 4, 3], 'col3':[5, 6, 7]})
      >>> display(df)

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  1 |      2 |      4 |      6 |
      +----+--------+--------+--------+
      |  2 |      1 |      3 |      7 |
      +----+--------+--------+--------+

      >>> df.sort_values()

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      1 |      3 |      7 |
      +----+--------+--------+--------+
      |  1 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  2 |      2 |      4 |      6 |
      +----+--------+--------+--------+

      >>> df.sort_values("col3")

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      1 |      3 |      7 |
      +----+--------+--------+--------+
      |  1 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  2 |      2 |      4 |      6 |
      +----+--------+--------+--------+



   .. py:method:: tail(n=5)

      Return the last `n` rows.

      This function returns the last `n` rows for the dataframe. It is
      useful for quickly testing if your object has the right type of data in
      it.

      :param n: Number of rows to select.
      :type n: int, default=5

      :returns: The last `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`arkouda.dataframe.head`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(10), 'col2': -1 * ak.arange(10)})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+
      |  5 |      5 |     -5 |
      +----+--------+--------+
      |  6 |      6 |     -6 |
      +----+--------+--------+
      |  7 |      7 |     -7 |
      +----+--------+--------+
      |  8 |      8 |     -8 |
      +----+--------+--------+
      |  9 |      9 |     -9 |
      +----+--------+--------+

      >>> df.tail()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      5 |     -5 |
      +----+--------+--------+
      |  1 |      6 |     -6 |
      +----+--------+--------+
      |  2 |      7 |     -7 |
      +----+--------+--------+
      |  3 |      8 |     -8 |
      +----+--------+--------+
      |  4 |      9 |     -9 |
      +----+--------+--------+

      >>> df.tail(n=2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      8 |     -8 |
      +----+--------+--------+
      |  1 |      9 |     -9 |
      +----+--------+--------+



   .. py:method:: to_csv(path: str, index: bool = False, columns: Optional[List[str]] = None, col_delim: str = ',', overwrite: bool = False)

      Writes DataFrame to CSV file(s). File will contain a column for each column in the DataFrame.
      All CSV Files written by Arkouda include a header denoting data types of the columns.
      Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      :param path: The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                   when they are written to disk.
      :type path: str
      :param index: If True, the index of the DataFrame will be written to the file
                    as a column.
      :type index: bool, default=False
      :param columns: Column names to assign when writing data.
      :type columns: list of str (Optional)
      :param col_delim: Value to be used to separate columns within the file.
                        Please be sure that the value used DOES NOT appear in your dataset.
      :type col_delim: str, default=","
      :param overwrite: If True, any existing files matching your provided prefix_path will
                        be overwritten. If False, an error will be returned if existing files are found.
      :type overwrite: bool, default=False

      :rtype: None

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations.
      - The column delimiter is expected to be the same for column names and data.
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline ("\\n") at this time.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'csv_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_csv(my_path + "/my_data")
      >>> df2 = DataFrame.read_csv(my_path + "/my_data" + "_LOCALE0000")
      >>> display(df2)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: to_hdf(path, index=False, columns=None, file_type='distribute')

      Save DataFrame to disk as hdf5, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: List, default = None
      :param file_type: Whether to save to a single file or distribute across Locales.
      :type file_type: str (single | distribute), default=distribute

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_parquet`, :obj:`load`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: to_markdown(mode='wt', index=True, tablefmt='grid', storage_options=None, **kwargs)

      Print DataFrame in Markdown-friendly format.

      :param mode: Mode in which file is opened, "wt" by default.
      :type mode: str, optional
      :param index: Add index (row) labels.
      :type index: bool, optional, default True
      :param tablefmt: Table format to call from tablulate:
                       https://pypi.org/project/tabulate/
      :type tablefmt: str = "grid"
      :param storage_options: Extra options that make sense for a particular storage connection,
                              e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec,
                              e.g., starting “s3://”, “gcs://”.
                              An error will be raised if providing this argument with a non-fsspec URL.
                              See the fsspec and backend storage implementation docs for the set
                              of allowed keys and values.
      :type storage_options: dict, optional
      :param \*\*kwargs: These parameters will be passed to tabulate.

      .. note::

         This function should only be called on small DataFrames as it calls pandas.DataFrame.to_markdown:
         https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.to_markdown.html

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"animal_1": ["elk", "pig"], "animal_2": ["dog", "quetzal"]})
      >>> print(df.to_markdown())
      +----+------------+------------+
      |    | animal_1   | animal_2   |
      +====+============+============+
      |  0 | elk        | dog        |
      +----+------------+------------+
      |  1 | pig        | quetzal    |
      +----+------------+------------+

      Suppress the index:

      >>> print(df.to_markdown(index = False))
      +------------+------------+
      | animal_1   | animal_2   |
      +============+============+
      | elk        | dog        |
      +------------+------------+
      | pig        | quetzal    |
      +------------+------------+





   .. py:method:: to_pandas(datalimit=1073741824, retain_index=False)

      Send this DataFrame to a pandas DataFrame.

      :param datalimit: The maximum number size, in megabytes to transfer. The requested
                        DataFrame will be converted to a pandas DataFrame only if the
                        estimated size of the DataFrame does not exceed this value.
      :type datalimit: int, default=arkouda.client.maxTransferBytes
      :param retain_index: Normally, to_pandas() creates a new range index object. If you want
                           to keep the index column, set this to True.
      :type retain_index: bool, default=False

      :returns: The result of converting this DataFrame to a pandas DataFrame.
      :rtype: pandas.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> ak_df = ak.DataFrame({"A": ak.arange(2), "B": -1 * ak.arange(2)})
      >>> type(ak_df)
      arkouda.dataframe.DataFrame
      >>> display(ak_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+

      >>> import pandas as pd
      >>> pd_df = ak_df.to_pandas()
      >>> type(pd_df)
      pandas.core.frame.DataFrame
      >>> display(pd_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+



   .. py:method:: to_parquet(path, index=False, columns=None, compression: Optional[str] = None, convert_categoricals: bool = False)

      Save DataFrame to disk as parquet, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: list
      :param compression: Provide the compression type to use when writing the file.
                          Supported values: snappy, gzip, brotli, zstd, lz4
      :type compression: str (Optional), default=None
      :param convert_categoricals: Parquet requires all columns to be the same size and Categoricals
                                   don't satisfy that requirement.
                                   If set, write the equivalent Strings in place of any Categorical columns.
      :type convert_categoricals: bool, default=False

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_hdf`, :obj:`load`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'parquet_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_parquet(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   B |   A |
      +====+=====+=====+
      |  0 |   3 |   1 |
      +----+-----+-----+
      |  1 |   4 |   2 |
      +----+-----+-----+



   .. py:method:: transfer(hostname, port)

      Sends a DataFrame to a different Arkouda server.

      :param hostname: The hostname where the Arkouda server intended to
                       receive the DataFrame is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :returns: A message indicating a complete transfer.
      :rtype: str

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister()

      Unregister this DataFrame object in the arkouda server which was previously
      registered using register() and/or attached to using attach().

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister.

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_dataframe_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: unregister_dataframe_by_name(user_defined_name: str) -> str

      Function to unregister DataFrame object by name which was registered
      with the arkouda server via register().

      :param user_defined_name: Name under which the DataFrame object was registered.
      :type user_defined_name: str

      :raises TypeError: If user_defined_name is not a string.
      :raises RegistrationError: If there is an issue attempting to unregister any underlying components.

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister_dataframe_by_name("my_table_name")
      >>> df.is_registered()
      False



   .. py:method:: update_hdf(prefix_path: str, index=False, columns=None, repack: bool = True)

      Overwrite the dataset with the name provided with this dataframe. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share.
      :type prefix_path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: List, default=None
      :param repack: HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool, default=True

      :returns: Success message if successful.
      :rtype: str

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      If the dataset provided does not exist, it will be added.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+

      >>> df2 = ak.DataFrame({"A":[5,6],"B":[7,8]})
      >>> df2.update_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   5 |   7 |
      +----+-----+-----+
      |  1 |   6 |   8 |
      +----+-----+-----+



   .. py:method:: update_nrows()

      Computes the number of rows on the arkouda server and updates the size parameter.




.. py:class:: Datetime(pda, unit: str = _BASE_UNIT)

   Bases: :py:obj:`_AbstractBaseTime`


   Represents a date and/or time.

   Datetime is the Arkouda analog to pandas DatetimeIndex and
   other timeseries data types.

   :param pda:
   :type pda: int64 pdarray, pd.DatetimeIndex, pd.Series, or np.datetime64 array
   :param unit: For int64 pdarray, denotes the unit of the input. Ignored for pandas
                and numpy arrays, which carry their own unit. Not case-sensitive;
                prefixes of full names (like 'sec') are accepted.

                Possible values:

                * 'weeks' or 'w'
                * 'days' or 'd'
                * 'hours' or 'h'
                * 'minutes', 'm', or 't'
                * 'seconds' or 's'
                * 'milliseconds', 'ms', or 'l'
                * 'microseconds', 'us', or 'u'
                * 'nanoseconds', 'ns', or 'n'

                Unlike in pandas, units cannot be combined or mixed with integers
   :type unit: str, default 'ns'

   .. rubric:: Notes

   The ``.values`` attribute is always in nanoseconds with int64 dtype.


   .. py:property:: date


   .. py:property:: day


   .. py:property:: day_of_week


   .. py:property:: day_of_year


   .. py:property:: dayofweek


   .. py:property:: dayofyear


   .. py:property:: hour


   .. py:property:: is_leap_year


   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isocalendar()


   .. py:property:: microsecond


   .. py:property:: millisecond


   .. py:property:: minute


   .. py:property:: month


   .. py:property:: nanosecond


   .. py:method:: register(user_defined_name)

      Register this Datetime object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Datetime is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Datetime which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Datetimes with the same name.
      :rtype: Datetime

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Datetimes with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: second


   .. py:attribute:: special_objType
      :value: 'Datetime'



   .. py:method:: sum()

      Return the sum of all elements in the array.



   .. py:attribute:: supported_opeq


   .. py:attribute:: supported_with_datetime


   .. py:attribute:: supported_with_pdarray


   .. py:attribute:: supported_with_r_datetime


   .. py:attribute:: supported_with_r_pdarray


   .. py:attribute:: supported_with_r_timedelta


   .. py:attribute:: supported_with_timedelta


   .. py:method:: to_pandas()

      Convert array to a pandas DatetimeIndex. Note: if the array size
      exceeds client.maxTransferBytes, a RuntimeError is raised.

      .. seealso:: :obj:`to_ndarray`



   .. py:method:: unregister()

      Unregister this Datetime object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: week


   .. py:property:: weekday


   .. py:property:: weekofyear


   .. py:property:: year


.. py:class:: GroupBy

   Group an array or list of arrays by value, usually in preparation
   for aggregating the within-group values of another array.

   :param keys: The array to group by value, or if list, the column arrays to group by row
   :type keys: (list of) pdarray, Strings, or Categorical
   :param assume_sorted: If True, assume keys is already sorted (Default: False)
   :type assume_sorted: bool

   .. attribute:: nkeys

      The number of key arrays (columns)

      :type: int

   .. attribute:: size

      The length of the input array(s), i.e. number of rows

      :type: int

   .. attribute:: permutation

      The permutation that sorts the keys array(s) by value (row)

      :type: pdarray

   .. attribute:: unique_keys

      The unique values of the keys array(s), in grouped order

      :type: (list of) pdarray, Strings, or Categorical

   .. attribute:: ngroups

      The length of the unique_keys array(s), i.e. number of groups

      :type: int

   .. attribute:: segments

      The start index of each group in the grouped array(s)

      :type: pdarray

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. attribute:: dropna

      If True, and the groupby keys contain NaN values,
      the NaN values together with the corresponding row will be dropped.
      Otherwise, the rows corresponding to NaN values will be kept.

      :type: bool (default=True)

   :raises TypeError: Raised if keys is a pdarray with a dtype other than int64

   .. rubric:: Notes

   Integral pdarrays, Strings, and Categoricals are natively supported, but
   float64 and bool arrays are not.

   For a user-defined class to be groupable, it must inherit from pdarray
   and define or overload the grouping API:
     1) a ._get_grouping_keys() method that returns a list of pdarrays
        that can be (co)argsorted.
     2) (Optional) a .group() method that returns the permutation that
        groups the array
   If the input is a single array with a .group() method defined, method 2
   will be used; otherwise, method 1 will be used.


   .. py:method:: AND(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise AND of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise AND reduction on
      each group.

      :param values: The values to group and reduce with AND
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise AND of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: OR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise OR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise OR reduction on
      each group.

      :param values: The values to group and reduce with OR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise OR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: Reductions(*args, **kwargs)

      frozenset() -> empty frozenset object
      frozenset(iterable) -> frozenset object

      Build an immutable unordered collection of unique elements.




   .. py:method:: XOR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise XOR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise XOR reduction on
      each group.

      :param values: The values to group and reduce with XOR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise XOR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: aggregate(values: groupable, operator: str, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, groupable]

      Using the permutation stored in the GroupBy instance, group another
      array of values and apply a reduction to each group's values.

      :param values: The values to group and reduce
      :type values: pdarray
      :param operator: The name of the reduction operator to use
      :type operator: str
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **aggregates** (*groupable*) -- One aggregate value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if the requested operator is not supported for the
          values dtype

      .. rubric:: Examples

      >>> keys = ak.arange(0, 10)
      >>> vals = ak.linspace(-1, 1, 10)
      >>> g = ak.GroupBy(keys)
      >>> g.aggregate(vals, 'sum')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777768,
      -0.55555555555555536, -0.33333333333333348, -0.11111111111111116,
      0.11111111111111116, 0.33333333333333348, 0.55555555555555536, 0.77777777777777768,
      1]))
      >>> g.aggregate(vals, 'min')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777779,
      -0.55555555555555558, -0.33333333333333337, -0.11111111111111116, 0.11111111111111116,
      0.33333333333333326, 0.55555555555555536, 0.77777777777777768, 1]))



   .. py:method:: all(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform an "and" reduction on
      each group.

      :param values: The values to group and reduce with "and"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: any(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and perform an "or" reduction on each group.

      :param values: The values to group and reduce with "or"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array



   .. py:method:: argmax(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      maximum of each group's values.

      :param values: The values to group and find argmax
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argmaxima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The returned indices refer to the original values array as passed in,
      not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmax(b)
      (array([2, 3, 4]), array([9, 3, 2]))



   .. py:method:: argmin(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      minimum of each group's values.

      :param values: The values to group and find argmin
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argminima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values
          size or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if argmin is not supported for the values dtype

      .. rubric:: Notes

      The returned indices refer to the original values array as
      passed in, not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmin(b)
      (array([2, 3, 4]), array([5, 4, 2]))



   .. py:method:: attach(user_defined_name: str) -> GroupBy

      Function to return a GroupBy object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which GroupBy object was registered under
      :type user_defined_name: str

      :returns: The GroupBy object created by re-attaching to the corresponding server components
      :rtype: GroupBy

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_groupby_by_name`



   .. py:method:: broadcast(values: Union[pdarray, Strings], permute: bool = True) -> Union[pdarray, Strings]

      Fill each group's segment with a constant value.

      :param values: The values to put in each group's segment
      :type values: pdarray, Strings
      :param permute: If True (default), permute broadcast values back to the ordering
                      of the original array on which GroupBy was called. If False, the
                      broadcast values are grouped by value.
      :type permute: bool

      :returns: The broadcasted values
      :rtype: pdarray, Strings

      :raises TypeError: Raised if value is not a pdarray object
      :raises ValueError: Raised if the values array does not have one
          value per segment

      .. rubric:: Notes

      This function is a sparse analog of ``np.broadcast``. If a
      GroupBy object represents a sparse matrix (tensor), then
      this function takes a (dense) column vector and replicates
      each value to the non-zero elements in the corresponding row.

      .. rubric:: Examples

      >>> a = ak.array([0, 1, 0, 1, 0])
      >>> values = ak.array([3, 5])
      >>> g = ak.GroupBy(a)
      # By default, result is in original order
      >>> g.broadcast(values)
      array([3, 5, 3, 5, 3])
      # With permute=False, result is in grouped order
      >>> g.broadcast(values, permute=False)
      array([3, 3, 3, 5, 5]
      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 1, 4, 4, 4, 1, 3, 3, 2, 2])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> g.broadcast(counts > 2)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts == 3)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts < 4)
      array([True True True True True True True True True True])



   .. py:method:: build_from_components(user_defined_name: Optional[str] = None, **kwargs) -> GroupBy

      function to build a new GroupBy object from component keys and permutation.

      :param user_defined_name: and assign it the given name
      :type user_defined_name: str (Optional) Passing a name will init the new GroupBy
      :param kwargs: Expected keys are "orig_keys", "permutation", "unique_keys", and "segments"
      :type kwargs: dict Dictionary of components required for rebuilding the GroupBy.

      :returns: The GroupBy object created by using the given components
      :rtype: GroupBy



   .. py:method:: count(values: pdarray) -> Tuple[groupable, pdarray]

      Count the number of elements in each group.  NaN values will be excluded from the total.

      :param values: The values to be count by group (excluding NaN values).
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears (excluding NaN values).

      .. rubric:: Examples

      >>> a = ak.array([1, 0, -1, 1, 0, -1])
      >>> a
      array([1 0 -1 1 0 -1])
      >>> b = ak.array([1, np.nan, -1, np.nan, np.nan, -1], dtype = "float64")
      >>> b
      array([1.00000000000000000 nan -1.00000000000000000 nan nan -1.00000000000000000])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.count(b)
      >>> keys
      array([-1 0 1])
      >>> counts
      array([2 0 1])



   .. py:method:: first(values: groupable_element_type) -> Tuple[groupable, groupable_element_type]

      First value in each group.

      :param values: The values from which to take the first of each group
      :type values: pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first value of each group



   .. py:method:: from_return_msg(rep_msg)


   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_groupby_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: max(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the maximum of each
      group's values.

      :param values: The values to group and find maxima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_maxima** (*pdarray*) -- One maximum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if max is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if max is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.max(b)
      (array([2, 3, 4]), array([4, 4, 3]))



   .. py:method:: mean(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the mean of each group's
      values.

      :param values: The values to group and average
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_means** (*pdarray, float64*) -- One mean value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.mean(b)
      (array([2, 3, 4]), array([2.6666666666666665, 2.7999999999999998, 3]))



   .. py:method:: median(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the median of each group's
      values.

      :param values: The values to group and find median
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_medians** (*pdarray, float64*) -- One median value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,9)
      >>> a
      array([4 1 4 3 2 2 2 3 3])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([4 1 4 3 2 2 2 3 3])
      >>> b = ak.linspace(-5,5,9)
      >>> b
      array([-5 -3.75 -2.5 -1.25 0 1.25 2.5 3.75 5])
      >>> g.median(b)
      (array([1 2 3 4]), array([-3.75 1.25 3.75 -3.75]))



   .. py:method:: min(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the minimum of each group's
      values.

      :param values: The values to group and find minima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_minima** (*pdarray*) -- One minimum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if min is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if min is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.min(b)
      (array([2, 3, 4]), array([1, 1, 3]))



   .. py:method:: mode(values: groupable) -> Tuple[groupable, groupable]

      Most common value in each group. If a group is multi-modal, return the
      modal value that occurs first.

      :param values: The values from which to take the mode of each group
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) pdarray-like*) -- The most common value of each group



   .. py:method:: most_common(values)

      (Deprecated) See `GroupBy.mode()`.




   .. py:method:: nunique(values: groupable) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and return the number of unique values in each group.

      :param values: The values to group and find unique values
      :type values: pdarray, int64

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **group_nunique** (*groupable*) -- Number of unique values per unique key in the GroupBy instance

      :raises TypeError: Raised if the dtype(s) of values array(s) does/do not support
          the nunique method
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if nunique is not supported for the values dtype

      .. rubric:: Examples

      >>> data = ak.array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> data
      array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> labels = ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> labels
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g = ak.GroupBy(labels)
      >>> g.keys
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g.nunique(data)
      array([1,2,3,4]), array([2, 2, 3, 1])
      #    Group (1,1,1) has values [3,4,3] -> there are 2 unique values 3&4
      #    Group (2,2,2) has values [1,1,4] -> 2 unique values 1&4
      #    Group (3,3,3) has values [3,4,1] -> 3 unique values
      #    Group (4) has values [4] -> 1 unique value



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: prod(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the product of each group's
      values.

      :param values: The values to group and multiply
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_products** (*pdarray, float64*) -- One product per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if prod is not supported for the values dtype

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.prod(b)
      (array([2, 3, 4]), array([12, 108.00000000000003, 8.9999999999999982]))



   .. py:method:: register(user_defined_name: str) -> GroupBy

      Register this GroupBy object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the GroupBy is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same GroupBy which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different GroupBys with the same name.
      :rtype: GroupBy

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the GroupBy with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: sample(values: groupable, n=None, frac=None, replace=False, weights=None, random_state=None, return_indices=False, permute_samples=False)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param values: The values from which to sample, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the value more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the groupby keys
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional
      :param return_indices: if True, return the indices of the sampled values.
                             Otherwise, return the sample values.
      :type return_indices: bool, default False
      :param permute_samples: if True, return permute the samples according to group
                              Otherwise, keep samples in original order.
      :type permute_samples: bool, default False

      :returns: if return_indices is True, return the indices of the sampled values.
                Otherwise, return the sample values.
      :rtype: pdarray



   .. py:method:: size() -> Tuple[groupable, pdarray]

      Count the number of elements in each group, i.e. the number of times
      each key appears.  This counts the total number of rows (including NaN values).

      :param none:

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears

      .. seealso:: :obj:`count`

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 2, 3, 1, 2, 4, 3, 4, 3, 4])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> keys
      array([1, 2, 3, 4])
      >>> counts
      array([1, 2, 4, 3])



   .. py:method:: std(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the standard deviation of
      each group's values.

      :param values: The values to group and find standard deviation
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_stds** (*pdarray, float64*) -- One std value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The standard deviation is the square root of the average of the squared
      deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

      The average squared deviation is normally calculated as
      ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
      the divisor ``N - ddof`` is used instead. In standard statistical
      practice, ``ddof=1`` provides an unbiased estimator of the variance
      of the infinite population. ``ddof=0`` provides a maximum likelihood
      estimate of the variance for normally distributed variables. The
      standard deviation computed in this function is the square root of
      the estimated variance, so even with ``ddof=1``, it will not be an
      unbiased estimate of the standard deviation per se.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.std(b)
      (array([2 3 4]), array([1.5275252316519465 1.0954451150103321 0]))



   .. py:method:: sum(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and sum each group's values.

      :param values: The values to group and sum
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_sums** (*pdarray*) -- One sum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The grouped sum of a boolean ``pdarray`` returns integers.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.sum(b)
      (array([2, 3, 4]), array([8, 14, 6]))



   .. py:method:: to_hdf(prefix_path, dataset='groupby', mode='truncate', file_type='distribute')

      Save the GroupBy to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :returns: * *None*
                * *GroupBy is not currently supported by Parquet*



   .. py:method:: unique(values: groupable)

      Return the set of unique values in each group, as a SegArray.

      :param values: The values to unique
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) SegArray*) -- The unique values of each group

      :raises TypeError: Raised if values is or contains Strings or Categorical



   .. py:method:: unregister()

      Unregister this GroupBy object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_groupby_by_name(user_defined_name: str) -> None

      Function to unregister GroupBy object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the GroupBy object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'groupby', repack: bool = True)


   .. py:method:: var(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the variance of
      each group's values.

      :param values: The values to group and find variance
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_vars** (*pdarray, float64*) -- One var value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The variance is the average of the squared deviations from the mean,
      i.e.,  ``var = mean((x - x.mean())**2)``.

      The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
      If, however, `ddof` is specified, the divisor ``N - ddof`` is used
      instead.  In standard statistical practice, ``ddof=1`` provides an
      unbiased estimator of the variance of a hypothetical infinite population.
      ``ddof=0`` provides a maximum likelihood estimate of the variance for
      normally distributed variables.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.var(b)
      (array([2 3 4]), array([2.333333333333333 1.2 0]))



.. py:class:: Timedelta(pda, unit: str = _BASE_UNIT)

   Bases: :py:obj:`_AbstractBaseTime`


   Represents a duration, the difference between two dates or times.

   Timedelta is the Arkouda equivalent of pandas.TimedeltaIndex.

   :param pda:
   :type pda: int64 pdarray, pd.TimedeltaIndex, pd.Series, or np.timedelta64 array
   :param unit: For int64 pdarray, denotes the unit of the input. Ignored for pandas
                and numpy arrays, which carry their own unit. Not case-sensitive;
                prefixes of full names (like 'sec') are accepted.

                Possible values:

                * 'weeks' or 'w'
                * 'days' or 'd'
                * 'hours' or 'h'
                * 'minutes', 'm', or 't'
                * 'seconds' or 's'
                * 'milliseconds', 'ms', or 'l'
                * 'microseconds', 'us', or 'u'
                * 'nanoseconds', 'ns', or 'n'

                Unlike in pandas, units cannot be combined or mixed with integers
   :type unit: str, default 'ns'

   .. rubric:: Notes

   The ``.values`` attribute is always in nanoseconds with int64 dtype.


   .. py:method:: abs()

      Absolute value of time interval.



   .. py:property:: components


   .. py:property:: days


   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: microseconds


   .. py:property:: nanoseconds


   .. py:method:: register(user_defined_name)

      Register this Timedelta object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the timedelta is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Timedelta which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Timedeltas with the same name.
      :rtype: Timedelta

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the timedelta with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: seconds


   .. py:attribute:: special_objType
      :value: 'Timedelta'



   .. py:method:: std(ddof: arkouda.dtypes.int_scalars = 0)

      Returns the standard deviation as a pd.Timedelta object



   .. py:method:: sum()

      Return the sum of all elements in the array.



   .. py:attribute:: supported_opeq


   .. py:attribute:: supported_with_datetime


   .. py:attribute:: supported_with_pdarray


   .. py:attribute:: supported_with_r_datetime


   .. py:attribute:: supported_with_r_pdarray


   .. py:attribute:: supported_with_r_timedelta


   .. py:attribute:: supported_with_timedelta


   .. py:method:: to_pandas()

      Convert array to a pandas TimedeltaIndex. Note: if the array size
      exceeds client.maxTransferBytes, a RuntimeError is raised.

      .. seealso:: :obj:`to_ndarray`



   .. py:method:: total_seconds()


   .. py:method:: unregister()

      Unregister this timedelta object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



.. py:function:: arange(*args, **kwargs) -> arkouda.pdarrayclass.pdarray

   arange([start,] stop[, stride,] dtype=int64)

   Create a pdarray of consecutive integers within the interval [start, stop).
   If only one arg is given then arg is the stop parameter. If two args are
   given, then the first arg is start and second is stop. If three args are
   given, then the first arg is start, second is stop, third is stride.

   The return value is cast to type dtype

   :param start: Starting value (inclusive)
   :type start: int_scalars, optional
   :param stop: Stopping value (exclusive)
   :type stop: int_scalars
   :param stride: The difference between consecutive elements, the default stride is 1,
                  if stride is specified then start must also be specified.
   :type stride: int_scalars, optional
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Integers from start (inclusive) to stop (exclusive) by stride
   :rtype: pdarray, dtype

   :raises TypeError: Raised if start, stop, or stride is not an int object
   :raises ZeroDivisionError: Raised if stride == 0

   .. seealso:: :obj:`linspace`, :obj:`zeros`, :obj:`ones`, :obj:`randint`

   .. rubric:: Notes

   Negative strides result in decreasing values. Currently, only int64
   pdarrays can be created with this method. For float64 arrays, use
   the linspace method.

   .. rubric:: Examples

   >>> ak.arange(0, 5, 1)
   array([0, 1, 2, 3, 4])

   >>> ak.arange(5, 0, -1)
   array([5, 4, 3, 2, 1])

   >>> ak.arange(0, 10, 2)
   array([0, 2, 4, 6, 8])

   >>> ak.arange(-5, -10, -1)
   array([-5, -6, -7, -8, -9])


.. py:function:: date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, inclusive='both', **kwargs)

   Creates a fixed frequency Datetime range. Alias for
   ``ak.Datetime(pd.date_range(args))``. Subject to size limit
   imposed by client.maxTransferBytes.

   :param start: Left bound for generating dates.
   :type start: str or datetime-like, optional
   :param end: Right bound for generating dates.
   :type end: str or datetime-like, optional
   :param periods: Number of periods to generate.
   :type periods: int, optional
   :param freq: Frequency strings can have multiples, e.g. '5H'. See
                timeseries.offset_aliases for a list of
                frequency aliases.
   :type freq: str or DateOffset, default 'D'
   :param tz: Time zone name for returning localized DatetimeIndex, for example
              'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is
              timezone-naive.
   :type tz: str or tzinfo, optional
   :param normalize: Normalize start/end dates to midnight before generating date range.
   :type normalize: bool, default False
   :param name: Name of the resulting DatetimeIndex.
   :type name: str, default None
   :param closed: Make the interval closed with respect to the given frequency to
                  the 'left', 'right', or both sides (None, the default).
                  *Deprecated*
   :type closed: {None, 'left', 'right'}, optional
   :param inclusive: Include boundaries. Whether to set each bound as closed or open.
   :type inclusive: {"both", "neither", "left", "right"}, default "both"
   :param \*\*kwargs: For compatibility. Has no effect on the result.

   :returns: **rng**
   :rtype: DatetimeIndex

   .. rubric:: Notes

   Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
   exactly three must be specified. If ``freq`` is omitted, the resulting
   ``DatetimeIndex`` will have ``periods`` linearly spaced elements between
   ``start`` and ``end`` (closed on both sides).

   To learn more about the frequency strings, please see `this link
   <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.


.. py:function:: hist_all(ak_df: arkouda.dataframe.DataFrame, cols: list = [])

   Create a grid plot histogramming all numeric columns in ak dataframe

   :param ak_df: Full Arkouda DataFrame containing data to be visualized
   :type ak_df: ak.DataFrame
   :param cols: (Optional) A specified list of columns to be plotted
   :type cols: list

   .. rubric:: Notes

   This function displays the plot.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.plotting import hist_all
   >>> ak_df = ak.DataFrame({"a": ak.array(np.random.randn(100)),
                             "b": ak.array(np.random.randn(100)),
                             "c": ak.array(np.random.randn(100)),
                             "d": ak.array(np.random.randn(100))
                             })
   >>> hist_all(ak_df)


.. py:function:: histogram(pda: arkouda.pdarrayclass.pdarray, bins: arkouda.dtypes.int_scalars = 10) -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

   Compute a histogram of evenly spaced bins over the range of an array.

   :param pda: The values to histogram
   :type pda: pdarray
   :param bins: The number of equal-size bins to use (default: 10)
   :type bins: int_scalars

   :returns: Bin edges and The number of values present in each bin
   :rtype: (pdarray, Union[pdarray, int64 or float64])

   :raises TypeError: Raised if the parameter is not a pdarray or if bins is
       not an int.
   :raises ValueError: Raised if bins < 1
   :raises NotImplementedError: Raised if pdarray dtype is bool or uint8

   .. seealso:: :obj:`value_counts`, :obj:`histogram2d`

   .. rubric:: Notes

   The bins are evenly spaced in the interval [pda.min(), pda.max()].

   .. rubric:: Examples

   >>> import matplotlib.pyplot as plt
   >>> A = ak.arange(0, 10, 1)
   >>> nbins = 3
   >>> h, b = ak.histogram(A, bins=nbins)
   >>> h
   array([3, 3, 4])
   >>> b
   array([0., 3., 6., 9.])

   # To plot, export the left edges and the histogram to NumPy
   >>> plt.plot(b.to_ndarray()[::-1], h.to_ndarray())


.. py:function:: isnan(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise isnan check applied to the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing boolean values indicating whether the
             input array elements are NaN
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray
   :raises RuntimeError: if the underlying pdarray is not float-based

   .. rubric:: Examples

   >>> ak.isnan(ak.array[1.0, 2.0, 1.0 / 0.0])
   array([False, False, True])


.. py:function:: plot_dist(b, h, log=True, xlabel=None, newfig=True)

   Plot the distribution and cumulative distribution of histogram Data

   :param b: Bin edges
   :type b: np.ndarray
   :param h: Histogram data
   :type h: np.ndarray
   :param log: use log to scale y
   :type log: bool
   :param xlabel: Label for the x axis of the graph
   :type xlabel: str
   :param newfig: Generate a new figure or not
   :type newfig: bool

   .. rubric:: Notes

   This function does not return or display the plot. A user must have matplotlib imported in
   addition to arkouda to display plots. This could be updated to return the object or have a
   flag to show the resulting plots.
   See Examples Below.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from matplotlib import pyplot as plt
   >>> b, h = ak.histogram(ak.arange(10), 3)
   >>> ak.plot_dist(b, h.to_ndarray())
   >>> # to show the plot
   >>> plt.show()


.. py:function:: skew(pda: pdarray, bias: bool = True) -> numpy.float64

   Computes the sample skewness of an array.
   Skewness > 0 means there's greater weight in the right tail of the distribution.
   Skewness < 0 means there's greater weight in the left tail of the distribution.
   Skewness == 0 means the data is normally distributed.
   Based on the `scipy.stats.skew` function.

   :param pda: A pdarray of values that will be calculated to find the skew
   :type pda: pdarray
   :param bias: If False, then the calculations are corrected for statistical bias.
   :type bias: bool, optional

   :returns: *

               np.float64
                   The skew of all elements in the array
             * *Examples*
             * *>>> a = ak.array([1, 1, 1, 5, 10])*
             * *>>> ak.skew(a)*
             * *0.9442193396379163*


.. py:function:: timedelta_range(start=None, end=None, periods=None, freq=None, name=None, closed=None, **kwargs)

   Return a fixed frequency TimedeltaIndex, with day as the default
   frequency. Alias for ``ak.Timedelta(pd.timedelta_range(args))``.
   Subject to size limit imposed by client.maxTransferBytes.

   :param start: Left bound for generating timedeltas.
   :type start: str or timedelta-like, default None
   :param end: Right bound for generating timedeltas.
   :type end: str or timedelta-like, default None
   :param periods: Number of periods to generate.
   :type periods: int, default None
   :param freq: Frequency strings can have multiples, e.g. '5H'.
   :type freq: str or DateOffset, default 'D'
   :param name: Name of the resulting TimedeltaIndex.
   :type name: str, default None
   :param closed: Make the interval closed with respect to the given frequency to
                  the 'left', 'right', or both sides (None).
   :type closed: str, default None

   :returns: **rng**
   :rtype: TimedeltaIndex

   .. rubric:: Notes

   Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
   exactly three must be specified. If ``freq`` is omitted, the resulting
   ``TimedeltaIndex`` will have ``periods`` linearly spaced elements between
   ``start`` and ``end`` (closed on both sides).

   To learn more about the frequency strings, please see `this link
   <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.


