arkouda
=======

.. py:module:: arkouda

.. autoapi-nested-parse::

   Arkouda: Exploratory data science at scale.

   Arkouda is a Python API for exploratory data analysis on massive datasets. It
   leverages a Chapel-based backend to enable high-performance computing on
   distributed systems, while exposing a familiar NumPy- and Pandas-like interface
   to Python users.

   Key Features
   ------------
   - `pdarray` and `Strings` types for working with large numeric and string arrays.
   - `Categorical`, `Series`, `DataFrame`, and `Index` for labeled data analysis.
   - High-performance `GroupBy`, reductions, and broadcasting operations.
   - Interoperability with NumPy and Pandas for ease of use.
   - A scalable architecture suitable for HPC and cloud environments.

   Example:
   -------
   >>> import arkouda as ak
   >>> ak.connect()  # doctest: +SKIP
   >>> a = ak.array([1, 2, 3])
   >>> b = a + 5
   >>> print(b)
   array([6 7 8])

   For full documentation, visit: https://bears-r-us.github.io/arkouda/



Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/arkouda/apply/index
   /autoapi/arkouda/array_api/index
   /autoapi/arkouda/client_dtypes/index
   /autoapi/arkouda/comm_diagnostics/index
   /autoapi/arkouda/core/index
   /autoapi/arkouda/history/index
   /autoapi/arkouda/numpy/index
   /autoapi/arkouda/pandas/index
   /autoapi/arkouda/plotting/index
   /autoapi/arkouda/scipy/index
   /autoapi/arkouda/testing/index


Attributes
----------

.. autoapisummary::

   arkouda.AllSymbols
   arkouda.ArkoudaArrayLike
   arkouda.RegisteredSymbols


Classes
-------

.. autoapisummary::

   arkouda.ArkoudaArray
   arkouda.ArkoudaBigintDtype
   arkouda.ArkoudaBoolDtype
   arkouda.ArkoudaCategorical
   arkouda.ArkoudaCategoricalDtype
   arkouda.ArkoudaFloat64Dtype
   arkouda.ArkoudaInt64Dtype
   arkouda.ArkoudaStringArray
   arkouda.ArkoudaStringDtype
   arkouda.ArkoudaUint64Dtype
   arkouda.ArkoudaUint8Dtype
   arkouda.CachedAccessor
   arkouda.Categorical
   arkouda.DataFrame
   arkouda.DataFrameGroupBy
   arkouda.DatetimeAccessor
   arkouda.DiffAggregate
   arkouda.Index
   arkouda.LogLevel
   arkouda.MultiIndex
   arkouda.Power_divergenceResult
   arkouda.Properties
   arkouda.Row
   arkouda.Series
   arkouda.StringAccessor


Functions
---------

.. autoapisummary::

   arkouda.apply
   arkouda.assert_almost_equal
   arkouda.assert_almost_equivalent
   arkouda.assert_arkouda_array_equal
   arkouda.assert_arkouda_array_equivalent
   arkouda.assert_arkouda_pdarray_equal
   arkouda.assert_arkouda_segarray_equal
   arkouda.assert_arkouda_strings_equal
   arkouda.assert_attr_equal
   arkouda.assert_categorical_equal
   arkouda.assert_class_equal
   arkouda.assert_contains_all
   arkouda.assert_copy
   arkouda.assert_dict_equal
   arkouda.assert_equal
   arkouda.assert_equivalent
   arkouda.assert_frame_equal
   arkouda.assert_frame_equivalent
   arkouda.assert_index_equal
   arkouda.assert_index_equivalent
   arkouda.assert_is_sorted
   arkouda.assert_series_equal
   arkouda.assert_series_equivalent
   arkouda.chisquare
   arkouda.compute_join_size
   arkouda.date_operators
   arkouda.disableVerbose
   arkouda.disable_verbose
   arkouda.enableVerbose
   arkouda.enable_verbose
   arkouda.export
   arkouda.from_series
   arkouda.gen_ranges
   arkouda.get_columns
   arkouda.get_datasets
   arkouda.get_filetype
   arkouda.get_null_indices
   arkouda.import_data
   arkouda.information
   arkouda.intersect
   arkouda.intx
   arkouda.invert_permutation
   arkouda.join_on_eq_with_dt
   arkouda.list_registry
   arkouda.list_symbol_table
   arkouda.load
   arkouda.load_all
   arkouda.load_checkpoint
   arkouda.ls
   arkouda.ls_csv
   arkouda.merge
   arkouda.power_divergence
   arkouda.pretty_print_information
   arkouda.read
   arkouda.read_csv
   arkouda.read_hdf
   arkouda.read_parquet
   arkouda.read_tagged_data
   arkouda.read_zarr
   arkouda.receive
   arkouda.receive_dataframe
   arkouda.restore
   arkouda.save_checkpoint
   arkouda.snapshot
   arkouda.string_operators
   arkouda.to_csv
   arkouda.to_hdf
   arkouda.to_parquet
   arkouda.to_zarr
   arkouda.update_hdf
   arkouda.write_log
   arkouda.xlogy


Package Contents
----------------

.. py:data:: AllSymbols
   :value: '__AllSymbols__'


.. py:class:: ArkoudaArray(data: arkouda.numpy.pdarrayclass.pdarray | numpy.ndarray | Sequence[Any] | ArkoudaArray, dtype: Any = None, copy: bool = False)

   Bases: :py:obj:`arkouda.pandas.extension._arkouda_extension_array.ArkoudaExtensionArray`, :py:obj:`pandas.api.extensions.ExtensionArray`


   Arkouda-backed numeric/bool pandas ExtensionArray.

   Wraps or converts supported inputs into an Arkouda ``pdarray`` to serve as the
   backing store. Ensures the underlying array is 1-D and lives on the Arkouda server.

   :param data: Input to wrap or convert.
                - If an Arkouda ``pdarray``, it is used directly unless ``dtype`` is given
                  or ``copy=True``, in which case a new array is created via ``ak.array``.
                - If a NumPy array, it is transferred to Arkouda via ``ak.array``.
                - If a Python sequence, it is converted to NumPy then to Arkouda.
                - If another ``ArkoudaArray``, its underlying ``pdarray`` is reused.
   :type data: pdarray | ndarray | Sequence[Any] | ArkoudaArray
   :param dtype: Desired dtype to cast to (NumPy dtype or Arkouda dtype string). If omitted,
                 dtype is inferred from ``data``.
   :type dtype: Any, optional
   :param copy: If True, attempt to copy the underlying data when converting/wrapping.
                Default is False.
   :type copy: bool

   :raises TypeError: If ``data`` cannot be interpreted as an Arkouda array-like object.
   :raises ValueError: If the resulting array is not one-dimensional.

   .. attribute:: default_fill_value

      Sentinel used when filling missing values (default: -1).

      :type: int

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.pandas.extension import ArkoudaArray
   >>> ArkoudaArray(ak.arange(5))
   ArkoudaArray([0 1 2 3 4])
   >>> ArkoudaArray([10, 20, 30])
   ArkoudaArray([10 20 30])


   .. py:method:: all(axis=0, skipna=True, **kwargs)

      Return whether all elements are True.

      This is mainly to support pandas' BaseExtensionArray.equals, which
      calls `.all()` on the result of a boolean expression.



   .. py:method:: any(axis=0, skipna=True, **kwargs)

      Return whether any element is True.

      Added for symmetry with `.all()` and to support potential pandas
      boolean-reduction calls.



   .. py:method:: astype(dtype: numpy.dtype[Any], copy: bool = True) -> numpy.typing.NDArray[Any]
                  astype(dtype: pandas.core.dtypes.dtypes.ExtensionDtype, copy: bool = True) -> pandas.api.extensions.ExtensionArray
                  astype(dtype: Any, copy: bool = True) -> Union[pandas.api.extensions.ExtensionArray, numpy.typing.NDArray[Any]]

      Cast the array to a specified dtype.

      Casting rules:

      * If ``dtype`` requests ``object``, returns a NumPy ``NDArray[Any]`` of
        dtype ``object`` containing the array values.
      * Otherwise, the target dtype is normalized using Arkouda's dtype
        resolution rules.
      * If the normalized dtype matches the current dtype and ``copy=False``,
        returns ``self``.
      * In all other cases, casts the underlying Arkouda array to the target
        dtype and returns an Arkouda-backed ``ArkoudaExtensionArray``.

      :param dtype: Target dtype. May be a NumPy dtype, pandas dtype, Arkouda dtype,
                    or any dtype-like object accepted by Arkouda.
      :type dtype: Any
      :param copy: Whether to force a copy when the target dtype matches the current dtype.
                   Default is True.
      :type copy: bool

      :returns: The cast result. Returns a NumPy array only when casting to ``object``;
                otherwise returns an Arkouda-backed ExtensionArray.
      :rtype: Union[ExtensionArray, NDArray[Any]]

      .. rubric:: Examples

      Basic numeric casting returns an Arkouda-backed array:

      >>> import arkouda as ak
      >>> from arkouda.pandas.extension import ArkoudaArray
      >>> a = ArkoudaArray(ak.array([1, 2, 3], dtype="int64"))
      >>> a.astype("float64").to_ndarray()
      array([1., 2., 3.])

      Casting to the same dtype with ``copy=False`` returns the original object:

      >>> b = a.astype("int64", copy=False)
      >>> b is a
      True

      Forcing a copy when the dtype is unchanged returns a new array:

      >>> c = a.astype("int64", copy=True)
      >>> c is a
      False
      >>> c.to_ndarray()
      array([1, 2, 3])

      Casting to ``object`` materializes the data to a NumPy array:

      >>> a.astype(object)
      array([1, 2, 3], dtype=object)

      NumPy and pandas dtype objects are also accepted:

      >>> import numpy as np
      >>> a.astype(np.dtype("bool")).to_ndarray()
      array([ True,  True,  True])



   .. py:attribute:: default_fill_value
      :type:  int
      :value: -1



   .. py:property:: dtype

      An instance of ExtensionDtype.

      .. seealso::

         :py:obj:`api.extensions.ExtensionDtype`
             Base class for extension dtypes.

         :py:obj:`api.extensions.ExtensionArray`
             Base class for extension array types.

         :py:obj:`api.extensions.ExtensionArray.dtype`
             The dtype of an ExtensionArray.

         :py:obj:`Series.dtype`
             The dtype of a Series.

         :py:obj:`DataFrame.dtype`
             The dtype of a DataFrame.

      .. rubric:: Examples

      >>> pd.array([1, 2, 3]).dtype
      Int64Dtype()


   .. py:method:: equals(other)

      Return if another array is equivalent to this array.

      Equivalent means that both arrays have the same shape and dtype, and
      all values compare equal. Missing values in the same location are
      considered equal (in contrast with normal equality).

      :param other: Array to compare to this Array.
      :type other: ExtensionArray

      :returns: Whether the arrays are equivalent.
      :rtype: boolean

      .. seealso::

         :py:obj:`numpy.array_equal`
             Equivalent method for numpy array.

         :py:obj:`Series.equals`
             Equivalent method for Series.

         :py:obj:`DataFrame.equals`
             Equivalent method for DataFrame.

      .. rubric:: Examples

      >>> arr1 = pd.array([1, 2, np.nan])
      >>> arr2 = pd.array([1, 2, np.nan])
      >>> arr1.equals(arr2)
      True

      >>> arr1 = pd.array([1, 3, np.nan])
      >>> arr2 = pd.array([1, 2, np.nan])
      >>> arr1.equals(arr2)
      False



   .. py:method:: isna() -> numpy.ndarray

      Return a boolean mask indicating missing values.

      This method implements the pandas ExtensionArray.isna contract
      and always returns a NumPy ndarray of dtype ``bool`` with the
      same length as the array.

      :returns: A boolean mask where ``True`` marks elements considered missing.
      :rtype: np.ndarray

      :raises TypeError: If the underlying data buffer does not support missing-value
          detection or cannot produce a boolean mask.



   .. py:method:: isnull()

      Alias for isna().



   .. py:property:: nbytes

      The number of bytes needed to store this object in memory.

      .. seealso::

         :py:obj:`ExtensionArray.shape`
             Return a tuple of the array dimensions.

         :py:obj:`ExtensionArray.size`
             The number of elements in the array.

      .. rubric:: Examples

      >>> pd.array([1, 2, 3]).nbytes
      27


   .. py:method:: value_counts(dropna: bool = True) -> pandas.Series

      Return counts of unique values as a pandas Series.

      This method computes the frequency of each distinct value in the
      underlying Arkouda array and returns the result as a pandas
      ``Series``, with the unique values as the index and their counts
      as the data.

      :param dropna: Whether to exclude missing values. Currently, missing-value
                     handling is supported only for floating-point data, where
                     ``NaN`` values are treated as missing. Default is True.
      :type dropna: bool

      :returns: A Series containing the counts of unique values.
                The index is an ``ArkoudaArray`` of unique values, and the
                values are an ``ArkoudaArray`` of counts.
      :rtype: pd.Series

      .. rubric:: Notes

      - Only ``dropna=True`` is supported.
      - The following pandas options are not yet implemented:
        ``normalize``, ``sort``, and ``bins``.
      - Counting is performed server-side in Arkouda; only the small
        result (unique values and counts) is materialized on the client.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda.pandas.extension import ArkoudaArray
      >>>
      >>> a = ArkoudaArray(ak.array([1, 2, 1, 3, 2, 1]))
      >>> a.value_counts()
      1    3
      2    2
      3    1
      dtype: int64

      Floating-point data with NaN values:

      >>> b = ArkoudaArray(ak.array([1.0, 2.0, float("nan"), 1.0]))
      >>> b.value_counts()
      1.0    2
      2.0    1
      dtype: int64



.. py:data:: ArkoudaArrayLike

.. py:class:: ArkoudaBigintDtype

   Bases: :py:obj:`_ArkoudaBaseDtype`


   Arkouda-backed arbitrary-precision integer dtype.

   This dtype integrates Arkouda's server-backed ``pdarray<bigint>`` with
   the pandas ExtensionArray interface via :class:`ArkoudaArray`. It enables
   pandas objects (Series, DataFrame) to hold and operate on very large
   integers that exceed 64-bit precision, while keeping the data distributed
   on the Arkouda server.

   .. method:: construct_array_type()

      Returns the :class:`ArkoudaArray` class used for storage.



   .. py:method:: construct_array_type()
      :classmethod:


      Return the ExtensionArray subclass that handles storage for this dtype.

      :returns: The :class:`ArkoudaArray` class associated with this dtype.
      :rtype: type



   .. py:attribute:: kind
      :value: 'O'


      A character code (one of 'biufcmMOSUV'), default 'O'

      This should match the NumPy dtype used when the array is
      converted to an ndarray, which is probably 'O' for object if
      the extension type cannot be represented as a built-in NumPy
      type.

      .. seealso:: :py:obj:`numpy.dtype.kind`


   .. py:attribute:: na_value
      :value: -1


      Default NA value to use for this type.

      This is used in e.g. ExtensionArray.take. This should be the
      user-facing "boxed" version of the NA value, not the physical NA value
      for storage.  e.g. for JSONArray, this is an empty dictionary.


   .. py:attribute:: name
      :value: 'bigint'


      A string identifying the data type.

      Will be used for display in, e.g. ``Series.dtype``


   .. py:attribute:: type

      The scalar type for the array, e.g. ``int``

      It's expected ``ExtensionArray[item]`` returns an instance
      of ``ExtensionDtype.type`` for scalar ``item``, assuming
      that value is valid (not NA). NA values do not need to be
      instances of `type`.


.. py:class:: ArkoudaBoolDtype

   Bases: :py:obj:`_ArkoudaBaseDtype`


   Arkouda-backed boolean dtype.

   This dtype integrates Arkouda's server-backed `pdarray<bool>` with
   the pandas ExtensionArray interface via :class:`ArkoudaArray`. It allows
   pandas objects (Series, DataFrame) to store and manipulate distributed
   boolean arrays without materializing them on the client.

   .. method:: construct_array_type()

      Returns the :class:`ArkoudaArray` class used for storage.



   .. py:method:: construct_array_type()
      :classmethod:


      Return the ExtensionArray subclass that handles storage for this dtype.

      :returns: The :class:`ArkoudaArray` class associated with this dtype.
      :rtype: type



   .. py:attribute:: kind
      :value: 'b'


      A character code (one of 'biufcmMOSUV'), default 'O'

      This should match the NumPy dtype used when the array is
      converted to an ndarray, which is probably 'O' for object if
      the extension type cannot be represented as a built-in NumPy
      type.

      .. seealso:: :py:obj:`numpy.dtype.kind`


   .. py:attribute:: na_value
      :value: False


      Default NA value to use for this type.

      This is used in e.g. ExtensionArray.take. This should be the
      user-facing "boxed" version of the NA value, not the physical NA value
      for storage.  e.g. for JSONArray, this is an empty dictionary.


   .. py:attribute:: name
      :value: 'bool_'


      A string identifying the data type.

      Will be used for display in, e.g. ``Series.dtype``


   .. py:attribute:: type

      The scalar type for the array, e.g. ``int``

      It's expected ``ExtensionArray[item]`` returns an instance
      of ``ExtensionDtype.type`` for scalar ``item``, assuming
      that value is valid (not NA). NA values do not need to be
      instances of `type`.


.. py:class:: ArkoudaCategorical(data: arkouda.pandas.categorical.Categorical | ArkoudaCategorical | numpy.ndarray | Sequence[Any])

   Bases: :py:obj:`arkouda.pandas.extension._arkouda_extension_array.ArkoudaExtensionArray`, :py:obj:`pandas.api.extensions.ExtensionArray`


   Arkouda-backed categorical pandas ExtensionArray.

   Ensures the underlying data is an Arkouda ``Categorical``. Accepts an existing
   ``Categorical`` or converts from Python/NumPy sequences of labels.

   :param data: Input to wrap or convert.
                - If ``Categorical``, used directly.
                - If another ``ArkoudaCategorical``, its backing object is reused.
                - If list/tuple/ndarray, converted via ``ak.Categorical(ak.array(data))``.
   :type data: Categorical | ArkoudaCategorical | ndarray | Sequence[Any]

   :raises TypeError: If ``data`` cannot be converted to Arkouda ``Categorical``.

   .. attribute:: default_fill_value

      Sentinel used when filling missing values (default: "").

      :type: str


   .. py:method:: add_categories(*args, **kwargs)


   .. py:method:: as_ordered(*args, **kwargs)


   .. py:method:: as_unordered(*args, **kwargs)


   .. py:method:: astype(dtype: numpy.dtype[Any], copy: bool = True) -> numpy.typing.NDArray[Any]
                  astype(dtype: pandas.core.dtypes.dtypes.ExtensionDtype, copy: bool = True) -> pandas.api.extensions.ExtensionArray
                  astype(dtype: Any, copy: bool = True) -> Union[pandas.api.extensions.ExtensionArray, numpy.typing.NDArray[Any]]

      Cast to a specified dtype.

      * If ``dtype`` is categorical (pandas ``category`` / ``CategoricalDtype`` /
        ``ArkoudaCategoricalDtype``), returns an Arkouda-backed
        ``ArkoudaCategorical`` (optionally copied).
      * If ``dtype`` requests ``object``, returns a NumPy ``ndarray`` of dtype object
        containing the category labels (materialized to the client).
      * If ``dtype`` requests a string dtype, returns an Arkouda-backed
        ``ArkoudaStringArray`` containing the labels as strings.
      * Otherwise, casts the labels (as strings) to the requested dtype and returns an
        Arkouda-backed ExtensionArray.

      :param dtype: Target dtype.
      :type dtype: Any
      :param copy: Whether to force a copy when possible. If categorical-to-categorical and
                   ``copy=True``, attempts to copy the underlying Arkouda ``Categorical`` (if
                   supported). Default is True.
      :type copy: bool

      :returns: The cast result. Returns a NumPy array only when casting to ``object``;
                otherwise returns an Arkouda-backed ExtensionArray.
      :rtype: Union[ExtensionArray, NDArray[Any]]

      .. rubric:: Examples

      Casting to ``category`` returns an Arkouda-backed categorical array:

      >>> import arkouda as ak
      >>> from arkouda.pandas.extension import ArkoudaCategorical
      >>> c = ArkoudaCategorical(ak.Categorical(ak.array(["x", "y", "x"])))
      >>> out = c.astype("category")
      >>> out is c
      False

      Forcing a copy when casting to the same categorical dtype returns a new array:

      >>> out2 = c.astype("category", copy=True)
      >>> out2 is c
      False
      >>> out2.to_ndarray()
      array(['x', 'y', 'x'], dtype='<U...')

      Casting to ``object`` materializes the category labels to a NumPy object array:

      >>> c.astype(object)
      array(['x', 'y', 'x'], dtype=object)

      Casting to a string dtype returns an Arkouda-backed string array of labels:

      >>> s = c.astype("string")
      >>> s.to_ndarray()
      array(['x', 'y', 'x'], dtype='<U1')

      Casting to another dtype casts the labels-as-strings and returns an Arkouda-backed array:

      >>> c_num = ArkoudaCategorical(ak.Categorical(ak.array(["1", "2", "3"])))
      >>> a = c_num.astype("int64")
      >>> a.to_ndarray()
      array([1, 2, 3])



   .. py:method:: check_for_ordered(*args, **kwargs)


   .. py:attribute:: default_fill_value
      :type:  str
      :value: ''



   .. py:method:: describe(*args, **kwargs)


   .. py:property:: dtype

      An instance of ExtensionDtype.

      .. seealso::

         :py:obj:`api.extensions.ExtensionDtype`
             Base class for extension dtypes.

         :py:obj:`api.extensions.ExtensionArray`
             Base class for extension array types.

         :py:obj:`api.extensions.ExtensionArray.dtype`
             The dtype of an ExtensionArray.

         :py:obj:`Series.dtype`
             The dtype of a Series.

         :py:obj:`DataFrame.dtype`
             The dtype of a DataFrame.

      .. rubric:: Examples

      >>> pd.array([1, 2, 3]).dtype
      Int64Dtype()


   .. py:method:: from_codes(*args, **kwargs)
      :classmethod:

      :abstractmethod:



   .. py:method:: isna() -> numpy.ndarray

      # Return a boolean mask indicating missing values.

      # This implements the pandas ExtensionArray.isna contract and returns a
      # NumPy ndarray[bool] of the same length as this categorical array.

      # Returns
      # -------
      # np.ndarray
      #     Boolean mask where True indicates a missing value.

      # Raises
      # ------
      # TypeError
      #     If the underlying categorical cannot expose its codes or if missing
      #     detection is unsupported.
      #



   .. py:method:: isnull()

      Alias for isna().



   .. py:method:: max(*args, **kwargs)


   .. py:method:: memory_usage(*args, **kwargs)


   .. py:method:: min(*args, **kwargs)


   .. py:method:: notna(*args, **kwargs)


   .. py:method:: notnull(*args, **kwargs)


   .. py:method:: remove_categories(*args, **kwargs)


   .. py:method:: remove_unused_categories(*args, **kwargs)


   .. py:method:: rename_categories(*args, **kwargs)


   .. py:method:: reorder_categories(*args, **kwargs)


   .. py:method:: set_categories(*args, **kwargs)


   .. py:method:: set_ordered(*args, **kwargs)


   .. py:method:: sort_values(*args, **kwargs)


   .. py:method:: to_list(*args, **kwargs)


   .. py:method:: value_counts(dropna: bool = True) -> pandas.Series

      Return counts of categories as a pandas Series.

      This method computes category frequencies from the underlying Arkouda
      ``Categorical`` and returns them as a pandas ``Series``, where the
      index contains the category labels and the values contain the
      corresponding counts.

      :param dropna: Whether to drop missing values from the result. When ``True``,
                     the result is filtered using the categorical's ``na_value``.
                     When ``False``, all categories returned by the underlying
                     computation are included. Default is True.
      :type dropna: bool

      :returns: A Series containing category counts.
                The index is an ``ArkoudaStringArray`` of category labels and the
                values are an ``ArkoudaArray`` of counts.
      :rtype: pd.Series

      .. rubric:: Notes

      - The result is computed server-side in Arkouda; only the (typically small)
        output of categories and counts is materialized for the pandas ``Series``.
      - This method does not yet support pandas options such as ``normalize``,
        ``sort``, or ``bins``.
      - The handling of missing values depends on the Arkouda ``Categorical``
        definition of ``na_value``.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda.pandas.extension import ArkoudaCategorical
      >>>
      >>> a = ArkoudaCategorical(["a", "b", "a", "c", "b", "a"])
      >>> a.value_counts()
      a    3
      b    2
      c    1
      dtype: int64



.. py:class:: ArkoudaCategoricalDtype

   Bases: :py:obj:`_ArkoudaBaseDtype`


   Arkouda-backed categorical dtype.

   This dtype integrates Arkouda's distributed ``Categorical`` type with
   the pandas ExtensionArray interface via :class:`ArkoudaCategorical`.
   It enables pandas objects (Series, DataFrame) to hold categorical data
   stored and processed on the Arkouda server, while exposing familiar
   pandas APIs.

   .. method:: construct_array_type()

      Returns the :class:`ArkoudaCategorical` used as the storage class.



   .. py:method:: construct_array_type()
      :classmethod:


      Return the ExtensionArray subclass that handles storage for this dtype.

      :returns: The :class:`ArkoudaCategorical` class associated with this dtype.
      :rtype: type



   .. py:attribute:: kind
      :value: 'O'


      A character code (one of 'biufcmMOSUV'), default 'O'

      This should match the NumPy dtype used when the array is
      converted to an ndarray, which is probably 'O' for object if
      the extension type cannot be represented as a built-in NumPy
      type.

      .. seealso:: :py:obj:`numpy.dtype.kind`


   .. py:attribute:: na_value
      :value: -1


      Default NA value to use for this type.

      This is used in e.g. ExtensionArray.take. This should be the
      user-facing "boxed" version of the NA value, not the physical NA value
      for storage.  e.g. for JSONArray, this is an empty dictionary.


   .. py:attribute:: name
      :value: 'category'


      A string identifying the data type.

      Will be used for display in, e.g. ``Series.dtype``


   .. py:attribute:: type

      The scalar type for the array, e.g. ``int``

      It's expected ``ExtensionArray[item]`` returns an instance
      of ``ExtensionDtype.type`` for scalar ``item``, assuming
      that value is valid (not NA). NA values do not need to be
      instances of `type`.


.. py:class:: ArkoudaFloat64Dtype

   Bases: :py:obj:`_ArkoudaBaseDtype`


   Arkouda-backed 64-bit floating-point dtype.

   This dtype integrates Arkouda's server-backed `pdarray<float64>` with
   the pandas ExtensionArray interface via :class:`ArkoudaArray`. It allows
   pandas objects (Series, DataFrame) to store and manipulate large
   distributed float64 arrays without materializing them on the client.

   .. method:: construct_array_type()

      Returns the :class:`ArkoudaArray` class used for storage.



   .. py:method:: construct_array_type()
      :classmethod:


      Return the ExtensionArray subclass that handles storage for this dtype.

      :returns: The :class:`ArkoudaArray` class associated with this dtype.
      :rtype: type



   .. py:attribute:: kind
      :value: 'f'


      A character code (one of 'biufcmMOSUV'), default 'O'

      This should match the NumPy dtype used when the array is
      converted to an ndarray, which is probably 'O' for object if
      the extension type cannot be represented as a built-in NumPy
      type.

      .. seealso:: :py:obj:`numpy.dtype.kind`


   .. py:attribute:: na_value

      Default NA value to use for this type.

      This is used in e.g. ExtensionArray.take. This should be the
      user-facing "boxed" version of the NA value, not the physical NA value
      for storage.  e.g. for JSONArray, this is an empty dictionary.


   .. py:attribute:: name
      :value: 'float64'


      A string identifying the data type.

      Will be used for display in, e.g. ``Series.dtype``


   .. py:attribute:: type

      The scalar type for the array, e.g. ``int``

      It's expected ``ExtensionArray[item]`` returns an instance
      of ``ExtensionDtype.type`` for scalar ``item``, assuming
      that value is valid (not NA). NA values do not need to be
      instances of `type`.


.. py:class:: ArkoudaInt64Dtype

   Bases: :py:obj:`_ArkoudaBaseDtype`


   Extension dtype for Arkouda-backed 64-bit integers.

   This dtype allows seamless use of Arkouda's distributed ``int64``
   arrays inside pandas objects (``Series``, ``Index``, ``DataFrame``).
   It is backed by :class:`arkouda.pdarray` with ``dtype='int64'``
   and integrates with pandas via the
   :class:`~arkouda.pandas.extension._arkouda_array.ArkoudaArray`
   extension array.

   .. method:: construct_array_type()

      Return the associated extension array class
      (:class:`ArkoudaArray`).



   .. py:method:: construct_array_type()
      :classmethod:


      Return the associated pandas ExtensionArray type.

      This is part of the pandas ExtensionDtype interface and is used
      internally by pandas when constructing arrays of this dtype.
      It ensures that operations like ``Series(..., dtype=ArkoudaInt64Dtype())``
      produce the correct Arkouda-backed extension array.

      :returns: The :class:`ArkoudaArray` class that implements the storage
                and behavior for this dtype.
      :rtype: type

      .. rubric:: Notes

      - This hook tells pandas which ExtensionArray to instantiate
        whenever this dtype is requested.
      - All Arkouda dtypes defined in this module will return
        :class:`ArkoudaArray` (or a subclass thereof).

      .. rubric:: Examples

      >>> from arkouda.pandas.extension import ArkoudaInt64Dtype
      >>> ArkoudaInt64Dtype.construct_array_type()
      <class 'arkouda.pandas.extension._arkouda_array.ArkoudaArray'>



   .. py:attribute:: kind
      :value: 'i'


      A character code (one of 'biufcmMOSUV'), default 'O'

      This should match the NumPy dtype used when the array is
      converted to an ndarray, which is probably 'O' for object if
      the extension type cannot be represented as a built-in NumPy
      type.

      .. seealso:: :py:obj:`numpy.dtype.kind`


   .. py:attribute:: na_value
      :value: -1


      Default NA value to use for this type.

      This is used in e.g. ExtensionArray.take. This should be the
      user-facing "boxed" version of the NA value, not the physical NA value
      for storage.  e.g. for JSONArray, this is an empty dictionary.


   .. py:attribute:: name
      :value: 'int64'


      A string identifying the data type.

      Will be used for display in, e.g. ``Series.dtype``


   .. py:attribute:: type

      The scalar type for the array, e.g. ``int``

      It's expected ``ExtensionArray[item]`` returns an instance
      of ``ExtensionDtype.type`` for scalar ``item``, assuming
      that value is valid (not NA). NA values do not need to be
      instances of `type`.


.. py:class:: ArkoudaStringArray(data: arkouda.numpy.strings.Strings | numpy.ndarray | Sequence[Any] | ArkoudaStringArray)

   Bases: :py:obj:`arkouda.pandas.extension._arkouda_extension_array.ArkoudaExtensionArray`, :py:obj:`pandas.api.extensions.ExtensionArray`


   Arkouda-backed string pandas ExtensionArray.

   Ensures the underlying data is an Arkouda ``Strings`` object. Accepts existing
   ``Strings`` or converts from NumPy arrays and Python sequences of strings.

   :param data: Input to wrap or convert.
                - If ``Strings``, used directly.
                - If NumPy/sequence, converted via ``ak.array``.
                - If another ``ArkoudaStringArray``, its backing ``Strings`` is reused.
   :type data: Strings | ndarray | Sequence[Any] | ArkoudaStringArray

   :raises TypeError: If ``data`` cannot be converted to Arkouda ``Strings``.

   .. attribute:: default_fill_value

      Sentinel used when filling missing values (default: "").

      :type: str


   .. py:method:: all(*args, **kwargs)


   .. py:method:: any(*args, **kwargs)


   .. py:method:: argpartition(*args, **kwargs)


   .. py:method:: astype(dtype: numpy.dtype[Any], copy: bool = True) -> numpy.typing.NDArray[Any]
                  astype(dtype: pandas.core.dtypes.dtypes.ExtensionDtype, copy: bool = True) -> pandas.api.extensions.ExtensionArray
                  astype(dtype: Any, copy: bool = True) -> Union[pandas.api.extensions.ExtensionArray, numpy.typing.NDArray[Any]]

      Cast to a specified dtype.

      Casting rules:

      * If ``dtype`` requests ``object``, returns a NumPy ``NDArray[Any]`` of dtype
        ``object`` containing the string values.
      * If ``dtype`` is a string dtype (e.g. pandas ``StringDtype``, NumPy unicode,
        or Arkouda string dtype), returns an ``ArkoudaStringArray``. If ``copy=True``,
        attempts to copy the underlying Arkouda ``Strings`` data.
      * For all other dtypes, casts the underlying Arkouda ``Strings`` using
        ``Strings.astype`` and returns an Arkouda-backed ``ArkoudaExtensionArray``
        constructed from the result.

      :param dtype: Target dtype. May be a NumPy dtype, pandas dtype, or Arkouda dtype.
      :type dtype: Any
      :param copy: Whether to force a copy when the result is an ``ArkoudaStringArray``.
                   Default is True.
      :type copy: bool

      :returns: The cast result. Returns a NumPy array only when casting to ``object``;
                otherwise returns an Arkouda-backed ExtensionArray.
      :rtype: Union[ExtensionArray, NDArray[Any]]

      .. rubric:: Examples

      Casting to a string dtype returns an Arkouda-backed string array:

      >>> import arkouda as ak
      >>> from arkouda.pandas.extension import ArkoudaStringArray
      >>> s = ArkoudaStringArray(ak.array(["a", "b", "c"]))
      >>> out = s.astype("string")
      >>> out is s
      False

      Forcing a copy when casting to a string dtype returns a new array:

      >>> out2 = s.astype("string", copy=True)
      >>> out2 is s
      False
      >>> out2.to_ndarray()
      array(['a', 'b', 'c'], dtype='<U1')

      Casting to ``object`` materializes the data to a NumPy array:

      >>> s.astype(object)
      array(['a', 'b', 'c'], dtype=object)

      Casting to a non-string dtype uses Arkouda to cast the underlying strings
      and returns an Arkouda-backed ExtensionArray:

      >>> s_num = ArkoudaStringArray(ak.array(["1", "2", "3"]))
      >>> a = s_num.astype("int64")
      >>> a.to_ndarray()
      array([1, 2, 3])

      NumPy and pandas dtype objects are also accepted:

      >>> import numpy as np
      >>> a = s_num.astype(np.dtype("float64"))
      >>> a.to_ndarray()
      array([1., 2., 3.])



   .. py:method:: byteswap(*args, **kwargs)


   .. py:method:: choose(*args, **kwargs)


   .. py:method:: clip(*args, **kwargs)


   .. py:method:: compress(*args, **kwargs)


   .. py:method:: conj(*args, **kwargs)


   .. py:method:: conjugate(*args, **kwargs)


   .. py:method:: cumprod(*args, **kwargs)


   .. py:method:: cumsum(*args, **kwargs)


   .. py:attribute:: default_fill_value
      :type:  str
      :value: ''



   .. py:method:: diagonal(*args, **kwargs)


   .. py:method:: dot(*args, **kwargs)


   .. py:property:: dtype

      An instance of ExtensionDtype.

      .. seealso::

         :py:obj:`api.extensions.ExtensionDtype`
             Base class for extension dtypes.

         :py:obj:`api.extensions.ExtensionArray`
             Base class for extension array types.

         :py:obj:`api.extensions.ExtensionArray.dtype`
             The dtype of an ExtensionArray.

         :py:obj:`Series.dtype`
             The dtype of a Series.

         :py:obj:`DataFrame.dtype`
             The dtype of a DataFrame.

      .. rubric:: Examples

      >>> pd.array([1, 2, 3]).dtype
      Int64Dtype()


   .. py:method:: dump(*args, **kwargs)


   .. py:method:: dumps(*args, **kwargs)


   .. py:method:: fill(*args, **kwargs)


   .. py:method:: flatten(*args, **kwargs)


   .. py:method:: getfield(*args, **kwargs)


   .. py:method:: isna()

      A 1-D array indicating if each value is missing.

      :returns: In most cases, this should return a NumPy ndarray. For
                exceptional cases like ``SparseArray``, where returning
                an ndarray would be expensive, an ExtensionArray may be
                returned.
      :rtype: numpy.ndarray or pandas.api.extensions.ExtensionArray

      .. seealso::

         :py:obj:`ExtensionArray.dropna`
             Return ExtensionArray without NA values.

         :py:obj:`ExtensionArray.fillna`
             Fill NA/NaN values using the specified method.

      .. rubric:: Notes

      If returning an ExtensionArray, then

      * ``na_values._is_boolean`` should be True
      * ``na_values`` should implement :func:`ExtensionArray._reduce`
      * ``na_values`` should implement :func:`ExtensionArray._accumulate`
      * ``na_values.any`` and ``na_values.all`` should be implemented

      .. rubric:: Examples

      >>> arr = pd.array([1, 2, np.nan, np.nan])
      >>> arr.isna()
      array([False, False,  True,  True])



   .. py:method:: item(*args, **kwargs)

      Return the array element at the specified position as a Python scalar.

      :param index: Position of the element. If not provided, the array must contain
                    exactly one element.
      :type index: int, optional

      :returns: The element at the specified position.
      :rtype: scalar

      :raises ValueError: If no index is provided and the array does not have exactly
          one element.
      :raises IndexError: If the specified position is out of bounds.

      .. seealso::

         :py:obj:`numpy.ndarray.item`
             Return the item of an array as a scalar.

      .. rubric:: Examples

      >>> arr = pd.array([1], dtype="Int64")
      >>> arr.item()
      np.int64(1)

      >>> arr = pd.array([1, 2, 3], dtype="Int64")
      >>> arr.item(0)
      np.int64(1)
      >>> arr.item(2)
      np.int64(3)



   .. py:method:: max(*args, **kwargs)


   .. py:method:: mean(*args, **kwargs)


   .. py:method:: min(*args, **kwargs)


   .. py:method:: nonzero(*args, **kwargs)


   .. py:method:: partition(*args, **kwargs)


   .. py:method:: prod(*args, **kwargs)


   .. py:method:: put(*args, **kwargs)


   .. py:method:: resize(*args, **kwargs)


   .. py:method:: round(*args, **kwargs)


   .. py:method:: setfield(*args, **kwargs)


   .. py:method:: setflags(*args, **kwargs)


   .. py:method:: sort(*args, **kwargs)


   .. py:method:: std(*args, **kwargs)


   .. py:method:: sum(*args, **kwargs)


   .. py:method:: swapaxes(*args, **kwargs)


   .. py:method:: to_device(*args, **kwargs)


   .. py:method:: tobytes(*args, **kwargs)


   .. py:method:: tofile(*args, **kwargs)


   .. py:method:: trace(*args, **kwargs)


   .. py:method:: value_counts(dropna: bool = True) -> pandas.Series

      Return counts of unique strings as a pandas Series.

      This method computes the frequency of each distinct string value in the
      underlying Arkouda ``Strings`` object and returns the result as a pandas
      ``Series``, with the unique string values as the index and their counts
      as the data.

      :param dropna: Whether to exclude missing values. Missing-value handling for
                     Arkouda string arrays is not yet implemented, so this parameter is
                     accepted for pandas compatibility but currently has no effect.
                     Default is True.
      :type dropna: bool

      :returns: A Series containing the counts of unique string values.
                The index is an ``ArkoudaStringArray`` of unique values, and the
                values are an ``ArkoudaArray`` of counts.
      :rtype: pd.Series

      .. rubric:: Notes

      - The following pandas options are not yet implemented:
        ``normalize``, ``sort``, and ``bins``.
      - Counting is performed server-side in Arkouda; only the small result
        (unique values and counts) is materialized on the client.

      .. rubric:: Examples

      Basic usage:

      >>> import arkouda as ak
      >>> from arkouda.pandas.extension import ArkoudaStringArray
      >>>
      >>> s = ArkoudaStringArray(["red", "blue", "red", "green", "blue", "red"])
      >>> s.value_counts()
      red      3
      blue     2
      green    1
      dtype: int64

      Empty input:

      >>> empty = ArkoudaStringArray([])
      >>> empty.value_counts()
      Series([], dtype: int64)



   .. py:method:: var(*args, **kwargs)


.. py:class:: ArkoudaStringDtype

   Bases: :py:obj:`_ArkoudaBaseDtype`


   Arkouda-backed string dtype.

   This dtype integrates Arkouda's distributed ``Strings`` type with the
   pandas ExtensionArray interface via :class:`ArkoudaStringArray`. It
   enables pandas objects (Series, DataFrame) to hold large, server-backed
   string columns without converting to NumPy or Python objects.

   .. method:: construct_array_type()

      Returns the :class:`ArkoudaStringArray` used as the storage class.



   .. py:method:: construct_array_type()
      :classmethod:


      Return the ExtensionArray subclass that handles storage for this dtype.

      :returns: The :class:`ArkoudaStringArray` class associated with this dtype.
      :rtype: type



   .. py:attribute:: kind
      :value: 'O'


      A character code (one of 'biufcmMOSUV'), default 'O'

      This should match the NumPy dtype used when the array is
      converted to an ndarray, which is probably 'O' for object if
      the extension type cannot be represented as a built-in NumPy
      type.

      .. seealso:: :py:obj:`numpy.dtype.kind`


   .. py:attribute:: na_value
      :value: ''


      Default NA value to use for this type.

      This is used in e.g. ExtensionArray.take. This should be the
      user-facing "boxed" version of the NA value, not the physical NA value
      for storage.  e.g. for JSONArray, this is an empty dictionary.


   .. py:attribute:: name
      :value: 'string'


      A string identifying the data type.

      Will be used for display in, e.g. ``Series.dtype``


   .. py:attribute:: type

      The scalar type for the array, e.g. ``int``

      It's expected ``ExtensionArray[item]`` returns an instance
      of ``ExtensionDtype.type`` for scalar ``item``, assuming
      that value is valid (not NA). NA values do not need to be
      instances of `type`.


.. py:class:: ArkoudaUint64Dtype

   Bases: :py:obj:`_ArkoudaBaseDtype`


   Arkouda-backed unsigned 64-bit integer dtype.

   This dtype integrates Arkoudaâ€™s ``uint64`` arrays with pandas,
   allowing users to create :class:`pandas.Series` or
   :class:`pandas.DataFrame` objects that store their data on
   the Arkouda server while still conforming to the pandas
   ExtensionArray API.

   .. method:: construct_array_type()

      Return the :class:`ArkoudaArray` class used as the storage
      container for this dtype.


   .. rubric:: Examples

   >>> import arkouda as ak
   >>> import pandas as pd
   >>> from arkouda.pandas.extension import ArkoudaUint64Dtype, ArkoudaArray

   >>> arr = ArkoudaArray(ak.array([1, 2, 3], dtype="uint64"))
   >>> s = pd.Series(arr, dtype=ArkoudaUint64Dtype())
   >>> s
   0    1
   1    2
   2    3
   dtype: uint64


   .. py:method:: construct_array_type()
      :classmethod:


      Return the ExtensionArray class associated with this dtype.

      This is required by the pandas ExtensionDtype API. It tells pandas
      which :class:`~pandas.api.extensions.ExtensionArray` subclass should
      be used to hold data of this dtype inside a :class:`pandas.Series`
      or :class:`pandas.DataFrame`.

      :returns: The :class:`ArkoudaArray` class, which implements the storage
                and operations for Arkouda-backed arrays.
      :rtype: type



   .. py:attribute:: kind
      :value: 'u'


      A character code (one of 'biufcmMOSUV'), default 'O'

      This should match the NumPy dtype used when the array is
      converted to an ndarray, which is probably 'O' for object if
      the extension type cannot be represented as a built-in NumPy
      type.

      .. seealso:: :py:obj:`numpy.dtype.kind`


   .. py:attribute:: na_value
      :value: -1


      Default NA value to use for this type.

      This is used in e.g. ExtensionArray.take. This should be the
      user-facing "boxed" version of the NA value, not the physical NA value
      for storage.  e.g. for JSONArray, this is an empty dictionary.


   .. py:attribute:: name
      :value: 'uint64'


      A string identifying the data type.

      Will be used for display in, e.g. ``Series.dtype``


   .. py:attribute:: type

      The scalar type for the array, e.g. ``int``

      It's expected ``ExtensionArray[item]`` returns an instance
      of ``ExtensionDtype.type`` for scalar ``item``, assuming
      that value is valid (not NA). NA values do not need to be
      instances of `type`.


.. py:class:: ArkoudaUint8Dtype

   Bases: :py:obj:`_ArkoudaBaseDtype`


   Arkouda-backed unsigned 8-bit integer dtype.

   This dtype integrates Arkouda's ``uint8`` arrays with the pandas
   ExtensionArray API, allowing pandas ``Series`` and ``DataFrame``
   objects to store and operate on Arkouda-backed unsigned 8-bit
   integers. The underlying storage is an Arkouda ``pdarray<uint8>``,
   exposed through the :class:`ArkoudaArray` extension array.

   .. method:: construct_array_type()

      Returns the :class:`ArkoudaArray` type that provides the storage
      and behavior for this dtype.



   .. py:method:: construct_array_type()
      :classmethod:


      Return the ExtensionArray subclass that handles storage for this dtype.

      This method is required by the pandas ExtensionDtype interface.
      It tells pandas which ExtensionArray class to use when creating
      arrays of this dtype (for example, when calling
      ``Series(..., dtype="arkouda.uint8")``).

      :returns: The :class:`ArkoudaArray` class associated with this dtype.
      :rtype: type



   .. py:attribute:: kind
      :value: 'u'


      A character code (one of 'biufcmMOSUV'), default 'O'

      This should match the NumPy dtype used when the array is
      converted to an ndarray, which is probably 'O' for object if
      the extension type cannot be represented as a built-in NumPy
      type.

      .. seealso:: :py:obj:`numpy.dtype.kind`


   .. py:attribute:: na_value
      :value: -1


      Default NA value to use for this type.

      This is used in e.g. ExtensionArray.take. This should be the
      user-facing "boxed" version of the NA value, not the physical NA value
      for storage.  e.g. for JSONArray, this is an empty dictionary.


   .. py:attribute:: name
      :value: 'uint8'


      A string identifying the data type.

      Will be used for display in, e.g. ``Series.dtype``


   .. py:attribute:: type

      The scalar type for the array, e.g. ``int``

      It's expected ``ExtensionArray[item]`` returns an instance
      of ``ExtensionDtype.type`` for scalar ``item``, assuming
      that value is valid (not NA). NA values do not need to be
      instances of `type`.


.. py:class:: CachedAccessor(name: str, accessor)

   Descriptor for caching namespace-based accessors.

   This custom property-like object enables lazy initialization of accessors
   (e.g., `.str`, `.dt`) on Series-like objects, similar to pandas-style extension
   accessors.

   :param name: The name of the namespace to be accessed (e.g., ``df.foo``).
   :type name: str
   :param accessor: A class implementing the accessor logic.
   :type accessor: type

   .. rubric:: Notes

   The `accessor` class's ``__init__`` method must accept a single positional
   argument, which should be one of ``Series``, ``DataFrame``, or ``Index``.


.. py:class:: Categorical(values, **kwargs)

   Represents an array of values belonging to named categories.

   Converting a Strings object to Categorical often saves memory and speeds up operations,
   especially if there are many repeated values, at the cost of some one-time
   work in initialization.

   :param values: Values to convert to categories
   :type values: Strings, Categorical, pd.Categorical
   :param na_value: The value to use to represent missing/null data
   :type na_value: str scalar

   .. attribute:: categories

      The set of category labels (determined automatically)

      :type: Strings

   .. attribute:: codes

      The category indices of the values or -1 for N/A

      :type: pdarray, int64

   .. attribute:: permutation

      The permutation that groups the values in the same order as categories

      :type: pdarray, int64

   .. attribute:: segments

      When values are grouped, the starting offset of each group

      :type: Union[pdarray, None]

   .. attribute:: size

      The number of items in the array

      :type: int_scalars

   .. attribute:: nlevels

      The number of distinct categories

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      The sizes of each dimension of the array

      :type: tuple


   .. py:attribute:: BinOps


   .. py:property:: NAvalue

      Deprecated alias for `na_value`.

      :rtype: Same value as `na_value`.


   .. py:attribute:: RegisterablePieces


   .. py:attribute:: RequiredPieces


   .. py:method:: argsort(algorithm: arkouda.numpy.sorting.SortingAlgorithm = SortingAlgorithm.RadixSortLSD, ascending: bool = True) -> arkouda.numpy.pdarrayclass.pdarray

      Return the permutation of indices that would sort the Categorical.

      Sorting is based on the order of the Categorical's `categories`,
      not on the underlying codes.

      :param algorithm: The sorting algorithm to use.
      :type algorithm: SortingAlgorithm, default SortingAlgorithm.RadixSortLSD
      :param ascending: Whether to return indices that would sort the Categorical in ascending
                        category order. If False, returns indices for descending order.
      :type ascending: bool, default True

      :returns: An array of indices such that `self[index]` is sorted by category order.
      :rtype: pdarray

      .. rubric:: Examples

      >>> import arkouda as ak

      >>> cat = ak.Categorical(ak.array(['dog', 'cat', 'dog', 'bird']))
      >>> cat.argsort()
      array([3 1 0 2])

      >>> cat.argsort(ascending=False)
      array([2 0 1 3])

      The result can be used to reorder the Categorical:
      >>> sorted_cat = cat[cat.argsort()]
      >>> sorted_cat
      array(['bird', 'cat', 'dog', 'dog'])



   .. py:attribute:: categories
      :type:  arkouda.numpy.strings.Strings


   .. py:attribute:: codes
      :type:  arkouda.numpy.pdarrayclass.pdarray


   .. py:method:: concatenate(others: Sequence[Categorical], ordered: bool = True) -> Categorical

      Merge this Categorical with other Categorical objects in the array.

      Merge this Categorical with other Categorical objects in the array,
      concatenating the arrays and synchronizing the categories.

      :param others: The Categorical arrays to concatenate and merge with this one
      :type others: Sequence[Categorical]
      :param ordered: If True (default), the arrays will be appended in the
                      order given. If False, array data may be interleaved
                      in blocks, which can greatly improve performance but
                      results in non-deterministic ordering of elements.
      :type ordered: bool

      :returns: The merged Categorical object
      :rtype: Categorical

      :raises TypeError: Raised if any others array objects are not Categorical objects

      .. rubric:: Notes

      This operation can be expensive -- slower than concatenating Strings.



   .. py:method:: contains(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.numpy.pdarrayclass.pdarray

      Check whether each element contains the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that contain substr, False otherwise
      :rtype: pdarray

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :py:obj:`Categorical.startswith`, :py:obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:method:: copy() -> Categorical

      Return an copy of the given Categorical.

      :returns: A deep copy of the Categorical.
      :rtype: Categorical



   .. py:attribute:: dtype


   .. py:method:: endswith(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.numpy.pdarrayclass.pdarray

      Check whether each element ends with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that end with substr, False otherwise
      :rtype: pdarray

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :py:obj:`Categorical.startswith`, :py:obj:`Categorical.contains`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether Categoricals are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the Categoricals are the same, o.w. False.
      :rtype: bool_scalars

      .. rubric:: Examples

      >>> import arkouda as ak

      >>> c = Categorical(ak.array(["a", "b", "c"]))
      >>> c_cpy = Categorical(ak.array(["a", "b", "c"]))
      >>> c.equals(c_cpy)
      np.True_
      >>> c2 = Categorical(ak.array(["a", "x", "c"]))
      >>> c.equals(c2)
      np.False_



   .. py:method:: from_codes(codes: arkouda.numpy.pdarrayclass.pdarray, categories: arkouda.numpy.strings.Strings, permutation=None, segments=None, **kwargs) -> Categorical
      :classmethod:


      Make a Categorical from codes and categories arrays.

      If codes and
      categories have already been pre-computed, this constructor saves
      time. If not, please use the normal constructor.

      :param codes: Category indices of each value
      :type codes: pdarray, int64
      :param categories: Unique category labels
      :type categories: Strings
      :param permutation: The permutation that groups the values in the same order
                          as categories
      :type permutation: pdarray, int64
      :param segments: When values are grouped, the starting offset of each group
      :type segments: pdarray, int64

      :returns: The Categorical object created from the input parameters
      :rtype: Categorical

      :raises TypeError: Raised if codes is not a pdarray of int64 objects or if
          categories is not a Strings object



   .. py:method:: from_return_msg(rep_msg) -> Categorical
      :classmethod:


      Create categorical from return message from server.

      .. rubric:: Notes

      This is currently only used when reading a Categorical from HDF5 files.



   .. py:method:: group() -> arkouda.numpy.pdarrayclass.pdarray

      Return the permutation that groups the array, placing equivalent categories together.

      All instances of the same category are guaranteed
      to lie in one contiguous block of the permuted array, but the blocks
      are not necessarily ordered.

      :returns: The permutation that groups the array by value
      :rtype: pdarray

      .. seealso:: :py:obj:`GroupBy`, :py:obj:`unique`

      .. rubric:: Notes

      This method is faster than the corresponding Strings method. If the
      Categorical was created from a Strings object, then this function
      simply returns the cached permutation. Even if the Categorical was
      created using from_codes(), this function will be faster than
      Strings.group() because it sorts dense integer values, rather than
      128-bit hash values.



   .. py:method:: hash() -> Tuple[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.pdarrayclass.pdarray]

      Compute a 128-bit hash of each element of the Categorical.

      :returns: A tuple of two int64 pdarrays. The ith hash value is the concatenation
                of the ith values from each array.
      :rtype: Tuple[pdarray,pdarray]

      .. rubric:: Notes

      The implementation uses SipHash128, a fast and balanced hash function (used
      by Python for dictionaries and sets). For realistic numbers of strings (up
      to about 10**15), the probability of a collision between two 128-bit hash
      values is negligible.



   .. py:method:: in1d(test: Union[arkouda.numpy.strings.Strings, Categorical]) -> arkouda.numpy.pdarrayclass.pdarray

      Whether each element is also present in the test Strings or Categorical object.

      Returns a boolean array the same length as `self` that is True
      where an element of `self` is in `test` and False otherwise.

      :param test: The values against which to test each value of 'self`.
      :type test: Union[Strings,Categorical]

      :returns: The values `self[in1d]` are in the `test` Strings or Categorical object.
      :rtype: pdarray

      :raises TypeError: Raised if test is not a Strings or Categorical object

      .. seealso:: :py:obj:`unique`, :py:obj:`intersect1d`, :py:obj:`union1d`

      .. rubric:: Notes

      `in1d` can be considered as an element-wise function version of the
      python keyword `in`, for 1-D sequences. ``in1d(a, b)`` is logically
      equivalent to ``ak.array([item in b for item in a])``, but is much
      faster and scales to arbitrarily large ``a``.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> strings = ak.array([f'String {i}' for i in range(0,5)])
      >>> cat = ak.Categorical(strings)
      >>> ak.in1d(cat,strings)
      array([True True True True True])
      >>> strings = ak.array([f'String {i}' for i in range(5,9)])
      >>> catTwo = ak.Categorical(strings)
      >>> ak.in1d(cat,catTwo)
      array([False False False False False])



   .. py:property:: inferred_type
      :type: str


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Return a JSON formatted string containing information about all components of self.

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry or is a component of a registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :py:obj:`register`, :py:obj:`attach`, :py:obj:`unregister`, :py:obj:`unregister_categorical_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isna()

      Find where values are missing or null (as defined by self.na_value).



   .. py:attribute:: logger


   .. py:property:: nbytes

      The size of the Categorical in bytes.

      :returns: The size of the Categorical in bytes.
      :rtype: int


   .. py:attribute:: ndim
      :type:  arkouda.numpy.dtypes.int_scalars


   .. py:attribute:: nlevels
      :type:  arkouda.numpy.dtypes.int_scalars


   .. py:attribute:: objType
      :value: 'Categorical'



   .. py:attribute:: permutation
      :type:  Union[arkouda.numpy.pdarrayclass.pdarray, None]


   .. py:method:: pretty_print_info() -> None

      Print information about all components of self in a human-readable format.



   .. py:method:: register(user_defined_name: str) -> Categorical

      Register this Categorical object and underlying components with the Arkouda server.

      :param user_defined_name: user defined name the Categorical is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Categorical which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Categoricals with the same name.
      :rtype: Categorical

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Categorical with the user_defined_name

      .. seealso:: :py:obj:`unregister`, :py:obj:`attach`, :py:obj:`unregister_categorical_by_name`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reset_categories() -> Categorical

      Recompute the category labels, discarding any unused labels.

      This method is often useful after slicing or indexing a Categorical array,
      when the resulting array only contains a subset of the original
      categories. In this case, eliminating unused categories can speed up
      other operations.

      :returns: A Categorical object generated from the current instance
      :rtype: Categorical



   .. py:attribute:: segments
      :type:  Union[arkouda.numpy.pdarrayclass.pdarray, None]


   .. py:method:: set_categories(new_categories, na_value=None)

      Set categories to user-defined values.

      :param new_categories: The array of new categories to use. Must be unique.
      :type new_categories: Strings
      :param na_value: The value to use to represent missing/null data
      :type na_value: str scalar

      :returns: A new Categorical with the user-defined categories. Old values present
                in new categories will appear unchanged. Old values not present will
                be assigned the NA value.
      :rtype: Categorical



   .. py:attribute:: shape
      :type:  tuple


   .. py:attribute:: size
      :type:  arkouda.numpy.dtypes.int_scalars


   .. py:method:: sort_values()

      Return a sorted Categorical by category labels.

      :returns: A new Categorical with values sorted by category.
      :rtype: Categorical



   .. py:method:: standardize_categories(arrays, na_value='N/A')
      :classmethod:


      Standardize an array of Categoricals so that they share the same categories.

      :param arrays: The Categoricals to standardize
      :type arrays: sequence of Categoricals
      :param na_value: The value to use to represent missing/null data
      :type na_value: str scalar

      :returns: A list of the original Categoricals remapped to the shared categories.
      :rtype: List of Categoricals



   .. py:method:: startswith(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.numpy.pdarrayclass.pdarray

      Check whether each element starts with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that start with substr, False otherwise
      :rtype: pdarray

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :py:obj:`Categorical.contains`, :py:obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding
      method on Strings objects, because it searches the unique category
      labels instead of the full array.



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'categorical_array', mode: Literal['truncate', 'append'] = 'truncate', file_type: Literal['single', 'distribute'] = 'distribute')

      Save the Categorical to HDF5.

      The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: {'truncate', 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
      :type file_type: {"single", "distribute"}

      .. seealso:: :py:obj:`load`



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray.

      Convert the array to a np.ndarray, transferring array data from
      the arkouda server to Python. This conversion discards category
      information and produces an ndarray of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A numpy ndarray of strings corresponding to the values in
                this array
      :rtype: np.ndarray

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``ak.core.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.core.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_pandas() -> pandas.Categorical

      Return the equivalent Pandas Categorical.



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'categorical_array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      [Not Yet Implemented] Save the Categorical to a Parquet dataset.

      !!! This method is currently not supported and will raise a RuntimeError. !!!
      Parquet support for Categorical is under development.

      When implemented, this method will write the Categorical to a set of Parquet
      files, one file per locale on the Arkouda server. Each file will be named
      using the `prefix_path` with locale-specific suffixes.

      :param prefix_path: The directory and filename prefix shared by all output files.
      :type prefix_path: str
      :param dataset: The dataset name to use to create the Parquet files.
      :type dataset: str, default="categorical_array"
      :param mode: Specifies write behavior. Use 'truncate' to overwrite existing files or
                   'append' to add to them. (Appending is not yet efficient.)
      :type mode: {'truncate', 'append'}, default='truncate'
      :param compression: Compression algorithm to use when writing the file.
                          Supported values include: 'snappy', 'gzip', 'brotli', 'zstd', 'lz4'.
                          Default is None (no compression).
      :type compression: str, optional

      :returns: A message indicating the result of the operation.
      :rtype: str

      :raises RuntimeError: Always raised. Parquet export for Categorical is not yet supported.

      .. rubric:: Notes

      - The specified `prefix_path` must be writable and accessible to the Arkouda server.
      - The user must have write permission.
      - Output files will be named as ``<prefix_path>_LOCALE<i>`` for each locale `i`.
      - Appending mode requires that the existing files already match the serverâ€™s locale layout.
      - Appending mode is supported, but is not efficient.
      - File extensions are not used to determine file type.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.

      .. seealso::

         :py:obj:`to_hdf`
             Save the Categorical to HDF5 format (currently supported).



   .. py:method:: to_strings() -> arkouda.numpy.strings.Strings

      Convert the Categorical to Strings.

      :returns: A Strings object corresponding to the values in
                this Categorical.
      :rtype: Strings

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> a = ak.array(["a","b","c"])
      >>> a
      array(['a', 'b', 'c'])
      >>> c = ak.Categorical(a)
      >>> c.to_strings()
      array(['a', 'b', 'c'])

      >>> isinstance(c.to_strings(), ak.Strings)
      True



   .. py:method:: tolist() -> List[str]

      Convert the Categorical to a list.

      Convert the Categorical to a list, transferring data from
      the arkouda server to Python. This conversion discards category
      information and produces a list of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A list of strings corresponding to the values in
                this Categorical
      :rtype: List[str]

      .. rubric:: Notes

      The number of bytes in the Categorical cannot exceed ``ak.core.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.core.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Send a Categorical object to a different Arkouda server.

      :param hostname: The hostname where the Arkouda server intended to
                       receive the Categorical is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unique() -> Categorical

      Return the unique category values in the Categorical.

      :returns: A new Categorical containing only the unique category labels in use.
      :rtype: Categorical



   .. py:method:: unregister() -> None

      Unregister this Categorical object.

      Unregister this Categorical object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :py:obj:`register`, :py:obj:`attach`, :py:obj:`unregister_categorical_by_name`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: update_hdf(prefix_path, dataset='categorical_array', repack=True)

      Overwrite the dataset with the name provided with this Categorical object.

      If the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :raises RuntimeError: Raised if a server-side error is thrown saving the Categorical

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, the repack option allows for
        automatic creation of a file without the inaccessible data.



.. py:class:: DataFrame(initialdata=None, index=None, columns=None)

   Bases: :py:obj:`collections.UserDict`


   A DataFrame structure based on arkouda arrays.

   :param initialdata: Each list/dictionary entry corresponds to one column of the data and
                       should be a homogenous type. Different columns may have different
                       types. If using a dictionary, keys should be strings.
   :type initialdata: List or dictionary of lists, tuples, or pdarrays
   :param index: Index for the resulting frame. Defaults to an integer range.
   :type index: Index, pdarray, or Strings
   :param columns: Column labels to use if the data does not include them. Elements must
                   be strings. Defaults to an stringified integer range.
   :type columns: List, tuple, pdarray, or Strings

   .. rubric:: Examples

   >>> import arkouda as ak

   Create an empty DataFrame and add a column of data:
   >>> import arkouda as ak
   >>> df = ak.DataFrame()
   >>> df['a'] = ak.array([1,2,3])
   >>> df
      a
   0  1
   1  2
   2  3 (3 rows x 1 columns)

   Create a new DataFrame using a dictionary of data:

   >>> userName = ak.array(['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice'])
   >>> userID = ak.array([111, 222, 111, 333, 222, 111])
   >>> item = ak.array([0, 0, 1, 1, 2, 0])
   >>> day = ak.array([5, 5, 6, 5, 6, 6])
   >>> amount = ak.array([0.5, 0.6, 1.1, 1.2, 4.3, 0.6])
   >>> df = ak.DataFrame({
   ...     'userName': userName,
   ...     'userID': userID,
   ...     'item': item,
   ...     'day': day,
   ...     'amount': amount
   ... })
   >>> df
     userName  userID  item  day  amount
   0    Alice     111     0    5     0.5
   1      Bob     222     0    5     0.6
   2    Alice     111     1    6     1.1
   3    Carol     333     1    5     1.2
   4      Bob     222     2    6     4.3
   5    Alice     111     0    6     0.6 (6 rows x 5 columns)

   Indexing works slightly differently than with pandas:
   >>> df[0]
   {'userName': np.str_('Alice'), 'userID': np.int64(111), 'item': np.int64(0),
   'day': np.int64(5), 'amount': np.float64(0.5)}
   >>> df['userID']
   array([111 222 111 333 222 111])

   >>> df['userName']
   array(['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice'])

   >>> df[ak.array([1,3,5])]
     userName  userID  item  day  amount
   1      Bob     222     0    5     0.6
   3    Carol     333     1    5     1.2
   5    Alice     111     0    6     0.6 (3 rows x 5 columns)

   Compute the stride:
   >>> df[1:5:1]
     userName  userID  item  day  amount
   1      Bob     222     0    5     0.6
   2    Alice     111     1    6     1.1
   3    Carol     333     1    5     1.2
   4      Bob     222     2    6     4.3 (4 rows x 5 columns)

   >>> df[ak.array([1,2,3])]
     userName  userID  item  day  amount
   1      Bob     222     0    5     0.6
   2    Alice     111     1    6     1.1
   3    Carol     333     1    5     1.2 (3 rows x 5 columns)

   >>> df[['userID', 'day']]
      userID  day
   0     111    5
   1     222    5
   2     111    6
   3     333    5
   4     222    6
   5     111    6 (6 rows x 2 columns)


   .. py:method:: all(axis=0) -> Union[arkouda.pandas.series.Series, bool]

      Return whether all elements are True, potentially over an axis.

      Returns True unless there at least one element along a Dataframe axis that is False.

      Currently, will ignore any columns that are not type bool.
      This is equivalent to the pandas option bool_only=True.

      :param axis: Indicate which axis or axes should be reduced.

                   0 / â€˜indexâ€™ : reduce the index, return a Series whose index is the original column labels.

                   1 / â€˜columnsâ€™ : reduce the columns, return a Series whose index is the original index.

                   None : reduce all axes, return a scalar.
      :type axis: {0 or â€˜indexâ€™, 1 or â€˜columnsâ€™, None}, default = 0

      :rtype: arkouda.pandas.series.Series or bool

      :raises ValueError: Raised if axis does not have a value in {0 or â€˜indexâ€™, 1 or â€˜columnsâ€™, None}.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"A":[True,True,True,False],"B":[True,True,True,False],
      ...          "C":[True,False,True,False],"D":[True,True,True,True]})
      >>> df
             A      B      C     D
      0   True   True   True  True
      1   True   True  False  True
      2   True   True   True  True
      3  False  False  False  True (4 rows x 4 columns)

      >>> df.all(axis=0)
      A    False
      B    False
      C    False
      D     True
      dtype: bool
      >>> df.all(axis=1)
      0     True
      1    False
      2     True
      3    False
      dtype: bool
      >>> df.all(axis=None)
      False



   .. py:method:: any(axis=0) -> Union[arkouda.pandas.series.Series, bool]

      Return whether any element is True, potentially over an axis.

      Returns False unless there is at least one element along a Dataframe axis that is True.

      Currently, will ignore any columns that are not type bool.
      This is equivalent to the pandas option bool_only=True.

      :param axis: Indicate which axis or axes should be reduced.

                   0 / â€˜indexâ€™ : reduce the index, return a Series whose index is the original column labels.

                   1 / â€˜columnsâ€™ : reduce the columns, return a Series whose index is the original index.

                   None : reduce all axes, return a scalar.
      :type axis: {0 or â€˜indexâ€™, 1 or â€˜columnsâ€™, None}, default = 0

      :rtype: arkouda.pandas.series.Series or bool

      :raises ValueError: Raised if axis does not have a value in {0 or â€˜indexâ€™, 1 or â€˜columnsâ€™, None}.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"A":[True,True,True,False],"B":[True,True,True,False],
      ...          "C":[True,False,True,False],"D":[False,False,False,False]})
      >>> df
             A      B      C      D
      0   True   True   True  False
      1   True   True  False  False
      2   True   True   True  False
      3  False  False  False  False (4 rows x 4 columns)

      >>> df.any(axis=0)
      A     True
      B     True
      C     True
      D    False
      dtype: bool
      >>> df.any(axis=1)
      0     True
      1     True
      2     True
      3    False
      dtype: bool
      >>> df.any(axis=None)
      True



   .. py:method:: append(other, ordered=True)

      Concatenate data from 'other' onto the end of this DataFrame, in place.

      Explicitly, use the arkouda concatenate function to append the data
      from each column in other to the end of self. This operation is done
      in place, in the sense that the underlying pdarrays are updated from
      the result of the arkouda concatenate function, rather than returning
      a new DataFrame object containing the result.

      :param other: The DataFrame object whose data will be appended to this DataFrame.
      :type other: DataFrame
      :param ordered: If False, allow rows to be interleaved for better performance (but
                      data within a row remains together). By default, append all rows
                      to the end, in input order.
      :type ordered: bool, default=True

      :returns: Appending occurs in-place, but result is returned for compatibility.
      :rtype: self

      .. rubric:: Examples

      >>> import arkouda as ak

      >>> df1 = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df1
          col1  col2
      0     1     3
      1     2     4 (2 rows x 2 columns)

      >>> df2 = ak.DataFrame({'col1': [3], 'col2': [5]})
      >>> df2
          col1  col2
      0     3     5 (1 rows x 2 columns)

      >>> df1.append(df2)
          col1  col2
      0     1     3
      1     2     4
      2     3     5 (3 rows x 2 columns)

      >>> df1
          col1  col2
      0     1     3
      1     2     4
      2     3     5 (3 rows x 2 columns)



   .. py:method:: apply_permutation(perm)

      Apply a permutation to an entire DataFrame.

      The operation is done in place and the original DataFrame will be modified.

      This may be useful if you want to unsort an DataFrame, or even to
      apply an arbitrary permutation such as the inverse of a sorting
      permutation.

      :param perm: A permutation array. Should be the same size as the data
                   arrays, and should consist of the integers [0,size-1] in
                   some order. Very minimal testing is done to ensure this
                   is a permutation.
      :type perm: pdarray

      .. seealso:: :py:obj:`sort`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df
         col1  col2
      0     1     4
      1     2     5
      2     3     6 (3 rows x 2 columns)

      >>> perm_arry = ak.array([0, 2, 1])
      >>> df.apply_permutation(perm_arry)
      >>> df
         col1  col2
      0     1     4
      2     3     6
      1     2     5 (3 rows x 2 columns)



   .. py:method:: argsort(key, ascending=True)

      Return the permutation that sorts the dataframe by `key`.

      :param key: The key to sort on.
      :type key: str
      :param ascending: If true, sort the key in ascending order.
                        Otherwise, sort the key in descending order.
      :type ascending: bool, default = True

      :returns: The permutation array that sorts the data on `key`.
      :rtype: arkouda.numpy.pdarrayclass.pdarray

      .. seealso:: :py:obj:`coargsort`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1.1, 3.1, 2.1], 'col2': [6, 5, 4]})
      >>> df
         col1  col2
      0   1.1     6
      1   3.1     5
      2   2.1     4 (3 rows x 2 columns)

      >>> df.argsort('col1')
      array([0 2 1])
      >>> sorted_df1 = df[df.argsort('col1')]
      >>> sorted_df1
         col1  col2
      0   1.1     6
      2   2.1     4
      1   3.1     5 (3 rows x 2 columns)

      >>> df.argsort('col2')
      array([2 1 0])
      >>> sorted_df2 = df[df.argsort('col2')]
      >>> sorted_df2
         col1  col2
      2   2.1     4
      1   3.1     5
      0   1.1     6 (3 rows x 2 columns)



   .. py:method:: assign(**kwargs) -> DataFrame

      Assign new columns to a DataFrame.

      Return a new object with all original columns in addition to new ones.
      Existing columns that are re-assigned will be overwritten.

      :param \*\*kwargs: The column names are keywords. If the values are
                         callable, they are computed on the DataFrame and
                         assigned to the new columns. The callable must not
                         change input DataFrame (though pandas doesn't check it).
                         If the values are not callable, (e.g. a Series, scalar, or array),
                         they are simply assigned.
      :type \*\*kwargs: dict of {str: callable or Series}

      :returns: A new DataFrame with the new columns in addition to
                all the existing columns.
      :rtype: DataFrame

      .. rubric:: Notes

      Assigning multiple columns within the same ``assign`` is possible.
      Later items in '\*\*kwargs' may refer to newly created or modified
      columns in 'df'; items are computed and assigned into 'df' in order.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'temp_c': [17.0, 25.0]},
      ...                   index=['Portland', 'Berkeley'])
      >>> df
                temp_c
      Portland    17.0
      Berkeley    25.0 (2 rows x 1 columns)

      Where the value is a callable, evaluated on `df`:
      >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)
                temp_c  temp_f
      Portland    17.0    62.6
      Berkeley    25.0    77.0 (2 rows x 2 columns)

      Alternatively, the same behavior can be achieved by directly
      referencing an existing Series or sequence:

      >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)
                temp_c  temp_f
      Portland    17.0    62.6
      Berkeley    25.0    77.0 (2 rows x 2 columns)

      You can create multiple columns within the same assign where one
      of the columns depends on another one defined within the same assign:

      >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,
      ...           temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9)
                temp_c  temp_f  temp_k
      Portland    17.0    62.6  290.15
      Berkeley    25.0    77.0  298.15 (2 rows x 3 columns)



   .. py:method:: coargsort(keys, ascending=True)

      Return the permutation that sorts the dataframe by `keys`.

      Note: Sorting using Strings may not yield correct sort order.

      :param keys: The keys to sort on.
      :type keys: list of str

      :returns: The permutation array that sorts the data on `keys`.
      :rtype: arkouda.numpy.pdarrayclass.pdarray

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [2, 2, 1], 'col2': [3, 4, 3], 'col3':[5, 6, 7]})
      >>> df
         col1  col2  col3
      0     2     3     5
      1     2     4     6
      2     1     3     7 (3 rows x 3 columns)

      >>> df.coargsort(['col1', 'col2'])
      array([2 0 1])
      >>>



   .. py:property:: columns

      An Index where the values are the column names of the dataframe.

      :returns: The values of the index are the column names of the dataframe.
      :rtype: arkouda.pandas.index.Index

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df
         col1  col2
      0     1     3
      1     2     4 (2 rows x 2 columns)

      >>> df.columns
      Index(['col1', 'col2'], dtype='<U0')


   .. py:method:: concat(items, ordered=True)
      :classmethod:


      Essentially an append, but different formatting.



   .. py:method:: copy(deep=True)

      Make a copy of this object's data.

      When `deep = True` (default), a new object will be created with a copy of
      the calling object's data. Modifications to the data of the copy will not
      be reflected in the original object.


      When `deep = False` a new object will be created without copying the
      calling object's data. Any changes to the data of the original object will
      be reflected in the shallow copy, and vice versa.

      :param deep: When True, return a deep copy. Otherwise, return a shallow copy.
      :type deep: bool, default=True

      :returns: A deep or shallow copy according to caller specification.
      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df
         col1  col2
      0     1     3
      1     2     4 (2 rows x 2 columns)

      >>> df_deep = df.copy(deep=True)
      >>> df_deep['col1'] +=1
      >>> df
         col1  col2
      0     1     3
      1     2     4 (2 rows x 2 columns)

      >>> df_shallow = df.copy(deep=False)
      >>> df_shallow['col1'] +=1
      >>> df
         col1  col2
      0     2     3
      1     3     4 (2 rows x 2 columns)



   .. py:method:: corr() -> DataFrame

      Return new DataFrame with pairwise correlation of columns.

      :returns: Arkouda DataFrame containing correlation matrix of all columns.
      :rtype: DataFrame

      :raises RuntimeError: Raised if there's a server-side error thrown.

      .. seealso:: :py:obj:`pdarray.corr`

      .. rubric:: Notes

      Generate the correlation matrix using Pearson R for all columns.

      Attempts to convert to numeric values where possible for inclusion in the matrix.

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [-1, -2]})
      >>> df
         col1  col2
      0     1    -1
      1     2    -2 (2 rows x 2 columns)

      >>> corr = df.corr()
      >>> corr
            col1  col2
      col1   1.0  -1.0
      col2  -1.0   1.0 (2 rows x 2 columns)



   .. py:method:: count(axis: Union[int, str] = 0, numeric_only=False) -> arkouda.pandas.series.Series

      Count non-NA cells for each column or row.

      The values np.NaN are considered NA.

      :param axis: If 0 or â€˜indexâ€™ counts are generated for each column.
                   If 1 or â€˜columnsâ€™ counts are generated for each row.
      :type axis: {0 or 'index', 1 or 'columns'}, default 0
      :param numeric_only: Include only float, int or boolean data.
      :type numeric_only: bool = False

      :returns: For each column/row the number of non-NA/null entries.
      :rtype: Series

      :raises ValueError: Raised if axis is not 0, 1, 'index', or 'columns'.

      .. seealso:: :py:obj:`GroupBy.count`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import numpy as np
      >>> df = ak.DataFrame({'col_A': ak.array([7, np.nan]), 'col_B':ak.array([1, 9])})
      >>> df
         col_A  col_B
      0    7.0      1
      1    NaN      9 (2 rows x 2 columns)

      >>> df.count()
      col_A    1
      col_B    2
      dtype: int64

      >>> df = ak.DataFrame({'col_A': ak.array(["a","b","c"]), 'col_B':ak.array([1, np.nan, np.nan])})
      >>> df
        col_A  col_B
      0     a    1.0
      1     b    NaN
      2     c    NaN (3 rows x 2 columns)

      >>> df.count()
      col_A    3
      col_B    1
      dtype: int64

      >>> df.count(numeric_only=True)
      col_B    1
      dtype: int64

      >>> df.count(axis=1)
      0    2
      1    1
      2    1
      dtype: int64



   .. py:method:: drop(keys: Union[str, int, List[Union[str, int]]], axis: Union[str, int] = 0, inplace: bool = False) -> Union[None, DataFrame]

      Drop column/s or row/s from the dataframe.

      :param keys: The labels to be dropped on the given axis.
      :type keys: str, int or list
      :param axis: The axis on which to drop from. 0/'index' - drop rows, 1/'columns' - drop columns.
      :type axis: int or str
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`
      :rtype: DataFrame or None

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df
         col1  col2
      0     1     3
      1     2     4 (2 rows x 2 columns)

      Drop column
      >>> df.drop('col1', axis = 1)
         col2
      0     3
      1     4 (2 rows x 1 columns)

      Drop row
      >>> df.drop(0, axis = 0)
         col1  col2
      1     2     4 (1 rows x 2 columns)



   .. py:method:: drop_duplicates(subset=None, keep='first')

      Drop duplcated rows and returns resulting DataFrame.

      If a subset of the columns are provided then only one instance of each
      duplicated row will be returned (keep determines which row).

      :param subset: Iterable of column names to use to dedupe.
      :type subset: Iterable
      :param keep: Determines which duplicates (if any) to keep.
      :type keep: {'first', 'last'}, default='first'

      :returns: DataFrame with duplicates removed.
      :rtype: DataFrame

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2, 2, 3], 'col2': [4, 5, 5, 6]})
      >>> df
         col1  col2
      0     1     4
      1     2     5
      2     2     5
      3     3     6 (4 rows x 2 columns)

      >>> df.drop_duplicates()
         col1  col2
      0     1     4
      1     2     5
      3     3     6 (3 rows x 2 columns)



   .. py:method:: dropna(axis: Union[int, str] = 0, how: Optional[str] = None, thresh: Optional[int] = None, ignore_index: bool = False) -> DataFrame

      Remove missing values.

      :param axis: Determine if rows or columns which contain missing values are removed.

                   0, or 'index': Drop rows which contain missing values.

                   1, or 'columns': Drop columns which contain missing value.

                   Only a single axis is allowed.
      :type axis: {0 or 'index', 1 or 'columns'}, default = 0
      :param how: Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.

                  'any': If any NA values are present, drop that row or column.

                  'all': If all values are NA, drop that row or column.
      :type how: {'any', 'all'}, default='any'
      :param thresh: Require that many non - NA values.Cannot be combined with how.
      :type thresh: int, optional
      :param ignore_index: If ``True``, the resulting axis will be labeled 0, 1, â€¦, n - 1.
      :type ignore_index: bool, default ``False``

      :returns: DataFrame with NA entries dropped from it.
      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import numpy as np
      >>> df = ak.DataFrame(
      ...    {
      ...        "A": [True, True, True, True],
      ...        "B": [1, np.nan, 2, np.nan],
      ...        "C": [1, 2, 3, np.nan],
      ...        "D": [False, False, False, False],
      ...        "E": [1, 2, 3, 4],
      ...        "F": ["a", "b", "c", "d"],
      ...        "G": [1, 2, 3, 4],
      ...    }
      ...   )

      >>> df
            A    B    C      D  E  F  G
      0  True  1.0  1.0  False  1  a  1
      1  True  NaN  2.0  False  2  b  2
      2  True  2.0  3.0  False  3  c  3
      3  True  NaN  NaN  False  4  d  4 (4 rows x 7 columns)

      >>> df.dropna()
            A    B    C      D  E  F  G
      0  True  1.0  1.0  False  1  a  1
      2  True  2.0  3.0  False  3  c  3 (2 rows x 7 columns)

      >>> df.dropna(axis=1)
            A      D  E  F  G
      0  True  False  1  a  1
      1  True  False  2  b  2
      2  True  False  3  c  3
      3  True  False  4  d  4 (4 rows x 5 columns)

      >>> df.dropna(axis=1, thresh=3)
            A    C      D  E  F  G
      0  True  1.0  False  1  a  1
      1  True  2.0  False  2  b  2
      2  True  3.0  False  3  c  3
      3  True  NaN  False  4  d  4 (4 rows x 6 columns)

      >>> df.dropna(axis=1, how="all")
            A    B    C      D  E  F  G
      0  True  1.0  1.0  False  1  a  1
      1  True  NaN  2.0  False  2  b  2
      2  True  2.0  3.0  False  3  c  3
      3  True  NaN  NaN  False  4  d  4 (4 rows x 7 columns)



   .. py:property:: dtypes

      The dtypes of the dataframe.

      :returns: **dtypes** -- The dtypes of the dataframe.
      :rtype: arkouda.pandas.row.Row

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': ["a", "b"]})
      >>> df
         col1 col2
      0     1    a
      1     2    b (2 rows x 2 columns)

      >>> df.dtypes
      {'col1': 'int64', 'col2': 'str'}


   .. py:property:: empty

      Whether the dataframe is empty.

      :returns: True if the dataframe is empty, otherwise False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({})
      >>> df
      Empty DataFrame
      Columns: []
      Index: [] (None rows x 0 columns)

      >>> df.empty
      True


   .. py:method:: filter_by_range(keys, low=1, high=None)

      Filter rows by the size of groups defined on one or more columns.

      Group the DataFrame by the specified `keys`, compute the count of each group,
      and return a boolean mask indicating which rows belong to groups whose sizes
      fall within the inclusive range [`low`, `high`].

      :param keys: Column name or list of column names to group by.
      :type keys: str or list of str
      :param low: Minimum group size (inclusive). Must be >= 0.
      :type low: int, default=1
      :param high: Maximum group size (inclusive). If `None`, no upper bound is applied.
      :type high: int or None, default=None

      :returns: A boolean mask array of length equal to the number of rows in the DataFrame,
                where `True` indicates the rowâ€™s group size is between `low` and `high`.
      :rtype: pdarray of bool

      :raises ValueError: If `low` is negative, or if `high` is not `None` and `high < low`.
      :raises TypeError: If `keys` is not a string or list of strings.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2, 2, 2, 3, 3], 'col2': [4, 5, 6, 7, 8, 9]})
      >>> df
         col1  col2
      0     1     4
      1     2     5
      2     2     6
      3     2     7
      4     3     8
      5     3     9 (6 rows x 2 columns)

      >>> df.filter_by_range("col1", low=1, high=2)
      array([True False False False True True])

      >>> filtered_df = df[df.filter_by_range("col1", low=1, high=2)]
      >>> filtered_df
         col1  col2
      0     1     4
      4     3     8
      5     3     9 (3 rows x 2 columns)



   .. py:method:: from_pandas(pd_df)
      :classmethod:


      Copy the data from a pandas DataFrame into a new arkouda.pandas.dataframe.DataFrame.

      :param pd_df: A pandas DataFrame to convert.
      :type pd_df: pandas.DataFrame

      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import pandas as pd
      >>> pd_df = pd.DataFrame({"A":[1,2],"B":[3,4]})
      >>> type(pd_df)
      <class 'pandas....DataFrame'>
      >>> pd_df
         A  B
      0  1  3
      1  2  4

      >>> ak_df = DataFrame.from_pandas(pd_df)
      >>> type(ak_df)
      <class 'arkouda....DataFrame'>
      >>> ak_df
         A  B
      0  1  3
      1  2  4 (2 rows x 2 columns)



   .. py:method:: from_return_msg(rep_msg)
      :classmethod:


      Create a DataFrame object from an arkouda server response message.

      :param rep_msg: Server response message used to create a DataFrame.
      :type rep_msg: string

      :rtype: DataFrame



   .. py:method:: groupby(keys, use_series=True, as_index=True, dropna=True)

      Group the dataframe by a column or a list of columns.

      Alias for GroupBy.

      :param keys: An (ordered) list of column names or a single string to group by.
      :type keys: str or list of str
      :param use_series: If True, returns an arkouda.pandas.dataframe.DataFrameGroupBy object.
                         Otherwise an arkouda.pandas.groupbyclass.GroupBy object.
      :type use_series: bool, default=True
      :param as_index: If True, groupby columns will be set as index
                       otherwise, the groupby columns will be treated as DataFrame columns.
      :type as_index: bool, default=True
      :param dropna: If True, and the groupby keys contain NaN values,
                     the NaN values together with the corresponding row will be dropped.
                     Otherwise, the rows corresponding to NaN values will be kept.
      :type dropna: bool, default=True

      :returns: If use_series = True, returns an arkouda.pandas.dataframe.DataFrameGroupBy object.
                Otherwise returns an arkouda.pandas.groupbyclass.GroupBy object.
      :rtype: arkouda.pandas.dataframe.DataFrameGroupBy or arkouda.pandas.groupbyclass.GroupBy

      .. seealso:: :py:obj:`arkouda.GroupBy`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1.0, 1.0, 2.0, np.nan], 'col2': [4, 5, 6, 7]})
      >>> df
         col1  col2
      0   1.0     4
      1   1.0     5
      2   2.0     6
      3   NaN     7 (4 rows x 2 columns)

      >>> df.groupby("col1") # doctest: +SKIP
      <arkouda.pandas.groupbyclass.GroupBy object at 0x795584773f00>
      >>> df.groupby("col1").size()
      col1
      1.0    2
      2.0    1
      dtype: int64

      >>> df.groupby("col1",use_series=True).size()
      col1
      1.0    2
      2.0    1
      dtype: int64
      >>> df.groupby("col1",use_series=True, as_index = False).size()
         col1  size
      0   1.0     2
      1   2.0     1 (2 rows x 2 columns)



   .. py:method:: head(n=5)

      Return the first `n` rows.

      This function returns the first `n` rows of the the dataframe. It is
      useful for quickly verifying data, for example, after sorting or
      appending rows.

      :param n: Number of rows to select.
      :type n: int, default = 5

      :returns: The first `n` rows of the DataFrame.
      :rtype: DataFrame

      .. seealso:: :py:obj:`tail`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': ak.arange(10), 'col2': -1 * ak.arange(10)})
      >>> df
         col1  col2
      0     0     0
      1     1    -1
      2     2    -2
      3     3    -3
      4     4    -4
      5     5    -5
      6     6    -6
      7     7    -7
      8     8    -8
      9     9    -9 (10 rows x 2 columns)

      >>> df.head()
         col1  col2
      0     0     0
      1     1    -1
      2     2    -2
      3     3    -3
      4     4    -4 (5 rows x 2 columns)

      >>> df.head(n=2)
         col1  col2
      0     0     0
      1     1    -1 (2 rows x 2 columns)



   .. py:property:: index

      The index of the dataframe.

      :returns: The index of the dataframe.
      :rtype: arkouda.pandas.index.Index or arkouda.pandas.index.MultiIndex

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df
         col1  col2
      0     1     3
      1     2     4 (2 rows x 2 columns)

      >>> df.index
      Index(array([0 1]), dtype='int64')


   .. py:property:: info

      Return a summary string of this dataframe.

      :returns: A summary string of this dataframe.
      :rtype: str

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': ["a", "b"]})
      >>> df
         col1 col2
      0     1    a
      1     2    b (2 rows x 2 columns)

      >>> df.info
      "DataFrame(['col1', 'col2'], 2 rows, 36.00 B)"


   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry.

      :returns: Indicates if the object is contained in the registry.
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components.

      .. seealso:: :py:obj:`register`, :py:obj:`unregister`, :py:obj:`unregister_dataframe_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
         col1  col2
      0     1     4
      1     2     5
      2     3     6 (3 rows x 2 columns)

      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: isin(values: Union[arkouda.numpy.pdarrayclass.pdarray, Dict, arkouda.pandas.series.Series, DataFrame]) -> DataFrame

      Determine whether each element in the DataFrame is contained in values.

      :param values: The values to check for in DataFrame. Series can only have a single index.
      :type values: pdarray, dict, Series, or DataFrame

      :returns: Arkouda DataFrame of booleans showing whether each element in the DataFrame is
                contained in values.
      :rtype: DataFrame

      .. seealso:: :py:obj:`ak.Series.isin`

      .. rubric:: Notes

      - Pandas supports values being an iterable type. In arkouda, we replace this with pdarray.
      - Pandas supports ~ operations. Currently, ak.DataFrame does not support this.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col_A': ak.array([7, 3]), 'col_B':ak.array([1, 9])})
      >>> df
         col_A  col_B
      0      7      1
      1      3      9 (2 rows x 2 columns)

      When `values` is a pdarray, check every value in the DataFrame to determine if
      it exists in values.
      >>> df.isin(ak.array([0, 1]))
         col_A  col_B
      0  False   True
      1  False  False (2 rows x 2 columns)

      When `values` is a dict, the values in the dict are passed to check the column
      indicated by the key.
      >>> df.isin({'col_A': ak.array([0, 3])})
         col_A  col_B
      0  False  False
      1   True  False (2 rows x 2 columns)

      When `values` is a Series, each column is checked if values is present positionally.
      This means that for `True` to be returned, the indexes must be the same.
      >>> i = ak.Index(ak.arange(2))
      >>> s = ak.Series(data=[3, 9], index=i)
      >>> df.isin(s)
         col_A  col_B
      0  False  False
      1  False   True (2 rows x 2 columns)

      When `values` is a DataFrame, the index and column must match.
      Note that 9 is not found because the column name does not match.
      >>> other_df = ak.DataFrame({'col_A':ak.array([7, 3]), 'col_C':ak.array([0, 9])})
      >>> df.isin(other_df)
         col_A  col_B
      0   True  False
      1   True  False (2 rows x 2 columns)



   .. py:method:: isna() -> DataFrame

      Detect missing values.

      Return a boolean same-sized object indicating if the values are NA.
      numpy.NaN values get mapped to True values.
      Everything else gets mapped to False values.

      :returns: Mask of bool values for each element in DataFrame
                that indicates whether an element is an NA value.
      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import numpy as np
      >>> df = ak.DataFrame({"A": [np.nan, 2, 2, 3], "B": [3, np.nan, 5, 6],
      ...          "C": [1, np.nan, 2, np.nan], "D":["a","b","c","d"]})
      >>> df
           A    B    C  D
      0  NaN  3.0  1.0  a
      1  2.0  NaN  NaN  b
      2  2.0  5.0  2.0  c
      3  3.0  6.0  NaN  d (4 rows x 4 columns)

      >>> df.isna()
             A      B      C      D
      0   True  False  False  False
      1  False   True   True  False
      2  False  False  False  False
      3  False  False   True  False (4 rows x 4 columns)



   .. py:method:: load(prefix_path, file_format='INFER')
      :classmethod:


      Load dataframe from file.

      file_format needed for consistency with other load functions.

      :param prefix_path: The prefix path for the data.
      :type prefix_path: str
      :param file_format:
      :type file_format: string, default = "INFER"

      :returns: A dataframe loaded from the prefix_path.
      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak

      To store data in <my_dir>/my_data_LOCALE0000,
      use "<my_dir>/my_data" as the prefix.
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf5_output','my_data')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)
      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> df.to_parquet(my_path + "/my_data")

      >>> df.load(my_path + "/my_data")
         B  A
      0  0  0
      1 -1  1
      2 -2  2
      3 -3  3
      4 -4  4 (5 rows x 2 columns)



   .. py:method:: memory_usage(index=True, unit='B') -> arkouda.pandas.series.Series

      Return the memory usage of each column in bytes.

      The memory usage can optionally include the contribution of
      the index.

      :param index: Specifies whether to include the memory usage of the DataFrame's
                    index in returned Series. If ``index=True``, the memory usage of
                    the index is the first item in the output.
      :type index: bool, default True
      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: A Series whose index is the original column names and whose values
                is the memory usage of each column in bytes.
      :rtype: Series

      .. seealso:: :py:obj:`arkouda.numpy.pdarrayclass.nbytes`, :py:obj:`arkouda.pandas.index.Index.memory_usage`, :py:obj:`arkouda.pandas.index.MultiIndex.memory_usage`, :py:obj:`arkouda.pandas.series.Series.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> dtypes = {"int64":ak.int64, "float64":ak.float64,  "bool":ak.bool_}
      >>> data = dict([(t, ak.ones(5000, dtype=dtypes[t])) for t in dtypes.keys()])
      >>> df = ak.DataFrame(data)
      >>> df.head()
         int64  float64  bool
      0      1      1.0  True
      1      1      1.0  True
      2      1      1.0  True
      3      1      1.0  True
      4      1      1.0  True (5 rows x 3 columns)

      >>> df.memory_usage()
      Index      40000
      int64      40000
      float64    40000
      bool        5000
      dtype: int64

      >>> df.memory_usage(index=False)
      int64      40000
      float64    40000
      bool        5000
      dtype: int64

      >>> df.memory_usage(unit="KB")
      Index      39.062500
      int64      39.062500
      float64    39.062500
      bool        4.882812
      dtype: float64

      To get the approximate total memory usage:
      >>> df.memory_usage(index=True).sum()
      np.int64(125000)



   .. py:method:: memory_usage_info(unit='GB')

      Return a formatted string representation of the size of this DataFrame.

      :param unit: Unit to return. One of {'KB', 'MB', 'GB'}.
      :type unit: str, default = "GB"

      :returns: A string representation of the number of bytes used by this DataFrame in [unit]s.
      :rtype: str

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': ak.arange(1000), 'col2': ak.arange(1000)})
      >>> df.memory_usage_info()
      '0.00 GB'

      >>> df.memory_usage_info(unit="KB")
      '23.44 KB'



   .. py:method:: merge(right: DataFrame, on: Optional[Union[str, List[str]]] = None, how: str = 'inner', left_suffix: str = '_x', right_suffix: str = '_y', convert_ints: bool = True, sort: bool = True) -> DataFrame

      Merge Arkouda DataFrames with a database-style join.

      The resulting dataframe contains rows from both DataFrames as specified by
      the merge condition (based on the "how" and "on" parameters).

      Based on pandas merge functionality.
      https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html

      :param right: The Right DataFrame to be joined.
      :type right: DataFrame
      :param on: The name or list of names of the DataFrame column(s) to join on.
                 If on is None, this defaults to the intersection of the columns in both DataFrames.
      :type on: Optional[Union[str, List[str]]] = None
      :param how: The merge condition.
                  Must be "inner", "left", or "right".
      :type how: {"inner", "left", "right}, default = "inner"
      :param left_suffix: A string indicating the suffix to add to columns from the left dataframe for overlapping
                          column names in both left and right. Defaults to "_x". Only used when how is "inner".
      :type left_suffix: str, default = "_x"
      :param right_suffix: A string indicating the suffix to add to columns from the right dataframe for overlapping
                           column names in both left and right. Defaults to "_y". Only used when how is "inner".
      :type right_suffix: str, default = "_y"
      :param convert_ints: If True, convert columns with missing int values (due to the join) to float64.
                           This is to match pandas.
                           If False, do not convert the column dtypes.
                           This has no effect when how = "inner".
      :type convert_ints: bool = True
      :param sort: If True, DataFrame is returned sorted by "on".
                   Otherwise, the DataFrame is not sorted.
      :type sort: bool = True

      :returns: Joined Arkouda DataFrame.
      :rtype: DataFrame

      .. note:: Multiple column joins are only supported for integer columns.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> left_df = ak.DataFrame({'col1': ak.arange(5), 'col2': -1 * ak.arange(5)})
      >>> left_df
         col1  col2
      0     0     0
      1     1    -1
      2     2    -2
      3     3    -3
      4     4    -4 (5 rows x 2 columns)

      >>> right_df = ak.DataFrame({'col1': 2 * ak.arange(5), 'col2': 2 * ak.arange(5)})
      >>> right_df
         col1  col2
      0     0     0
      1     2     2
      2     4     4
      3     6     6
      4     8     8 (5 rows x 2 columns)

      >>> left_df.merge(right_df, on = "col1")
         col1  col2_x  col2_y
      0     0       0       0
      1     2      -2       2
      2     4      -4       4 (3 rows x 3 columns)

      >>> left_df.merge(right_df, on = "col1", how = "left")
         col1  col2_x  col2_y
      0     0       0     0.0
      1     1      -1     NaN
      2     2      -2     2.0
      3     3      -3     NaN
      4     4      -4     4.0 (5 rows x 3 columns)

      >>> left_df.merge(right_df, on = "col1", how = "right")
         col1  col2_x  col2_y
      0     0     0.0       0
      1     2    -2.0       2
      2     4    -4.0       4
      3     6     NaN       6
      4     8     NaN       8 (5 rows x 3 columns)

      >>> left_df.merge(right_df, on = "col1", how = "outer")
         col1  col2_x  col2_y
      0     0     0.0     0.0
      1     1    -1.0     NaN
      2     2    -2.0     2.0
      3     3    -3.0     NaN
      4     4    -4.0     4.0
      5     6     NaN     6.0
      6     8     NaN     8.0 (7 rows x 3 columns)



   .. py:method:: notna() -> DataFrame

      Detect existing (non-missing) values.

      Return a boolean same-sized object indicating if the values are not NA.
      numpy.NaN values get mapped to False values.

      :returns: Mask of bool values for each element in DataFrame
                that indicates whether an element is not an NA value.
      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import numpy as np
      >>> df = ak.DataFrame({"A": [np.nan, 2, 2, 3], "B": [3, np.nan, 5, 6],
      ...          "C": [1, np.nan, 2, np.nan], "D":["a","b","c","d"]})
      >>> df
           A    B    C  D
      0  NaN  3.0  1.0  a
      1  2.0  NaN  NaN  b
      2  2.0  5.0  2.0  c
      3  3.0  6.0  NaN  d (4 rows x 4 columns)

      >>> df.notna()
             A      B      C     D
      0  False   True   True  True
      1   True  False  False  True
      2   True   True   True  True
      3   True   True  False  True (4 rows x 4 columns)



   .. py:attribute:: objType
      :value: 'DataFrame'



   .. py:method:: read_csv(filename: str, col_delim: str = ',')
      :classmethod:


      Read the columns of a CSV file into an Arkouda DataFrame.

      If the file contains the appropriately formatted header, typed data will be returned.
      Otherwise, all data will be returned as a Strings objects.

      :param filename: Filename to read data from.
      :type filename: str
      :param col_delim: The delimiter for columns within the data.
      :type col_delim: str, default=","

      :returns: Arkouda DataFrame containing the columns from the CSV file.
      :rtype: DataFrame

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.

      .. seealso:: :py:obj:`to_csv`

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations.
      - The column delimiter is expected to be the same for column names and data.
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline ("\\n") at this time.
      - Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'csv_output','my_data')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_csv(my_path)
      >>> df2 = ak.DataFrame.read_csv(my_path + "_LOCALE0000")
      >>> df2
         A  B
      0  1  3
      1  2  4 (2 rows x 2 columns)



   .. py:method:: register(user_defined_name: str) -> DataFrame

      Register this DataFrame object and underlying components with the Arkouda server.

      :param user_defined_name: User defined name the DataFrame is to be registered under.
                                This will be the root name for underlying components.
      :type user_defined_name: str

      :returns: The same DataFrame which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different DataFrames with the same name.
      :rtype: DataFrame

      :raises TypeError: Raised if user_defined_name is not a str.
      :raises RegistrationError: If the server was unable to register the DataFrame with the user_defined_name.

      .. seealso:: :py:obj:`unregister`, :py:obj:`unregister_dataframe_by_name`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      Any changes made to a DataFrame object after registering with the server may not be reflected
      in attached copies.

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
          col1  col2
      0     1     4
      1     2     5
      2     3     6 (3 rows x 2 columns)
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:attribute:: registered_name
      :value: None



   .. py:method:: rename(mapper: Optional[Union[Callable, Dict]] = None, index: Optional[Union[Callable, Dict]] = None, column: Optional[Union[Callable, Dict]] = None, axis: Union[str, int] = 0, inplace: bool = False) -> Optional[DataFrame]

      Rename indexes or columns according to a mapping.

      :param mapper: Function or dictionary mapping existing values to new values.
                     Nonexistent names will not raise an error.
                     Uses the value of axis to determine if renaming column or index
      :type mapper: callable or dict-like, Optional
      :param index: Function or dictionary mapping existing index names to
                    new index names. Nonexistent names will not raise an
                    error.
                    When this is set, axis is ignored.
      :type index: callable or dict-like, Optional
      :param column: Function or dictionary mapping existing column names to
                     new column names. Nonexistent names will not raise an
                     error.
                     When this is set, axis is ignored.
      :type column: callable or dict-like, Optional
      :param axis: Indicates which axis to perform the rename.
                   0/"index" - Indexes
                   1/"column" - Columns
      :type axis: int or str, default=0
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`.
      :rtype: DataFrame or None

      .. rubric:: Examples

      >>> import arkouda as ak

      >>> df = ak.DataFrame({"A": ak.array([1, 2, 3]), "B": ak.array([4, 5, 6])})
      >>> df
         A  B
      0  1  4
      1  2  5
      2  3  6 (3 rows x 2 columns)

      Rename columns using a mapping:
      >>> df.rename(column={'A':'a', 'B':'c'})
         a  c
      0  1  4
      1  2  5
      2  3  6 (3 rows x 2 columns)

      Rename indexes using a mapping:
      >>> df.rename(index={0:99, 2:11})
          A  B
      99  1  4
      1   2  5
      11  3  6 (3 rows x 2 columns)

      Rename using an axis style parameter:
      >>> df.rename(str.lower, axis='column')
         a  b
      0  1  4
      1  2  5
      2  3  6 (3 rows x 2 columns)



   .. py:method:: reset_index(size: Optional[int] = None, inplace: bool = False) -> Union[None, DataFrame]

      Set the index to an integer range.

      Useful if this dataframe is the result of a slice operation from
      another dataframe, or if you have permuted the rows and no longer need
      to keep that ordering on the rows.

      :param size: If size is passed, do not attempt to determine size based on
                   existing column sizes. Assume caller handles consistency correctly.
      :type size: int, optional
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`.
      :rtype: DataFrame or None

      .. note::

         Pandas adds a column 'index' to indicate the original index. Arkouda does not currently
         support this behavior.

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"A": ak.array([1, 2, 3]), "B": ak.array([4, 5, 6])})
      >>> df
         A  B
      0  1  4
      1  2  5
      2  3  6 (3 rows x 2 columns)

      >>> perm_df = df[ak.array([0,2,1])]
      >>> perm_df
         A  B
      0  1  4
      2  3  6
      1  2  5 (3 rows x 2 columns)

      >>> perm_df.reset_index()
         A  B
      0  1  4
      1  3  6
      2  2  5 (3 rows x 2 columns)



   .. py:method:: sample(n=5) -> DataFrame

      Return a random sample of `n` rows.

      :param n: Number of rows to return.
      :type n: int, default=5

      :returns: The sampled `n` rows of the DataFrame.
      :rtype: DataFrame

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> df
         A  B
      0  0  0
      1  1 -1
      2  2 -2
      3  3 -3
      4  4 -4 (5 rows x 2 columns)

      Random output of size 3:
      >>> df.sample(n=3)  # doctest: +SKIP
         A  B
      4  4 -4
      3  3 -3
      1  1 -1 (3 rows x 2 columns)



   .. py:property:: shape

      The shape of the dataframe.

      :returns: Tuple of array dimensions.
      :rtype: tuple of int

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df
         col1  col2
      0     1     4
      1     2     5
      2     3     6 (3 rows x 2 columns)

      >>> df.shape
      (3, 2)


   .. py:property:: size

      Return the number of bytes on the arkouda server.

      :returns: The number of bytes on the arkouda server.
      :rtype: int

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df
         col1  col2
      0     1     4
      1     2     5
      2     3     6 (3 rows x 2 columns)

      >>> df.size
      6


   .. py:method:: sort_index(ascending=True)

      Sort the DataFrame by indexed columns.

      Note: Fails on sort order of arkouda.numpy.strings.Strings columns when
          multiple columns being sorted.

      :param ascending: Sort values in ascending (default) or descending order.
      :type ascending: bool, default = True

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1.1, 3.1, 2.1], 'col2': [6, 5, 4]},
      ...          index = Index(ak.array([2,0,1]), name="idx"))

      >>> df
           col1  col2
      idx
      2     1.1     6
      0     3.1     5
      1     2.1     4 (3 rows x 2 columns)

      >>> df.sort_index()
           col1  col2
      idx
      0     3.1     5
      1     2.1     4
      2     1.1     6 (3 rows x 2 columns)



   .. py:method:: sort_values(by=None, ascending=True)

      Sort the DataFrame by one or more columns.

      If no column is specified, all columns are used.

      Note: Fails on order of arkouda.numpy.strings.Strings columns when multiple columns being sorted.

      :param by: The name(s) of the column(s) to sort by.
      :type by: str or list/tuple of str, default = None
      :param ascending: Sort values in ascending (default) or descending order.
      :type ascending: bool, default = True

      .. seealso:: :py:obj:`apply_permutation`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [2, 2, 1], 'col2': [3, 4, 3], 'col3':[5, 6, 7]})
      >>> df
         col1  col2  col3
      0     2     3     5
      1     2     4     6
      2     1     3     7 (3 rows x 3 columns)

      >>> df.sort_values()
         col1  col2  col3
      2     1     3     7
      0     2     3     5
      1     2     4     6 (3 rows x 3 columns)

      >>> df.sort_values("col3")
         col1  col2  col3
      0     2     3     5
      1     2     4     6
      2     1     3     7 (3 rows x 3 columns)



   .. py:method:: tail(n=5)

      Return the last `n` rows.

      This function returns the last `n` rows for the dataframe. It is
      useful for quickly testing if your object has the right type of data in
      it.

      :param n: Number of rows to select.
      :type n: int, default=5

      :returns: The last `n` rows of the DataFrame.
      :rtype: DataFrame

      .. seealso:: :py:obj:`arkouda.pandas.dataframe.head`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': ak.arange(10), 'col2': -1 * ak.arange(10)})
      >>> df
         col1  col2
      0     0     0
      1     1    -1
      2     2    -2
      3     3    -3
      4     4    -4
      5     5    -5
      6     6    -6
      7     7    -7
      8     8    -8
      9     9    -9 (10 rows x 2 columns)

      >>> df.tail()
         col1  col2
      5     5    -5
      6     6    -6
      7     7    -7
      8     8    -8
      9     9    -9 (5 rows x 2 columns)

      >>> df.tail(n=2)
         col1  col2
      8     8    -8
      9     9    -9 (2 rows x 2 columns)



   .. py:method:: to_csv(path: str, index: bool = False, columns: Optional[List[str]] = None, col_delim: str = ',', overwrite: bool = False)

      Write DataFrame to CSV file(s).

      File will contain a column for each column in the DataFrame.
      All CSV Files written by Arkouda include a header denoting data types of the columns.
      Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      :param path: The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                   when they are written to disk.
      :type path: str
      :param index: If True, the index of the DataFrame will be written to the file
                    as a column.
      :type index: bool, default=False
      :param columns: Column names to assign when writing data.
      :type columns: list of str (Optional)
      :param col_delim: Value to be used to separate columns within the file.
                        Please be sure that the value used DOES NOT appear in your dataset.
      :type col_delim: str, default=","
      :param overwrite: If True, any existing files matching your provided prefix_path will
                        be overwritten. If False, an error will be returned if existing files are found.
      :type overwrite: bool, default=False

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations.
      - The column delimiter is expected to be the same for column names and data.
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline ("\\n") at this time.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'csv_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_csv(my_path)
      >>> df2 = ak.DataFrame.read_csv(my_path + "_LOCALE0000")
      >>> df2
         A  B
      0  1  3
      1  2  4 (2 rows x 2 columns)



   .. py:method:: to_hdf(path, index=False, columns=None, file_type: Literal['single', 'distribute'] = 'distribute')

      Save DataFrame to disk as hdf5, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: List, default = None
      :param file_type: Whether to save to a single file or distribute across Locales.
                        Default is "distribute".
      :type file_type: {"single", "distribute"}

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :py:obj:`to_parquet`, :py:obj:`load`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")
         A  B
      0  1  3
      1  2  4 (2 rows x 2 columns)



   .. py:method:: to_markdown(mode='wt', index=True, tablefmt='grid', storage_options=None, **kwargs)

      Print DataFrame in Markdown-friendly format.

      :param mode: Mode in which file is opened, "wt" by default.
      :type mode: str, optional
      :param index: Add index (row) labels.
      :type index: bool, optional, default True
      :param tablefmt: Table format to call from tablulate:
                       https://pypi.org/project/tabulate/
      :type tablefmt: str = "grid"
      :param storage_options: Extra options that make sense for a particular storage connection,
                              e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec,
                              e.g., starting â€œs3://â€, â€œgcs://â€.
                              An error will be raised if providing this argument with a non-fsspec URL.
                              See the fsspec and backend storage implementation docs for the set
                              of allowed keys and values.
      :type storage_options: dict, optional
      :param \*\*kwargs: These parameters will be passed to tabulate.

      .. note::

         This function should only be called on small DataFrames as it calls pandas.DataFrame.to_markdown:
         https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.to_markdown.html

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"animal_1": ["elk", "pig"], "animal_2": ["dog", "quetzal"]})
      >>> print(df.to_markdown())
      +----+------------+------------+
      |    | animal_1   | animal_2   |
      +====+============+============+
      |  0 | elk        | dog        |
      +----+------------+------------+
      |  1 | pig        | quetzal    |
      +----+------------+------------+

      Suppress the index:
      >>> print(df.to_markdown(index = False))
      +------------+------------+
      | animal_1   | animal_2   |
      +============+============+
      | elk        | dog        |
      +------------+------------+
      | pig        | quetzal    |
      +------------+------------+



   .. py:method:: to_pandas(datalimit=maxTransferBytes, retain_index=False)

      Send this DataFrame to a pandas DataFrame.

      :param datalimit: The maximum number size, in megabytes to transfer. The requested
                        DataFrame will be converted to a pandas DataFrame only if the
                        estimated size of the DataFrame does not exceed this value.
      :type datalimit: int, default=arkouda.core.client.maxTransferBytes
      :param retain_index: Normally, to_pandas() creates a new range index object. If you want
                           to keep the index column, set this to True.
      :type retain_index: bool, default=False

      :returns: The result of converting this DataFrame to a pandas DataFrame.
      :rtype: pandas.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak_df = ak.DataFrame({"A": ak.arange(2), "B": -1 * ak.arange(2)})
      >>> type(ak_df)
      <class 'arkouda...DataFrame'>
      >>> ak_df
         A  B
      0  0  0
      1  1 -1 (2 rows x 2 columns)

      >>> import pandas as pd
      >>> pd_df = ak_df.to_pandas()
      >>> type(pd_df)
      <class 'pandas...DataFrame'>
      >>> pd_df
         A  B
      0  0  0
      1  1 -1



   .. py:method:: to_parquet(path, index=False, columns=None, compression: Optional[str] = None, convert_categoricals: bool = False)

      Save DataFrame to disk as parquet, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: list
      :param compression: Provide the compression type to use when writing the file.
                          Supported values: snappy, gzip, brotli, zstd, lz4
      :type compression: str (Optional), default=None
      :param convert_categoricals: Parquet requires all columns to be the same size and Categoricals
                                   don't satisfy that requirement.
                                   If set, write the equivalent Strings in place of any Categorical columns.
      :type convert_categoricals: bool, default=False

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :py:obj:`to_hdf`, :py:obj:`load`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'parquet_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_parquet(my_path + "/my_data")

      >>> df.load(my_path + "/my_data")
         B  A
      0  3  1
      1  4  2 (2 rows x 2 columns)



   .. py:method:: transfer(hostname, port)

      Send a DataFrame to a different Arkouda server.

      :param hostname: The hostname where the Arkouda server intended to
                       receive the DataFrame is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :returns: A message indicating a complete transfer.
      :rtype: str

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister()

      Unregister this DataFrame object in the arkouda server.

      Unregister this DataFrame object in the arkouda server which was previously
      registered using register() and/or attached to using attach().

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister.

      .. seealso:: :py:obj:`register`, :py:obj:`unregister_dataframe_by_name`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. rubric:: Example

      >>> import arkouda as ak
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
                 col1  col2
      0     1     4
      1     2     5
      2     3     6 (3 rows x 2 columns)

      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: update_hdf(prefix_path: str, index=False, columns=None, repack: bool = True)

      Overwrite the dataset with the name provided with this dataframe.

      If the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share.
      :type prefix_path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: List, default=None
      :param repack: HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool, default=True

      :returns: Success message if successful.
      :rtype: str

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      If the dataset provided does not exist, it will be added.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")
         A  B
      0  1  3
      1  2  4 (2 rows x 2 columns)

      >>> df2 = ak.DataFrame({"A":[5,6],"B":[7,8]})
      >>> df2.update_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")
         A  B
      0  5  7
      1  6  8 (2 rows x 2 columns)



   .. py:method:: update_nrows()

      Compute the number of rows on the arkouda server and updates the size parameter.



.. py:class:: DataFrameGroupBy(gb, df, gb_key_names=None, as_index=True)

   A DataFrame that has been grouped by a subset of columns.

   :param gb_key_names: The column name(s) associated with the aggregated columns.
   :type gb_key_names: str or list(str), default=None
   :param as_index: If True, interpret aggregated column as index
                    (only implemented for single dimensional aggregates).
                    Otherwise, treat aggregated column as a dataframe column.
   :type as_index: bool, default=True

   .. attribute:: gb

      GroupBy object, where the aggregation keys are values of column(s) of a dataframe,
      usually in preparation for aggregating with respect to the other columns.

      :type: GroupBy

   .. attribute:: df

      The dataframe containing the original data.

      :type: DataFrame

   .. attribute:: gb_key_names

      The column name(s) associated with the aggregated columns.

      :type: Union[str, List[str]]

   .. attribute:: as_index

      If True the grouped values of the aggregation keys will be treated as an index.
      Defaults to True.

      :type: bool


   .. py:attribute:: all_non_nan
      :value: False



   .. py:attribute:: as_index
      :type:  bool


   .. py:method:: broadcast(x, permute=True)

      Fill each groupâ€™s segment with a constant value.

      :param x: The values to put in each groupâ€™s segment.
      :type x: Series or pdarray
      :param permute: If True (default), permute broadcast values back to the
                      ordering of the original array on which GroupBy was called.
                      If False, the broadcast values are grouped by value.
      :type permute: bool, default=True

      :returns: A Series with the Index of the original frame and the values of the broadcast.
      :rtype: arkouda.pandas.series.Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda.pandas.dataframe import DataFrameGroupBy
      >>> df = ak.DataFrame({"A":[1,2,2,3],"B":[3,4,5,6]})
      >>> df
         A  B
      0  1  3
      1  2  4
      2  2  5
      3  3  6 (4 rows x 2 columns)

      >>> gb = df.groupby("A")
      >>> x = ak.array([10,11,12])
      >>> s = DataFrameGroupBy.broadcast(gb, x)
      >>> df["C"] = s.values
      >>> df
         A  B   C
      0  1  3  10
      1  2  4  11
      2  2  5  11
      3  3  6  12 (4 rows x 3 columns)



   .. py:attribute:: df
      :type:  DataFrame


   .. py:method:: diff(colname)

      Create a difference aggregate for the given column.

      For each group, the difference between successive values is calculated.
      Aggregate operations (mean,min,max,std,var) can be done on the results.

      :param colname: Name of the column to compute the difference on.
      :type colname: str

      :returns: Object containing the differences, which can be aggregated.
      :rtype: DiffAggregate

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"A":[1,2,2,2,3,3],"B":[3,9,11,27,86,100]})
      >>> df
         A    B
      0  1    3
      1  2    9
      2  2   11
      3  2   27
      4  3   86
      5  3  100 (6 rows x 2 columns)

      >>> gb = df.groupby("A")
      >>> gb.diff("B").values
      array([nan nan 2.00000000000000000 16.00000000000000000 nan 14.00000000000000000])



   .. py:attribute:: dropna


   .. py:attribute:: gb
      :type:  arkouda.pandas.groupbyclass.GroupBy


   .. py:attribute:: gb_key_names
      :type:  Union[str, List[str]]


   .. py:method:: head(n: int = 5, sort_index: bool = True) -> DataFrame

      Return the first n rows from each group.

      :param n: Maximum number of rows to return for each group.
                If the number of rows in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param sort_index: If true, return the DataFrame with indices sorted.
      :type sort_index: bool, default = True

      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"a":ak.arange(10) %3 , "b":ak.arange(10)})
      >>> df
         a  b
      0  0  0
      1  1  1
      2  2  2
      3  0  3
      4  1  4
      5  2  5
      6  0  6
      7  1  7
      8  2  8
      9  0  9 (10 rows x 2 columns)

      >>> df.groupby("a").head(2)
         a  b
      0  0  0
      1  1  1
      2  2  2
      3  0  3
      4  1  4
      5  2  5 (6 rows x 2 columns)



   .. py:method:: sample(n=None, frac=None, replace=False, weights=None, random_state=None)

      Return a random sample from each group.

      You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the same row more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the underlying DataFrame
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional

      :returns: A new DataFrame containing items randomly sampled from each group
                sorted according to the grouped columns.
      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"A":[3,1,2,1,2,3],"B":[3,4,5,6,7,8]})
      >>> df
         A  B
      0  3  3
      1  1  4
      2  2  5
      3  1  6
      4  2  7
      5  3  8 (6 rows x 2 columns)

      >>> df.groupby("A").sample(random_state=6)
         A  B
      3  1  6
      4  2  7
      5  3  8 (3 rows x 2 columns)

      >>> df.groupby("A").sample(frac=0.5, random_state=3, weights=ak.array([1,1,1,0,0,0]))
         A  B
      1  1  4
      2  2  5
      0  3  3 (3 rows x 2 columns)

      >>> df.groupby("A").sample(n=3, replace=True, random_state=ak.random.default_rng(7))
         A  B
      1  1  4
      3  1  6
      1  1  4
      4  2  7
      4  2  7
      4  2  7
      0  3  3
      5  3  8
      5  3  8 (9 rows x 2 columns)



   .. py:method:: size(as_series=None, sort_index=True)

      Compute the size of each value as the total number of rows, including NaN values.

      :param as_series: Indicates whether to return arkouda.pandas.dataframe.DataFrame (if as_series = False) or
                        arkouda.pandas.series.Series (if as_series = True)
      :type as_series: bool, default=None
      :param sort_index: If True, results will be returned with index values sorted in ascending order.
      :type sort_index: bool, default=True

      :rtype: arkouda.pandas.dataframe.DataFrame or arkouda.pandas.series.Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"A":[1,2,2,3],"B":[3,4,5,6]})
      >>> df
         A  B
      0  1  3
      1  2  4
      2  2  5
      3  3  6 (4 rows x 2 columns)

      >>> df.groupby("A").size(as_series = False)
         size
      A
      1     1
      2     2
      3     1 (3 rows x 1 columns)



   .. py:method:: tail(n: int = 5, sort_index: bool = True) -> DataFrame

      Return the last n rows from each group.

      :param n: Maximum number of rows to return for each group.
                If the number of rows in a group is less than n,
                all the rows from that group will be returned.
      :type n: int, optional, default = 5
      :param sort_index: If true, return the DataFrame with indices sorted.
      :type sort_index: bool, default = True

      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> df = ak.DataFrame({"a":ak.arange(10) %3 , "b":ak.arange(10)})
      >>> df
         a  b
      0  0  0
      1  1  1
      2  2  2
      3  0  3
      4  1  4
      5  2  5
      6  0  6
      7  1  7
      8  2  8
      9  0  9 (10 rows x 2 columns)

      >>> df.groupby("a").tail(2)
         a  b
      4  1  4
      5  2  5
      6  0  6
      7  1  7
      8  2  8
      9  0  9 (6 rows x 2 columns)



   .. py:attribute:: where_not_nan
      :value: None



.. py:class:: DatetimeAccessor(series)

   Bases: :py:obj:`Properties`


   Accessor for datetime-like operations on Arkouda Series.

   Provides datetime methods such as `.floor()`, `.ceil()`, and `.round()`,
   mirroring the `.dt` accessor in pandas.

   This accessor is automatically attached to Series objects that wrap
   `arkouda.Datetime` values. It should not be instantiated directly.

   :param series: The Series object containing `Datetime` values.
   :type series: arkouda.pandas.Series

   :raises AttributeError: If the underlying Series values are not of type `arkouda.Datetime`.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda import Datetime, Series
   >>> s = Series(Datetime(ak.array([1_000_000_000_000])))
   >>> s.dt.floor("D")
   0   1970-01-01
   dtype: datetime64[ns]


   .. py:attribute:: series


.. py:class:: DiffAggregate(gb, series)

   A column in a GroupBy that has been differenced.

   Aggregation operations can be done on the result.

   .. attribute:: gb

      GroupBy object, where the aggregation keys are values of column(s) of a dataframe.

      :type: GroupBy

   .. attribute:: values

      A column to compute the difference on.

      :type: Series


   .. py:attribute:: gb
      :type:  arkouda.pandas.groupbyclass.GroupBy


   .. py:attribute:: values
      :type:  arkouda.pandas.series.Series


.. py:class:: Index(values: Union[List, arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.pandas.categorical.Categorical, pandas.Index, Index, pandas.Categorical], name: Optional[str] = None, allow_list=False, max_list_size=1000)

   Sequence used for indexing and alignment.

   The basic object storing axis labels for all DataFrame objects.

   :param values:
   :type values: List, pdarray, Strings, Categorical, pandas.Categorical, pandas.Index, or Index
   :param name: Name to be stored in the index.
   :type name: str, default=None
   :param allow_list = False: If False, list values will be converted to a pdarray.
                              If True, list values will remain as a list, provided the data length is less than max_list_size.
   :param : If False, list values will be converted to a pdarray.
            If True, list values will remain as a list, provided the data length is less than max_list_size.
   :param max_list_size = 1000: This is the maximum allowed data length for the values to be stored as a list object.

   :raises ValueError: Raised if allow_list=True and the size of values is > max_list_size.

   .. seealso:: :py:obj:`MultiIndex`

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.Index([1, 2, 3])
   Index(array([1 2 3]), dtype='int64')

   >>> ak.Index(list('abc'))
   Index(array(['a', 'b', 'c']), dtype='<U0')

   >>> ak.Index([1, 2, 3], allow_list=True)
   Index([1, 2, 3], dtype='int64')


   .. py:method:: argsort(ascending: bool = True) -> Union[list, arkouda.numpy.pdarrayclass.pdarray]

      Return the permutation that sorts the Index.

      :param ascending: If True (default), sort in ascending order.
                        If False, sort in descending order.
      :type ascending: bool, optional

      :returns: Indices that would sort the Index.
      :rtype: list or pdarray

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> idx = ak.Index([10, 3, 5])
      >>> idx.argsort()
      array([1 2 0])



   .. py:method:: concat(other)

      Concatenate this Index with another Index.

      :param other: The Index to concatenate with this one.
      :type other: Index

      :returns: A new Index with values from both indices.
      :rtype: Index

      :raises TypeError: If the types of the two Index objects do not match.



   .. py:method:: equals(other: Index) -> arkouda.numpy.dtypes.bool_scalars

      Whether Indexes are the same size, and all entries are equal.

      :param other: object to compare.
      :type other: Index

      :returns: True if the Indexes are the same, o.w. False.
      :rtype: bool_scalars

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> i = ak.Index([1, 2, 3])
      >>> i_cpy = ak.Index([1, 2, 3])
      >>> i.equals(i_cpy)
      np.True_
      >>> i2 = ak.Index([1, 2, 4])
      >>> i.equals(i2)
      np.False_

      MultiIndex case:

      >>> arrays = [ak.array([1, 1, 2, 2]), ak.array(["red", "blue", "red", "blue"])]
      >>> m = ak.MultiIndex(arrays, names=["numbers2", "colors2"])
      >>> m.equals(m)
      True
      >>> arrays2 = [ak.array([1, 1, 2, 2]), ak.array(["red", "blue", "red", "green"])]
      >>> m2 = ak.MultiIndex(arrays2, names=["numbers2", "colors2"])
      >>> m.equals(m2)
      False



   .. py:method:: factory(index)
      :staticmethod:


      Construct an Index or MultiIndex based on the input.

      :param index: If a single array-like, returns an Index.
                    If a tuple of array-like objects, returns a MultiIndex.
      :type index: array-like or tuple of array-like

      :returns: An Index if input is a single array-like, or a MultiIndex otherwise.
      :rtype: Index or MultiIndex



   .. py:method:: from_return_msg(rep_msg)
      :classmethod:


      Reconstruct an Index or MultiIndex from a return message.

      :param rep_msg: A string return message containing encoded index information.
      :type rep_msg: str

      :returns: The reconstructed Index or MultiIndex instance.
      :rtype: Index or MultiIndex



   .. py:property:: index

      Deprecated alias for `values`.

      This property is maintained for backward compatibility and returns the same
      array as the `values` attribute. It will be removed in a future release;
      use `values` directly instead.

      :returns: * *arkouda.numpy.pdarray* -- The underlying values of this object (same as `values`).
                * *Deprecated*
                * *----------*
                * Use the `values` attribute directly. This alias will be removed in a future release.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> idx = ak.Index(ak.array([1, 2, 3]))
      >>> idx.index
      array([1 2 3])


   .. py:property:: inferred_type
      :type: str


      Return a string of the type inferred from the values.


   .. py:method:: is_registered()

      Return whether the object is registered.

      Return True iff the object is contained in the registry or is a component of a
      registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :py:obj:`register`, :py:obj:`attach`, :py:obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: is_unique

      Property indicating if all values in the index are unique.

      :rtype: bool - True if all values are unique, False otherwise.


   .. py:method:: lookup(key)

      Check for presence of key(s) in the Index.

      :param key: The value(s) to look up in the Index. If a scalar is provided, it will
                  be converted to a one-element array.
      :type key: pdarray or scalar

      :returns: A boolean array of length ``len(self)``, indicating which entries of
                the Index are present in `key`.
      :rtype: pdarray

      :raises TypeError: If `key` cannot be converted to an arkouda array.



   .. py:method:: map(arg: Union[dict, arkouda.pandas.series.Series]) -> Index

      Map values of Index according to an input mapping.

      :param arg: The mapping correspondence.
      :type arg: dict or Series

      :returns: A new index with the values transformed by the mapping correspondence.
      :rtype: arkouda.pandas.index.Index

      :raises TypeError: Raised if arg is not of type dict or arkouda.pandas.Series.
          Raised if index values not of type pdarray, Categorical, or Strings.

      .. rubric:: Examples

      >>> import arkouda as ak

      >>> idx = ak.Index(ak.array([2, 3, 2, 3, 4]))
      >>> idx
      Index(array([2 3 2 3 4]), dtype='int64')
      >>> idx.map({4: 25.0, 2: 30.0, 1: 7.0, 3: 5.0})
      Index(array([30.00000000000000000 5.00000000000000000 30.00000000000000000
      5.00000000000000000 25.00000000000000000]), dtype='float64')
      >>> s2 = ak.Series(ak.array(["a","b","c","d"]), index = ak.array([4,2,1,3]))
      >>> idx.map(s2)
      Index(array(['b', 'd', 'b', 'd', 'a']), dtype='<U0')



   .. py:attribute:: max_list_size
      :value: 1000



   .. py:method:: memory_usage(unit='B')

      Return the memory usage of the Index values.

      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: Bytes of memory consumed.
      :rtype: int

      .. seealso:: :py:obj:`arkouda.numpy.pdarrayclass.nbytes`, :py:obj:`arkouda.pandas.index.MultiIndex.memory_usage`, :py:obj:`arkouda.pandas.series.Series.memory_usage`, :py:obj:`arkouda.pandas.dataframe.DataFrame.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> idx = Index(ak.array([1, 2, 3]))
      >>> idx.memory_usage()
      24



   .. py:property:: names

      Return Index or MultiIndex names.


   .. py:property:: ndim

      Number of dimensions of the underlying data, by definition 1.

      .. seealso:: :py:obj:`MultiIndex.ndim`


   .. py:property:: nlevels

      Integer number of levels in this Index.

      An Index will always have 1 level.

      .. seealso:: :py:obj:`MultiIndex.nlevels`


   .. py:attribute:: objType
      :value: 'Index'



   .. py:method:: register(user_defined_name)

      Register this Index object and underlying components with the Arkouda server.

      :param user_defined_name: user defined name the Index is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Index which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Indexes with the same name.
      :rtype: Index

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Index with the user_defined_name

      .. seealso:: :py:obj:`unregister`, :py:obj:`attach`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: set_dtype(dtype)

      Change the data type of the index.

      Currently only aku.ip_address and ak.array are supported.




   .. py:property:: shape

      Return the shape of the Index.

      :returns: A tuple representing the shape of the Index (size,).
      :rtype: tuple


   .. py:method:: sort_values(return_indexer: bool = False, ascending: bool = True, na_position: str = 'last') -> Union[Index, Tuple[Index, Union[arkouda.numpy.pdarrayclass.pdarray, list]]]

      Return a sorted copy of the index.

      :param return_indexer: If True, also return the integer positions that sort the index.
      :type return_indexer: bool, default False
      :param ascending: Sort in ascending order. Use False for descending.
      :type ascending: bool, default True
      :param na_position: Where to position NaNs. 'first' puts NaNs at the beginning,
                          'last' at the end.
      :type na_position: {'first', 'last'}, default 'last'

      :returns:

                sorted_index : arkouda.Index
                    A new Index whose values are sorted.
                indexer : Union[arkouda.pdarray, list], optional
                    The indices that would sort the original index.
                    Only returned when ``return_indexer=True``.
      :rtype: Union[Index, Tuple[Index, Union[pdarray, list]]]

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> idx = ak.Index([10, 100, 1, 1000])
      >>> idx
      Index(array([10 100 1 1000]), dtype='int64')

      Sort in ascending order (default):
      >>> idx.sort_values()
      Index(array([1 10 100 1000]), dtype='int64')

      Sort in descending order and get the sort positions:
      >>> idx.sort_values(ascending=False, return_indexer=True)
      (Index(array([1000 100 10 1]), dtype='int64'), array([3 1 0 2]))



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'index', col_delim: str = ',', overwrite: bool = False)

      Write Index to CSV file(s).

      File will contain a single column with the pdarray data.
      All CSV Files written by Arkouda include a header denoting data types of the columns.

      :param prefix_path: The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                          when they are written to disk.
      :type prefix_path: str
      :param dataset: Column name to save the pdarray under. Defaults to "array".
      :type dataset: str
      :param col_delim: Defaults to ",". Value to be used to separate columns within the file.
                        Please be sure that the value used DOES NOT appear in your dataset.
      :type col_delim: str
      :param overwrite: Defaults to False. If True, any existing files matching your provided prefix_path will
                        be overwritten. If False, an error will be returned if existing files are found.
      :type overwrite: bool

      :rtype: str reponse message

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.
          Raised if the Index values are a list.

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations
      - The column delimiter is expected to be the same for column names and data
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline (`\n`) at this time.



   .. py:method:: to_dict(label)

      Convert the Index to a dictionary with a specified label.

      :param label: The key to use in the resulting dictionary. If a list is provided,
                    only the first element is used. If None, defaults to "idx".
      :type label: str or list of str

      :returns: A dictionary with the label as the key and the Index as the value.
      :rtype: dict



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'index', mode: Literal['truncate', 'append'] = 'truncate', file_type: Literal['single', 'distribute'] = 'distribute') -> str

      Save the Index to HDF5.

      The object can be saved to a collection of files or single file.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises TypeError: Raised if the Index values are a list.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.



   .. py:method:: to_ndarray()

      Convert the Index values to a NumPy ndarray.

      :returns: A NumPy array representation of the Index values.
      :rtype: numpy.ndarray



   .. py:method:: to_pandas()

      Convert this Arkouda-backed index wrapper to an equivalent pandas Index.

      This method materializes the underlying values into a local NumPy array
      (or pandas Categorical, when applicable) and returns the corresponding
      pandas ``Index`` (or ``CategoricalIndex``).

      :returns: A pandas Index representing the same logical values. For categorical
                data, a ``pandas.CategoricalIndex`` is returned.
      :rtype: pandas.Index

      .. rubric:: Notes

      - If the underlying values are categorical, this returns a
        ``pandas.CategoricalIndex``.
      - For unicode string-like data (or object arrays inferred as strings),
        this attempts to return a pandas "string" dtype Index to match pandas'
        missing-value behavior (e.g., NA handling).
      - Fixed-width bytes data is preserved as bytes (no implicit decoding).

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> import pandas
      >>> idx = ak.Index(ak.array([1,2,3]))
      >>> pidx = idx.to_pandas()
      >>> pidx.dtype
      dtype('<i8')



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'index', mode: Literal['truncate', 'append'] = 'truncate', compression: Optional[str] = None)

      Save the Index to Parquet.

      The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises TypeError: Raised if the Index values are a list.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.



   .. py:method:: tolist()

      Convert the Index values to a Python list.

      :returns: A list containing the Index values.
      :rtype: list



   .. py:method:: unregister()

      Unregister this Index object in the arkouda server.

      Unregister this Index object in the arkouda server, which was previously
      registered using register() and/or attached to using attach().

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :py:obj:`register`, :py:obj:`attach`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'index', repack: bool = True)

      Overwrite the dataset with the name provided with this Index object.

      If the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :raises RuntimeError: Raised if a server-side error is thrown saving the index

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, this will create a copy of the
        file with the new data



.. py:class:: LogLevel(*args, **kwds)

   Bases: :py:obj:`enum.Enum`


   Enum for defining valid log levels used by ArkoudaLogger.

   Members
   -------
   INFO : str
       Confirmation that things are working as expected.
   DEBUG : str
       Detailed information, typically of interest only when diagnosing problems.
   WARN : str
       An indication that something unexpected happened, or indicative of some problem.
   ERROR : str
       A more serious problem, the software has not been able to perform some function.
   CRITICAL : str
       An extremely serious error, indicating the program itself may be unable to continue.

   .. rubric:: Notes

   This enum provides a controlled vocabulary for setting log levels on ArkoudaLogger
   instances. These are mapped internally to the standard Python `logging` levels.


   .. py:attribute:: CRITICAL
      :value: 'CRITICAL'



   .. py:attribute:: DEBUG
      :value: 'DEBUG'



   .. py:attribute:: ERROR
      :value: 'ERROR'



   .. py:attribute:: INFO
      :value: 'INFO'



   .. py:attribute:: WARN
      :value: 'WARN'



.. py:class:: MultiIndex(data: Union[list, tuple, pandas.MultiIndex, MultiIndex], name: Optional[str] = None, names: Optional[Iterable[Union[Hashable, None]]] = None)

   Bases: :py:obj:`Index`


   A multi-level, or hierarchical, index object for Arkouda DataFrames and Series.

   A MultiIndex allows you to represent multiple dimensions of indexing using
   a single object, enabling advanced indexing and grouping operations.

   This class mirrors the behavior of pandas' MultiIndex while leveraging Arkouda's
   distributed data structures. Internally, it stores a list of Index objects,
   each representing one level of the hierarchy.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.pandas.index import MultiIndex
   >>> a = ak.array([1, 2, 3])
   >>> b = ak.array(['a', 'b', 'c'])
   >>> mi = MultiIndex([a, b])
   >>> mi[1]
   MultiIndex([np.int64(2), np.str_('b')])


   .. py:method:: argsort(ascending=True)

      Return the indices that would sort the MultiIndex.

      :param ascending: If False, the result is in descending order.
      :type ascending: bool, default True

      :returns: An array of indices that would sort the MultiIndex.
      :rtype: pdarray



   .. py:method:: concat(other)

      Concatenate this MultiIndex with another, preserving duplicates and order.

      :param other: The other MultiIndex to concatenate with.
      :type other: MultiIndex

      :returns: A new MultiIndex containing values from both inputs, preserving order.
      :rtype: MultiIndex

      :raises TypeError: If the type of `other` does not match.



   .. py:property:: dtype
      :type: numpy.dtype


      Return the dtype object of the underlying data.


   .. py:method:: equal_levels(other: MultiIndex) -> bool

      Return True if the levels of both MultiIndex objects are the same.



   .. py:method:: get_level_values(level: Union[str, int])

      Return the values at a particular level of the MultiIndex.

      :param level: The level number or name. If a string is provided, it must match an entry
                    in `self.names`.
      :type level: int or str

      :returns: An Index object corresponding to the requested level.
      :rtype: Index

      :raises RuntimeError: If `self.names` is None and a string level is provided.
      :raises ValueError: If the provided string is not in `self.names`, or if the level index is out of bounds.



   .. py:property:: index

      Return the levels of the MultiIndex.

      :returns: A list of Index objects representing the levels of the MultiIndex.
      :rtype: list


   .. py:property:: inferred_type
      :type: str


      Return the inferred type of the MultiIndex.

      :returns: The string "mixed", indicating the MultiIndex may contain multiple types.
      :rtype: str


   .. py:method:: is_registered()

      Check if the MultiIndex is registered with the Arkouda server.

      :returns: True if the MultiIndex has a registered name and is recognized by the server,
                False otherwise.
      :rtype: bool



   .. py:attribute:: levels
      :type:  list[Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.pandas.categorical.Categorical]]


   .. py:method:: lookup(key: list[Any] | tuple[Any, Ellipsis]) -> arkouda.pandas.groupbyclass.groupable

      Perform element-wise lookup on the MultiIndex.

      :param key: A sequence of values, one for each level of the MultiIndex.

                  - If the elements are scalars (e.g., ``(1, "red")``), they are
                    treated as a single row key: the result is a boolean mask over
                    rows where all levels match the corresponding scalar.
                  - If the elements are arkouda arrays (e.g., list of pdarrays /
                    Strings), they must align one-to-one with the levels, and the
                    lookup is delegated to ``in1d(self.index, key)`` for multi-column
                    membership.
      :type key: list or tuple

      :returns: A boolean array indicating which rows in the MultiIndex match the key.
      :rtype: groupable

      :raises TypeError: If `key` is not a list or tuple.
      :raises ValueError: If the length of `key` does not match the number of levels.



   .. py:method:: memory_usage(unit='B')

      Return the memory usage of the MultiIndex levels.

      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: Bytes of memory consumed.
      :rtype: int

      .. seealso:: :py:obj:`arkouda.numpy.pdarrayclass.nbytes`, :py:obj:`arkouda.pandas.index.Index.memory_usage`, :py:obj:`arkouda.pandas.series.Series.memory_usage`, :py:obj:`arkouda.pandas.dataframe.DataFrame.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak

      >>> m = ak.pandas.index.MultiIndex([ak.array([1,2,3]),ak.array([4,5,6])])
      >>> m.memory_usage()
      48



   .. py:property:: name

      Return Index or MultiIndex name.


   .. py:property:: names

      Return Index or MultiIndex names.


   .. py:property:: ndim

      Number of dimensions of the underlying data, by definition 1.

      .. seealso:: :py:obj:`Index.ndim`


   .. py:property:: nlevels
      :type: int


      Integer number of levels in this MultiIndex.

      .. seealso:: :py:obj:`Index.nlevels`


   .. py:attribute:: objType
      :value: 'MultiIndex'



   .. py:method:: register(user_defined_name)

      Register this Index object and underlying components with the Arkouda server.

      :param user_defined_name: user defined name the Index is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Index which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Indexes with the same name.
      :rtype: MultiIndex

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Index with the user_defined_name

      .. seealso:: :py:obj:`unregister`, :py:obj:`attach`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Union[str, None]


   .. py:method:: set_dtype(dtype)

      Change the data type of the index.

      Currently only aku.ip_address and ak.array are supported.




   .. py:attribute:: size
      :type:  arkouda.numpy.dtypes.int_scalars


   .. py:method:: to_dict(labels=None)

      Convert the MultiIndex to a dictionary representation.

      :param labels: A list of column names for the index levels. If not provided,
                     defaults to ['idx_0', 'idx_1', ..., 'idx_n'].
      :type labels: list of str, optional

      :returns: A dictionary mapping each label to the corresponding Index object.
      :rtype: dict



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'index', mode: Literal['truncate', 'append'] = 'truncate', file_type: Literal['single', 'distribute'] = 'distribute') -> str

      Save the Index to HDF5.

      The object can be saved to a collection of files or single file.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: {"single" | "distribute"}

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.



   .. py:method:: to_ndarray()

      Convert the MultiIndex to a NumPy ndarray of arrays.

      :returns: A NumPy array where each element is an array corresponding to one level
                of the MultiIndex. Categorical levels are converted to their underlying arrays.
      :rtype: numpy.ndarray



   .. py:method:: to_pandas()

      Convert the MultiIndex to a pandas.MultiIndex object.

      :returns: A pandas MultiIndex with the same levels and names.
      :rtype: pandas.MultiIndex

      .. rubric:: Notes

      Categorical levels are converted to pandas categorical arrays,
      while others are converted to NumPy arrays.



   .. py:method:: tolist()

      Convert the MultiIndex to a list of lists.

      :returns: A list of Python lists, where each inner list corresponds to one level
                of the MultiIndex.
      :rtype: list



   .. py:method:: unregister()

      Unregister this MultiIndex from the Arkouda server.

      :raises RegistrationError: If the MultiIndex is not currently registered.



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'index', repack: bool = True)

      Overwrite the dataset with the name provided with this Index object.

      If the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :raises RuntimeError: Raised if a server-side error is thrown saving the index
      :raises TypeError: Raised if the Index levels are a list.

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, this will create a copy of the
        file with the new data



.. py:class:: Power_divergenceResult

   Bases: :py:obj:`Power_divergenceResult`


   The results of a power divergence statistical test.

   .. attribute:: statistic



      :type: float64

   .. attribute:: pvalue



      :type: float64


.. py:class:: Properties

   Base class for accessor implementations in Arkouda.

   Provides the `_make_op` class method to dynamically generate accessor methods
   that wrap underlying `Strings` or `Datetime` operations and return new Series.

   .. rubric:: Notes

   This class is subclassed by `StringAccessor` and `DatetimeAccessor`, and is not
   intended to be used directly.

   .. rubric:: Examples

   Subclasses should define `_make_op("operation_name")`, which will generate
   a method that applies `series.values.operation_name(...)` and returns a new Series.


.. py:data:: RegisteredSymbols
   :value: '__RegisteredSymbols__'


.. py:class:: Row(dict=None, /, **kwargs)

   Bases: :py:obj:`collections.UserDict`


   A dictionaryâ€like representation of a single row in an Arkouda DataFrame.

   Wraps the columnâ†’value mapping for one row and provides convenient ASCII
   and HTML formatting for display.

   :param data: Mapping of column names to their corresponding values for this row.
   :type data: dict

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.pandas.row import Row
   >>> df = ak.DataFrame({'x': ak.array([10, 20]), 'y': ak.array(['a', 'b'])})

   Suppose df[0] returns {'x': 10, 'y': 'a'}
   >>> row = Row({'x': 10, 'y': 'a'})
   >>> print(row)
   keys    values
   ------  --------
   x       10
   y       a


.. py:class:: Series(data: Union[Tuple, List, arkouda.pandas.groupbyclass.groupable_element_type, Series, arkouda.numpy.segarray.SegArray, pandas.Series, pandas.Categorical], name=None, index: Optional[Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, Tuple, List, arkouda.pandas.index.Index]] = None)

   One-dimensional arkouda array with axis labels.

   :param index: an array of indices associated with the data array.
                 If empty, it will default to a range of ints whose size match the size of the data.
                 optional
   :type index: pdarray, Strings
   :param data: a 1D array. Must not be None.
   :type data: Tuple, List, groupable_element_type, Series, SegArray

   :raises TypeError: Raised if index is not a pdarray or Strings object
       Raised if data is not a pdarray, Strings, or Categorical object
   :raises ValueError: Raised if the index size does not match data size

   .. rubric:: Notes

   The Series class accepts either positional arguments or keyword arguments.
   If entering positional arguments,
       2 arguments entered:
           argument 1 - data
           argument 2 - index
       1 argument entered:
           argument 1 - data
   If entering 1 positional argument, it is assumed that this is the data argument.
   If only 'data' argument is passed in, Index will automatically be generated.
   If entering keywords,
       'data' (see Parameters)
       'index' (optional) must match size of 'data'


   .. py:method:: add(b: Series) -> Series


   .. py:property:: at
      :type: _LocIndexer


      Accesses entries of a Series by label.

      :returns: An indexer for label-based access to Series entries.
      :rtype: _LocIndexer


   .. py:method:: concat(arrays: List, axis: int = 0, index_labels: Union[List[str], None] = None, value_labels: Union[List[str], None] = None, ordered: bool = False) -> Union[arkouda.pandas.dataframe.DataFrame, Series]
      :staticmethod:


      Concatenate a list of Arkouda Series or grouped arrays horizontally or vertically.

      If a list of grouped Arkouda arrays is passed, they are converted to Series. Each grouping
      is a 2-tuple where the first item is the key(s) and the second is the value. If concatenating
      horizontally (axis=1), all series/groupings must have the same length and the same index.
      The index is converted to a column in the resulting DataFrame; if it's a MultiIndex,
      each level is converted to a separate column.

      :param arrays: A list of Series or groupings (tuples of index and values) to concatenate.
      :type arrays: List
      :param axis: The axis to concatenate along:
                   - 0 = vertical (stack series into one)
                   - 1 = horizontal (align by index and produce a DataFrame)
                   Defaults to 0.
      :type axis: int
      :param index_labels: Column name(s) to label the index when axis=1.
      :type index_labels: List[str] or None, optional
      :param value_labels: Column names to label the values of each Series.
      :type value_labels: List[str] or None, optional
      :param ordered: Unused parameter. Reserved for future support of deterministic
                      vs. performance-optimized concatenation. Defaults to False.
      :type ordered: bool

      :returns:

                - If axis=0: a new Series
                - If axis=1: a new DataFrame
      :rtype: Series or DataFrame



   .. py:method:: diff() -> Series

      Diffs consecutive values of the series.

      Returns a new series with the same index and length.  First value is set to NaN.



   .. py:attribute:: dt


   .. py:property:: dtype
      :type: numpy.dtype



   .. py:method:: fillna(value: Union[supported_scalars, Series, arkouda.numpy.pdarrayclass.pdarray]) -> Series

      Fill NA/NaN values using the specified method.

      :param value: Value to use to fill holes (e.g. 0), alternately a
                    Series of values specifying which value to use for
                    each index.  Values not in the Series will not be filled.
                    This value cannot be a list.
      :type value: supported_scalars, Series, or pdarray

      :returns: Object with missing values filled.
      :rtype: Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda import Series
      >>> import numpy as np

      >>> data = ak.Series([1, np.nan, 3, np.nan, 5])
      >>> data
      0    1.0
      1    NaN
      2    3.0
      3    NaN
      4    5.0
      dtype: float64

      >>> fill_values1 = ak.ones(5)
      >>> data.fillna(fill_values1)
      0    1.0
      1    1.0
      2    3.0
      3    1.0
      4    5.0
      dtype: float64

      >>> fill_values2 = Series(ak.ones(5))
      >>> data.fillna(fill_values2)
      0    1.0
      1    1.0
      2    3.0
      3    1.0
      4    5.0
      dtype: float64

      >>> fill_values3 = 100.0
      >>> data.fillna(fill_values3)
      0      1.0
      1    100.0
      2      3.0
      3    100.0
      4      5.0
      dtype: float64



   .. py:method:: from_return_msg(rep_msg: str) -> Series
      :classmethod:


      Return a Series instance pointing to components created by the arkouda server.

      The user should not call this function directly.

      :param rep_msg:
                      + delimited string containing the values and indexes.
      :type rep_msg: builtin_str

      :returns: A Series representing a set of pdarray components on the server.
      :rtype: Series

      :raises RuntimeError: Raised if a server-side error is thrown in the process of creating
          the Series instance.



   .. py:method:: has_repeat_labels() -> bool

      Return whether the Series has any labels that appear more than once.



   .. py:method:: hasnans() -> arkouda.numpy.dtypes.bool_scalars

      Return True if there are any NaNs.

      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = ak.Series(ak.array([1, 2, 3, np.nan]))
      >>> s
      0    1.0
      1    2.0
      2    3.0
      3    NaN
      dtype: float64

      >>> s.hasnans()
      np.True_



   .. py:method:: head(n: int = 10) -> Series

      Return the first n values of the series.



   .. py:property:: iat
      :type: _iLocIndexer


      Accesses entries of a Series by position.

      :returns: An indexer for position-based access to a single element.
      :rtype: _iLocIndexer


   .. py:property:: iloc
      :type: _iLocIndexer


      Accesses entries of a Series by position.

      :returns: An indexer for position-based access to Series entries.
      :rtype: _iLocIndexer


   .. py:method:: is_registered() -> bool

      Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :py:obj:`register`, :py:obj:`attach`, :py:obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isin(lst: Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, List]) -> Series

      Find Series elements whose values are in the specified list.

      :param lst: Either a Python list or an Arkouda array to check membership against.
      :type lst: pdarray, Strings, or List

      :returns: A Series of booleans that is True for elements found in the list,
                and False otherwise.
      :rtype: Series



   .. py:method:: isna() -> Series

      Detect missing values.

      Return a boolean same-sized object indicating if the values are NA. NA values,
      such as numpy.NaN, gets mapped to True values.
      Everything else gets mapped to False values.
      Characters such as empty strings '' are not considered NA values.

      :returns: Mask of bool values for each element in Series
                that indicates whether an element is an NA value.
      :rtype: Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = Series(ak.array([1, 2, np.nan]), index = ak.array([1, 2, 4]))
      >>> s.isna()
      1    False
      2    False
      4     True
      dtype: bool



   .. py:method:: isnull() -> Series

      Series.isnull is an alias for Series.isna.

      Detect missing values.

      Return a boolean same-sized object indicating if the values are NA. NA values,
      such as numpy.NaN, gets mapped to True values.
      Everything else gets mapped to False values.
      Characters such as empty strings '' are not considered NA values.

      :returns: Mask of bool values for each element in Series
                that indicates whether an element is an NA value.
      :rtype: Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = Series(ak.array([1, 2, np.nan]), index = ak.array([1, 2, 4]))
      >>> s.isnull()
      1    False
      2    False
      4     True
      dtype: bool



   .. py:property:: loc
      :type: _LocIndexer


      Accesses entries of a Series by label.

      :returns: An indexer for label-based access to Series entries.
      :rtype: _LocIndexer


   .. py:method:: locate(key: Union[int, arkouda.numpy.pdarrayclass.pdarray, arkouda.pandas.index.Index, Series, List, Tuple]) -> Series

      Lookup values by index label.

      :param key: The key or keys to look up. This can be:
                  - A scalar
                  - A list of scalars
                  - A list of lists (for MultiIndex)
                  - A Series (in which case labels are preserved, and its values are used as keys)

                  Keys will be converted to Arkouda arrays as needed.
      :type key: int, pdarray, Index, Series, List, or Tuple

      :returns: A Series containing the values corresponding to the key.
      :rtype: Series



   .. py:method:: map(arg: Union[dict, arkouda.Series]) -> arkouda.Series

      Map values of Series according to an input mapping.

      :param arg: The mapping correspondence.
      :type arg: dict or Series

      :returns: A new series with the same index as the caller.
                When the input Series has Categorical values,
                the return Series will have Strings values.
                Otherwise, the return type will match the input type.
      :rtype: Series

      :raises TypeError: Raised if arg is not of type dict or arkouda.Series.
          Raised if series values not of type pdarray, Categorical, or Strings.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> s = ak.Series(ak.array([2, 3, 2, 3, 4]))
      >>> s
      0    2
      1    3
      2    2
      3    3
      4    4
      dtype: int64

      >>> s.map({4: 25.0, 2: 30.0, 1: 7.0, 3: 5.0})
      0    30.0
      1     5.0
      2    30.0
      3     5.0
      4    25.0
      dtype: float64

      >>> s2 = ak.Series(ak.array(["a","b","c","d"]), index = ak.array([4,2,1,3]))
      >>> s.map(s2)
      0    b
      1    d
      2    b
      3    d
      4    a
      dtype: ...



   .. py:method:: memory_usage(index: bool = True, unit: Literal['B', 'KB', 'MB', 'GB'] = 'B') -> int

      Return the memory usage of the Series.

      The memory usage can optionally include the contribution of
      the index.

      :param index: Specifies whether to include the memory usage of the Series index.
                    Defaults to True.
      :type index: bool
      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}. Defaults to "B".
      :type unit: {"B", "KB", "MB", "GB"}

      :returns: Bytes of memory consumed.
      :rtype: int

      .. seealso:: :py:obj:`arkouda.numpy.pdarrayclass.nbytes`, :py:obj:`arkouda.Index.memory_usage`, :py:obj:`arkouda.pandas.series.Series.memory_usage`, :py:obj:`arkouda.pandas.datafame.DataFrame.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda.pandas.series import Series
      >>> s = ak.Series(ak.arange(3))
      >>> s.memory_usage()
      48

      Not including the index gives the size of the rest of the data, which
      is necessarily smaller:

      >>> s.memory_usage(index=False)
      24

      Select the units:

      >>> s = ak.Series(ak.arange(3000))
      >>> s.memory_usage(unit="KB")
      46.875



   .. py:property:: ndim
      :type: int



   .. py:method:: notna() -> Series

      Detect existing (non-missing) values.

      Return a boolean same-sized object indicating if the values are not NA.
      Non-missing values get mapped to True.
      Characters such as empty strings '' are not considered NA values.
      NA values, such as numpy.NaN, get mapped to False values.

      :returns: Mask of bool values for each element in Series
                that indicates whether an element is not an NA value.
      :rtype: Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = Series(ak.array([1, 2, np.nan]), index = ak.array([1, 2, 4]))
      >>> s.notna()
      1     True
      2     True
      4    False
      dtype: bool



   .. py:method:: notnull() -> Series

      Series.notnull is an alias for Series.notna.

      Detect existing (non-missing) values.

      Return a boolean same-sized object indicating if the values are not NA.
      Non-missing values get mapped to True.
      Characters such as empty strings '' are not considered NA values.
      NA values, such as numpy.NaN, get mapped to False values.

      :returns: Mask of bool values for each element in Series
                that indicates whether an element is not an NA value.
      :rtype: Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = Series(ak.array([1, 2, np.nan]), index = ak.array([1, 2, 4]))
      >>> s.notnull()
      1     True
      2     True
      4    False
      dtype: bool



   .. py:attribute:: objType
      :value: 'Series'



   .. py:method:: pdconcat(arrays: List, axis: int = 0, labels: Union[arkouda.numpy.strings.Strings, None] = None) -> Union[pandas.Series, pandas.DataFrame]
      :staticmethod:


      Concatenate a list of Arkouda Series or grouped arrays, returning a local pandas object.

      If a list of grouped Arkouda arrays is passed, they are converted to Series. Each grouping
      is a 2-tuple with the first item being the key(s) and the second the value.

      If `axis=1` (horizontal), each Series or grouping must have the same length and the same index.
      The index is converted to a column in the resulting DataFrame. If it is a MultiIndex,
      each level is converted to a separate column.

      :param arrays: A list of Series or groupings (tuples of index and values) to concatenate.
      :type arrays: List
      :param axis: The axis along which to concatenate:
                   - 0 = vertical (stack into a Series)
                   - 1 = horizontal (align by index into a DataFrame)
                   Defaults to 0.
      :type axis: int
      :param labels: Names to assign to the resulting columns in the DataFrame.
      :type labels: Strings or None, optional

      :returns:

                - If axis=0: a local pandas Series
                - If axis=1: a local pandas DataFrame
      :rtype: Series or DataFrame



   .. py:method:: register(user_defined_name: str)

      Register this Series object and underlying components with the Arkouda server.

      :param user_defined_name: User-defined name the Series is to be registered under.
                                This will be the root name for the underlying components.
      :type user_defined_name: builtin_str

      :returns: The same Series which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Series with the same name.
      :rtype: Series

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Series with the user_defined_name

      .. seealso:: :py:obj:`unregister`, :py:obj:`attach`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:property:: shape
      :type: Tuple[int]



   .. py:attribute:: size


   .. py:method:: sort_index(ascending: bool = True) -> Series

      Sort the Series by its index.

      :param ascending: Whether to sort the index in ascending (default) or descending order.
                        Defaults to True.
      :type ascending: bool

      :returns: A new Series sorted by index.
      :rtype: Series



   .. py:method:: sort_values(ascending: bool = True) -> Series

      Sort the Series by its values.

      :param ascending: Whether to sort values in ascending (default) or descending order.
                        Defaults to True.
      :type ascending: bool

      :returns: A new Series sorted by its values.
      :rtype: Series



   .. py:attribute:: str


   .. py:method:: tail(n: int = 10) -> Series

      Return the last n values of the series.



   .. py:method:: to_dataframe(index_labels: Union[List[str], None] = None, value_label: Union[str, None] = None) -> arkouda.pandas.dataframe.DataFrame

      Convert the Series to an Arkouda DataFrame.

      :param index_labels: Column name(s) to label the index.
      :type index_labels: list of str or None, optional
      :param value_label: Column name to label the values.
      :type value_label: str or None, optional

      :returns: An Arkouda DataFrame representing the Series.
      :rtype: DataFrame



   .. py:method:: to_markdown(mode='wt', index=True, tablefmt='grid', storage_options=None, **kwargs)

      Print Series in Markdown-friendly format.

      :param mode: Mode in which file is opened, "wt" by default.
      :type mode: str, optional
      :param index: Add index (row) labels.
      :type index: bool, optional, default True
      :param tablefmt: Table format to call from tablulate:
                       https://pypi.org/project/tabulate/
      :type tablefmt: str = "grid"
      :param storage_options: Extra options that make sense for a particular storage connection,
                              e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec,
                              e.g., starting â€œs3://â€, â€œgcs://â€.
                              An error will be raised if providing this argument with a non-fsspec URL.
                              See the fsspec and backend storage implementation docs for the set
                              of allowed keys and values.
      :type storage_options: dict, optional
      :param \*\*kwargs: These parameters will be passed to tabulate.

      .. note::

         This function should only be called on small Series as it calls pandas.Series.to_markdown:
         https://pandas.pydata.org/docs/reference/api/pandas.Series.to_markdown.html

      .. rubric:: Examples

      >>> import arkouda as ak

      >>> s = ak.Series(["elk", "pig", "dog", "quetzal"], name="animal")
      >>> print(s.to_markdown())
      +----+----------+
      |    | animal   |
      +====+==========+
      |  0 | elk      |
      +----+----------+
      |  1 | pig      |
      +----+----------+
      |  2 | dog      |
      +----+----------+
      |  3 | quetzal  |
      +----+----------+

      Output markdown with a tabulate option.

      >>> print(s.to_markdown(tablefmt="grid"))
      +----+----------+
      |    | animal   |
      +====+==========+
      |  0 | elk      |
      +----+----------+
      |  1 | pig      |
      +----+----------+
      |  2 | dog      |
      +----+----------+
      |  3 | quetzal  |
      +----+----------+



   .. py:method:: to_ndarray() -> numpy.ndarray


   .. py:method:: to_pandas() -> pandas.Series

      Convert the series to a local PANDAS series.



   .. py:method:: tolist() -> list


   .. py:method:: topn(n: int = 10) -> Series

      Return the top values of the Series.

      :param n: Number of values to return. Defaults to 10.
      :type n: int

      :returns: A new Series containing the top `n` values.
      :rtype: Series



   .. py:method:: unregister()

      Unregister this Series object in the arkouda server which was previously
      registered using register() and/or attached to using attach().

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :py:obj:`register`, :py:obj:`attach`, :py:obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: validate_key(key: Union[Series, arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.pandas.categorical.Categorical, List, supported_scalars, arkouda.numpy.segarray.SegArray]) -> Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.pandas.categorical.Categorical, supported_scalars, arkouda.numpy.segarray.SegArray]

      Validate type requirements for keys when reading or writing the Series.

      Also converts list and tuple arguments into pdarrays.

      :param key: The key or container of keys that might be used to index into the Series.
      :type key: Series, pdarray, Strings, Categorical, List, supported_scalars, or SegArray

      :rtype: The validated key(s), with lists and tuples converted to pdarrays

      :raises TypeError: Raised if keys are not boolean values or the type of the labels
          Raised if key is not one of the supported types
      :raises KeyError: Raised if container of keys has keys not present in the Series
      :raises IndexError: Raised if the length of a boolean key array is different
          from the Series



   .. py:method:: validate_val(val: Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, supported_scalars, List]) -> Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, supported_scalars]

      Validate type requirements for values being written into the Series.

      Also converts list and tuple arguments into pdarrays.

      :param val: The value or container of values that might be assigned into the Series.
      :type val: pdarray, Strings, supported_scalars, or List

      :rtype: The validated value, with lists converted to pdarrays

      :raises TypeError: Raised if val is not the same type or a container with elements
            of the same time as the Series
          Raised if val is a string or Strings type.
          Raised if val is not one of the supported types



   .. py:method:: value_counts(sort: bool = True) -> Series

      Return a Series containing counts of unique values.

      :param sort: Whether to sort the result by count in descending order. If False,
                   the order of the results is not guaranteed. Defaults to True.
      :type sort: bool

      :returns: A Series where the index contains the unique values and the values are
                their counts in the original Series.
      :rtype: Series



.. py:class:: StringAccessor(series)

   Bases: :py:obj:`Properties`


   Accessor for string operations on Arkouda Series.

   Provides string-like methods such as `.contains()`, `.startswith()`, and
   `.endswith()` via the `.str` accessor, similar to pandas.

   This accessor is automatically attached to Series objects that wrap
   `arkouda.Strings` or `arkouda.Categorical` values. It should not be instantiated directly.

   :param series: The Series object containing `Strings` or `Categorical` values.
   :type series: arkouda.pandas.Series

   :raises AttributeError: If the underlying Series values are not `Strings` or `Categorical`.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda import Series
   >>> s = Series(["apple", "banana", "apricot"])
   >>> s.str.startswith("a")
   0     True
   1    False
   2     True
   dtype: bool


   .. py:attribute:: series


.. py:function:: apply(arr: arkouda.numpy.pdarrayclass.pdarray, func: Union[Callable, str], result_dtype: Optional[Union[numpy.dtype, str]] = None) -> arkouda.numpy.pdarrayclass.pdarray

   Apply a python function to a pdarray.

   The function should take one argument
   and return a new value. The function will then be called on each element in
   the pdarray.



   Warning: This function is experimental and may not work as expected.
   Known limitations:
   - Any python modules used inside of the function must be installed on the server.

   :param arr: The pdarray to which the function is applied
   :type arr: pdarray
   :param func: The function to apply to the array. This can be a callable function or
                a string, but either way it should take a single argument and return a
                single value. If a string, it should be a lambda function that takes a
                single argument, e.g. "lambda x,: x+1". Note the dangling comma after
                the argument, this is required for string functions.
   :type func: Union[Callable, str]
   :param result_dtype: The dtype of the resulting pdarray. If None, the dtype of the resulting
                        pdarray will be the same as the input pdarray. If a string, it should be
                        a valid numpy dtype string, e.g. "float64". If a numpy dtype, it should
                        be a valid numpy dtype object, e.g. np.float64. This is not supported
                        for functions passed as strings.
   :type result_dtype: Optional[Union[np.dtype, str]]

   :returns: The pdarray resulting from applying the function to the input array
   :rtype: pdarray

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> arr = ak.apply(ak.array([1, 2, 3]), lambda x: x+1)
   >>> arr
   array([2 3 4])

   Or,
   >>> import math
   >>> arr = ak.randint(0, 10, 4, seed=1)
   >>> def times_pi(x):
   ...        return x*math.pi
   >>> arr = ak.apply(arr, times_pi, "float64")
   >>> arr
   array([21.991148575128552 28.274333882308138 15.707963267948966 3.1415926535897931])


.. py:function:: assert_almost_equal(left, right, rtol: float = 1e-05, atol: float = 1e-08, **kwargs) -> None

   Check that the left and right objects are approximately equal.

   By approximately equal, we refer to objects that are numbers or that
   contain numbers which may be equivalent to specific levels of precision.

   :param left:
   :type left: object
   :param right:
   :type right: object
   :param rtol: Relative tolerance.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance.
   :type atol: float, default 1e-8

   .. warning::

      This function cannot be used on pdarray of size > ak.core.client.maxTransferBytes
      because it converts pdarrays to numpy arrays and calls np.allclose.


.. py:function:: assert_almost_equivalent(left, right, rtol: float = 1e-05, atol: float = 1e-08) -> None

   Check that two objects are approximately equal.

   By approximately equal, we refer to objects that are numbers or that
   contain numbers which may be equivalent to specific levels of precision.

   If the objects are pandas or numpy objects, they are converted to Arkouda objects.
   Then assert_almost_equal is applied to the result.

   :param left: First object to compare.
   :type left: object
   :param right: Second object to compare.
   :type right: object
   :param rtol: Relative tolerance. Default is 1e-5.
   :type rtol: float
   :param atol: Absolute tolerance. Default is 1e-8.
   :type atol: float

   :raises TypeError: If either input is not a supported numeric-like type.

   .. warning::

      This function cannot be used on pdarrays of size > ak.core.client.maxTransferBytes
      because it converts pdarrays to numpy arrays and calls np.allclose.

   .. seealso:: :py:obj:`assert_almost_equal`

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.testing import assert_almost_equivalent
   >>> assert_almost_equivalent(0.123456, 0.123457, rtol=1e-4)


.. py:function:: assert_arkouda_array_equal(left: arkouda.pdarray | arkouda.Strings | arkouda.Categorical | arkouda.SegArray, right: arkouda.pdarray | arkouda.Strings | arkouda.Categorical | arkouda.SegArray, check_dtype: bool = True, err_msg=None, check_same=None, obj: str = 'pdarray', index_values=None) -> None

   Check that two Arkouda arrays are equivalent. Supports pdarray, Strings,
   Categorical, and SegArray.

   :param left: The first array to compare.
   :type left: pdarray or Strings or Categorical or SegArray
   :param right: The second array to compare.
   :type right: pdarray or Strings or Categorical or SegArray
   :param check_dtype: Whether to check dtype if both `left` and `right` are ak.pdarray.
                       Defaults to True.
   :type check_dtype: bool
   :param err_msg: Custom assertion message, if provided. Defaults to None.
   :type err_msg: str or None
   :param check_same: If not None, assert whether `left` and `right` share the same memory.
                      - `'copy'`: assert that they do **not** share memory.
                      - `'same'`: assert that they **do** share memory.
                      Defaults to None.
   :type check_same: {'copy', 'same'} or None
   :param obj: Object name used in assertion error messages. Defaults to 'pdarray'.
   :type obj: str
   :param index_values: Optional index shared by both `left` and `right`, used to enhance
                        output in error messages. Defaults to None.
   :type index_values: Index or pdarray or None


.. py:function:: assert_arkouda_array_equivalent(left: arkouda.pdarray | arkouda.Strings | arkouda.Categorical | arkouda.SegArray | numpy.ndarray | pandas.Categorical, right: arkouda.pdarray | arkouda.Strings | arkouda.Categorical | arkouda.SegArray | numpy.ndarray | pandas.Categorical, check_dtype: bool = True, err_msg=None, check_same=None, obj: str = 'pdarray', index_values=None) -> None

    Check that two Arkouda-compatible arrays are equal.

   Supported types include numpy arrays, pandas Categorical, and Arkouda arrays.

   :param left: First array to compare.
   :type left: pdarray, Strings, Categorical, SegArray, np.ndarray, or pd.Categorical
   :param right: Second array to compare.
   :type right: pdarray, Strings, Categorical, SegArray, np.ndarray, or pd.Categorical
   :param check_dtype: Whether to verify that dtypes match. Default is True.
   :type check_dtype: bool
   :param err_msg: Optional message to display on failure.
   :type err_msg: str or None
   :param check_same: Whether to ensure identity or separation in memory. Default is None.
   :type check_same: None or {"copy", "same"}
   :param obj: Object label for error messages. Default is "pdarray".
   :type obj: str
   :param index_values: Shared index used in error output. Default is None.
   :type index_values: Index or pdarray, optional

   :raises TypeError: If either input is not a supported array type.

   .. seealso:: :py:obj:`assert_arkouda_array_equal`

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda import Strings
   >>> from arkouda.testing import assert_arkouda_array_equivalent
   >>> a = ak.array([1, 2, 3])
   >>> b = ak.array([1, 2, 3])
   >>> assert_arkouda_array_equivalent(a, b)
   >>> s1 = ak.array(['x', 'y'])
   >>> s2 = ak.array(['x', 'y'])
   >>> assert_arkouda_array_equivalent(s1, s2)


.. py:function:: assert_arkouda_pdarray_equal(left: arkouda.pdarray, right: arkouda.pdarray, check_dtype: bool = True, err_msg=None, check_same=None, obj: str = 'pdarray', index_values=None) -> None

   Check that two Arkouda pdarray objects are equivalent.

   :param left: The first array to compare.
   :type left: pdarray
   :param right: The second array to compare.
   :type right: pdarray
   :param check_dtype: Whether to check dtype if both arrays are pdarrays. Defaults to True.
   :type check_dtype: bool
   :param err_msg: Custom assertion message to display on failure. Defaults to None.
   :type err_msg: str or None
   :param check_same: If not None, asserts whether `left` and `right` share the same memory:
                      - 'copy': assert they do **not** share memory
                      - 'same': assert they **do** share memory
                      Defaults to None.
   :type check_same: {'copy', 'same'} or None
   :param obj: A name for the object being compared, used in assertion messages.
               Defaults to 'pdarray'.
   :type obj: str
   :param index_values: Optional index shared by both arrays, used to enhance output on failure.
                        Defaults to None.
   :type index_values: Index or pdarray or None


.. py:function:: assert_arkouda_segarray_equal(left: arkouda.SegArray, right: arkouda.SegArray, check_dtype: bool = True, err_msg=None, check_same=None, obj: str = 'segarray') -> None

   Check that two Arkouda SegArray objects are equivalent.

   :param left: The first SegArray to compare.
   :type left: SegArray
   :param right: The second SegArray to compare.
   :type right: SegArray
   :param check_dtype: Whether to check dtype if both arrays contain pdarrays. Defaults to True.
   :type check_dtype: bool
   :param err_msg: Custom assertion message. Defaults to None.
   :type err_msg: str or None
   :param check_same: If not None, asserts whether `left` and `right` share the same memory.
                      - 'copy': assert that they do **not** share memory.
                      - 'same': assert that they **do** share memory.
                      Defaults to None.
   :type check_same: {'copy', 'same'} or None
   :param obj: Name of the object being compared (used in assertion messages).
               Defaults to 'segarray'.
   :type obj: str


.. py:function:: assert_arkouda_strings_equal(left, right, err_msg=None, check_same=None, obj: str = 'Strings', index_values=None) -> None

   Check that two `ak.Strings` arrays are equivalent.

   :param left: The first Strings object to compare.
   :type left: Strings
   :param right: The second Strings object to compare.
   :type right: Strings
   :param err_msg: Custom assertion message. Defaults to None.
   :type err_msg: str or None
   :param check_same: If not None, assert whether `left` and `right` share the same memory.
                      - 'copy': assert that they do **not** share memory
                      - 'same': assert that they **do** share memory
                      Defaults to None.
   :type check_same: {'copy', 'same'} or None
   :param obj: A name for the object being compared, used in assertion messages.
               Defaults to 'Strings'.
   :type obj: str
   :param index_values: Optional index shared by both arrays, used in output. Defaults to None.
   :type index_values: Index or pdarray or None


.. py:function:: assert_attr_equal(attr: str, left, right, obj: str = 'Attributes') -> None

   Check that attributes are equal. Both objects must have the given attribute.

   :param attr: The name of the attribute being compared.
   :type attr: str
   :param left: The first object to compare.
   :type left: object
   :param right: The second object to compare.
   :type right: object
   :param obj: A name for the object being compared, used in assertion messages.
               Defaults to 'Attributes'.
   :type obj: str


.. py:function:: assert_categorical_equal(left, right, check_dtype: bool = True, check_category_order: bool = True, obj: str = 'Categorical') -> None

   Test that Categoricals are equivalent.

   :param left: The first Categorical to compare.
   :type left: Categorical
   :param right: The second Categorical to compare.
   :type right: Categorical
   :param check_dtype: Whether to check that the integer dtype of the codes is the same.
                       Defaults to True.
   :type check_dtype: bool
   :param check_category_order: Whether to compare the order of the categories (which implies identical integer codes).
                                If False, only the resulting values are compared. The `ordered` attribute is
                                always checked. Defaults to True.
   :type check_category_order: bool
   :param obj: A name for the object being compared, used in assertion messages.
               Defaults to 'Categorical'.
   :type obj: str


.. py:function:: assert_class_equal(left, right, exact: bool = True, obj: str = 'Input') -> None

   Check classes are equal.


.. py:function:: assert_contains_all(iterable, dic) -> None

   Assert that a dictionary contains all the elements of an iterable.

   :param iterable:
   :type iterable: iterable
   :param dic:
   :type dic: dict


.. py:function:: assert_copy(iter1, iter2, **eql_kwargs) -> None

   Check that the elements are equal, but not the same object.

   Does not check that items in sequences are also not the same object.

   :param iter1: Iterables that produce elements comparable with assert_almost_equal.
   :type iter1: iterable
   :param iter2: Iterables that produce elements comparable with assert_almost_equal.
   :type iter2: iterable


.. py:function:: assert_dict_equal(left, right, compare_keys: bool = True) -> None

   Assert that two dictionaries are equal.

   Values must be arkouda objects.

   :param left: The dictionaries to be compared.
   :type left: dict
   :param right: The dictionaries to be compared.
   :type right: dict
   :param compare_keys: Whether to compare the keys. Defaults to True.
                        If False, only the values are compared.
   :type compare_keys: bool


.. py:function:: assert_equal(left, right, **kwargs) -> None

   Wrap tm.assert_*_equal to dispatch to the appropriate test function.

   :param left: The two items to be compared.
   :type left: Index, Series, DataFrame, or pdarray
   :param right: The two items to be compared.
   :type right: Index, Series, DataFrame, or pdarray
   :param \*\*kwargs: All keyword arguments are passed through to the underlying assert method.


.. py:function:: assert_equivalent(left, right, **kwargs) -> None

   Dispatch to the appropriate assertion function depending on object types.

   :param left: First object to compare. Type determines which assertion function is used.
   :type left: Any
   :param right: Second object to compare.
   :type right: Any
   :param \*\*kwargs: Keyword arguments passed to the specific assertion function.
   :type \*\*kwargs: dict

   :raises AssertionError: If values are not equivalent.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> import pandas as pd
   >>> from arkouda.testing import assert_equivalent
   >>> ak_series = ak.Series([1, 2, 3])
   >>> pd_series = pd.Series([1, 2, 3])
   >>> assert_equivalent(ak_series, pd_series)


.. py:function:: assert_frame_equal(left: arkouda.DataFrame, right: arkouda.DataFrame, check_dtype: bool = True, check_index_type: bool = True, check_column_type: bool | Literal['equiv'] = 'equiv', check_frame_type: bool = True, check_names: bool = True, check_exact: bool = True, check_categorical: bool = True, check_like: bool = False, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'DataFrame') -> None

   Check that left and right DataFrame are equal.

   This function is intended to compare two DataFrames and output any
   differences. It is mostly intended for use in unit tests.
   Additional parameters allow varying the strictness of the
   equality checks performed.

   :param left: First DataFrame to compare.
   :type left: DataFrame
   :param right: Second DataFrame to compare.
   :type right: DataFrame
   :param check_dtype: Whether to check the DataFrame dtype is identical. Defaults to True.
   :type check_dtype: bool
   :param check_index_type: Whether to check the Index class, dtype, and inferred_type are identical.
                            Defaults to True.
   :type check_index_type: bool
   :param check_column_type: Whether to check the column class, dtype, and inferred_type are identical.
                             Passed as the ``exact`` argument of :func:`assert_index_equal`.
                             Defaults to 'equiv'.
   :type check_column_type: bool or {'equiv'}
   :param check_frame_type: Whether to check the DataFrame class is identical. Defaults to True.
   :type check_frame_type: bool
   :param check_names: Whether to check that the `names` attribute for both the `index`
                       and `column` attributes of the DataFrame is identical. Defaults to True.
   :type check_names: bool
   :param check_exact: Whether to compare numbers exactly. Defaults to False.
   :type check_exact: bool
   :param check_categorical: Whether to compare internal Categoricals exactly. Defaults to True.
   :type check_categorical: bool
   :param check_like: If True, ignore the order of index and columns.
                      Note: index labels must match their respective rows (as in columns);
                      same labels must be with the same data. Defaults to False.
   :type check_like: bool
   :param rtol: Relative tolerance. Only used when `check_exact` is False. Defaults to 1e-5.
   :type rtol: float
   :param atol: Absolute tolerance. Only used when `check_exact` is False. Defaults to 1e-8.
   :type atol: float
   :param obj: A name for the object being compared, used in assertion messages.
               Defaults to 'DataFrame'.
   :type obj: str

   .. seealso::

      :py:obj:`assert_series_equal`
          Equivalent method for asserting Series equality.

   .. rubric:: Examples

   >>> import arkouda as ak

   This example shows comparing two DataFrames that are equal
   but with columns of differing dtypes.

   >>> from arkouda.testing import assert_frame_equal
   >>> df1 = ak.DataFrame({'a': [1, 2], 'b': [3, 4]})
   >>> df2 = ak.DataFrame({'a': [1, 2], 'b': [3.0, 4.0]})

   df1 equals itself.
   >>> assert_frame_equal(df1, df1)

   df1 differs from df2 as column 'b' is of a different type.
   >>> assert_frame_equal(df1, df2) # doctest: +SKIP
   Traceback (most recent call last):
   ...
   AssertionError: Attributes of DataFrame.iloc[:, 1] (column name="b") are different

   Attribute "dtype" are different
   [left]:  int64
   [right]: float64

   Ignore differing dtypes in columns with check_dtype.

   >>> assert_frame_equal(df1, df2, check_dtype=False)


.. py:function:: assert_frame_equivalent(left: arkouda.DataFrame | pandas.DataFrame, right: arkouda.DataFrame | pandas.DataFrame, check_dtype: bool = True, check_index_type: bool = True, check_column_type: bool = True, check_frame_type: bool = True, check_names: bool = True, check_exact: bool = True, check_categorical: bool = True, check_like: bool = False, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'DataFrame') -> None

   Check that two DataFrames are equal.

   This function compares two DataFrames and raises an assertion if they differ.
   It is intended primarily for use in unit tests. pandas DataFrames are converted to
   Arkouda equivalents before comparison.

   :param left: First DataFrame to compare.
   :type left: DataFrame or pd.DataFrame
   :param right: Second DataFrame to compare.
   :type right: DataFrame or pd.DataFrame
   :param check_dtype: Whether to check that dtypes are identical. Default is True.
   :type check_dtype: bool
   :param check_index_type: Whether to check that index class, dtype, and inferred type are identical. Default is True.
   :type check_index_type: bool
   :param check_column_type: Whether to check that column class, dtype, and inferred type are identical. Default is True.
   :type check_column_type: bool
   :param check_frame_type: Whether to check that the DataFrame class is identical. Default is True.
   :type check_frame_type: bool
   :param check_names: Whether to check that the index and column names are identical. Default is True.
   :type check_names: bool
   :param check_exact: Whether to compare values exactly. Default is True.
   :type check_exact: bool
   :param check_categorical: Whether to compare internal categoricals exactly. Default is True.
   :type check_categorical: bool
   :param check_like: Whether to ignore the order of index and columns. Labels must still match their data. /
                      Default is False.
   :type check_like: bool
   :param rtol: Relative tolerance used when check_exact is False. Default is 1e-5.
   :type rtol: float
   :param atol: Absolute tolerance used when check_exact is False. Default is 1e-8.
   :type atol: float
   :param obj: Object name used in error messages. Default is "DataFrame".
   :type obj: str

   :raises TypeError: If either input is not a DataFrame or pd.DataFrame.

   .. seealso:: :py:obj:`assert_frame_equal`

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> import pandas as pd
   >>> from arkouda.testing import assert_frame_equivalent
   >>> df1 = ak.DataFrame({'a': [1, 2], 'b': [3, 4]})
   >>> df2 = pd.DataFrame({'a': [1, 2], 'b': [3.0, 4.0]})

   Fails because dtypes are different:
   >>> assert_frame_equivalent(df1, df2)  # doctest: +SKIP


.. py:function:: assert_index_equal(left: arkouda.Index, right: arkouda.Index, exact: bool = True, check_names: bool = True, check_exact: bool = True, check_categorical: bool = True, check_order: bool = True, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'Index') -> None

   Check that left and right Index are equal.

   :param left: The first Index to compare.
   :type left: Index
   :param right: The second Index to compare.
   :type right: Index
   :param exact: Whether to check that the Index class, dtype, and inferred_type
                 are identical. Defaults to True.
   :type exact: bool
   :param check_names: Whether to check the `name` attribute. Defaults to True.
   :type check_names: bool
   :param check_exact: Whether to compare numbers exactly. Defaults to True.
   :type check_exact: bool
   :param check_categorical: Whether to compare internal Categorical values exactly. Defaults to True.
   :type check_categorical: bool
   :param check_order: Whether to compare the order of index entries as well as their values.
                       If True, both indexes must contain the same elements, in the same order.
                       If False, both indexes must contain the same elements, but in any order.
                       Defaults to True.
   :type check_order: bool
   :param rtol: Relative tolerance. Only used when `check_exact` is False.
                Defaults to 1e-5.
   :type rtol: float
   :param atol: Absolute tolerance. Only used when `check_exact` is False.
                Defaults to 1e-8.
   :type atol: float
   :param obj: A name for the object being compared, used in assertion messages.
               Defaults to 'Index'.
   :type obj: str

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda import testing as tm
   >>> a = ak.Index([1, 2, 3])
   >>> b = ak.Index([1, 2, 3])
   >>> tm.assert_index_equal(a, b)


.. py:function:: assert_index_equivalent(left: arkouda.Index | pandas.Index, right: arkouda.Index | pandas.Index, exact: bool = True, check_names: bool = True, check_exact: bool = True, check_categorical: bool = True, check_order: bool = True, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'Index') -> None

   Check that two Index objects are equal.

   If the objects are pandas Index, they are converted to Arkouda Index.
   Then assert_index_equal is applied to the result.

   :param left: First Index to compare.
   :type left: Index or pd.Index
   :param right: Second Index to compare.
   :type right: Index or pd.Index
   :param exact: Whether to check that class, dtype, and inferred type are identical. Default is True.
   :type exact: bool
   :param check_names: Whether to check the names attribute. Default is True.
   :type check_names: bool
   :param check_exact: Whether to compare values exactly. Default is True.
   :type check_exact: bool
   :param check_categorical: Whether to compare internal Categoricals exactly. Default is True.
   :type check_categorical: bool
   :param check_order: Whether to require identical order in index values. Default is True.
   :type check_order: bool
   :param rtol: Relative tolerance used when check_exact is False. Default is 1e-5.
   :type rtol: float
   :param atol: Absolute tolerance used when check_exact is False. Default is 1e-8.
   :type atol: float
   :param obj: Object name used in error messages. Default is "Index".
   :type obj: str

   :raises TypeError: If either input is not an Index or pd.Index.

   .. seealso:: :py:obj:`assert_index_equal`

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda import testing as tm
   >>> import pandas as pd
   >>> a = ak.Index([1, 2, 3])
   >>> b = pd.Index([1, 2, 3])
   >>> tm.assert_index_equivalent(a, b)


.. py:function:: assert_is_sorted(seq) -> None

   Assert that the sequence is sorted.


.. py:function:: assert_series_equal(left, right, check_dtype: bool = True, check_index_type: bool = True, check_series_type: bool = True, check_names: bool = True, check_exact: bool = False, check_categorical: bool = True, check_category_order: bool = True, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'Series', *, check_index: bool = True, check_like: bool = False) -> None

   Check that left and right Series are equal.

   :param left: First Series to compare.
   :type left: Series
   :param right: Second Series to compare.
   :type right: Series
   :param check_dtype: Whether to check the Series dtype is identical. Defaults to True.
   :type check_dtype: bool
   :param check_index_type: Whether to check the Index class, dtype, and inferred_type are identical. Defaults to True.
   :type check_index_type: bool
   :param check_series_type: Whether to check that the Series class is identical. Defaults to True.
   :type check_series_type: bool
   :param check_names: Whether to check the Series and Index `name` attribute. Defaults to True.
   :type check_names: bool
   :param check_exact: Whether to compare numbers exactly. Defaults to False.
   :type check_exact: bool
   :param check_categorical: Whether to compare internal Categoricals exactly. Defaults to True.
   :type check_categorical: bool
   :param check_category_order: Whether to compare the category order of internal Categoricals. Defaults to True.
   :type check_category_order: bool
   :param rtol: Relative tolerance. Only used when `check_exact` is False. Defaults to 1e-5.
   :type rtol: float
   :param atol: Absolute tolerance. Only used when `check_exact` is False. Defaults to 1e-8.
   :type atol: float
   :param obj: Name of the object being compared, used in assertion messages. Defaults to 'Series'.
   :type obj: str
   :param check_index: Whether to check index equivalence. If False, only the values are compared. Defaults to True.
   :type check_index: bool
   :param check_like: If True, ignore the order of the index.
                      Must be False if `check_index` is False.
                      Note: same labels must be with the same data. Defaults to False.
   :type check_like: bool

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda import testing as tm
   >>> a = ak.Series([1, 2, 3, 4])
   >>> b = ak.Series([1, 2, 3, 4])
   >>> tm.assert_series_equal(a, b)


.. py:function:: assert_series_equivalent(left: arkouda.Series | pandas.Series, right: arkouda.Series | pandas.Series, check_dtype: bool = True, check_index_type: bool = True, check_series_type: bool = True, check_names: bool = True, check_exact: bool = False, check_categorical: bool = True, check_category_order: bool = True, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'Series', *, check_index: bool = True, check_like: bool = False) -> None

   Check that two Series are equal.

   This function compares two Series and raises an assertion if they differ.
   pandas Series are converted to Arkouda equivalents before comparison.
   The comparison can be customized using the provided keyword arguments.

   :param left: First Series to compare.
   :type left: Series or pd.Series
   :param right: Second Series to compare.
   :type right: Series or pd.Series
   :param check_dtype: Whether to check that dtypes are identical. Default is True.
   :type check_dtype: bool
   :param check_index_type: Whether to check that index class, dtype, and inferred type are identical. Default is True.
   :type check_index_type: bool
   :param check_series_type: Whether to check that the Series class is identical. Default is True.
   :type check_series_type: bool
   :param check_names: Whether to check that the Series and Index name attributes are identical. Default is True.
   :type check_names: bool
   :param check_exact: Whether to compare numbers exactly. Default is False.
   :type check_exact: bool
   :param check_categorical: Whether to compare internal Categoricals exactly. Default is True.
   :type check_categorical: bool
   :param check_category_order: Whether to compare category order in internal Categoricals. Default is True.
   :type check_category_order: bool
   :param rtol: Relative tolerance used when check_exact is False. Default is 1e-5.
   :type rtol: float
   :param atol: Absolute tolerance used when check_exact is False. Default is 1e-8.
   :type atol: float
   :param obj: Object name used in error messages. Default is "Series".
   :type obj: str
   :param check_index: Whether to check index equivalence. If False, only values are compared. Default is True.
   :type check_index: bool
   :param check_like: If True, ignore the order of the index. Must be False if check_index is False.
                      Note: identical labels must still correspond to the same data. Default is False.
   :type check_like: bool

   :raises TypeError: If either input is not a Series or pd.Series.

   .. seealso:: :py:obj:`assert_series_equal`

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda import testing as tm
   >>> import pandas as pd
   >>> a = ak.Series([1, 2, 3, 4])
   >>> b = pd.Series([1, 2, 3, 4])
   >>> tm.assert_series_equivalent(a, b)


.. py:function:: chisquare(f_obs, f_exp=None, ddof=0)

   Computes the chi square statistic and p-value.

   :param f_obs: The observed frequency.
   :type f_obs: pdarray
   :param f_exp: The expected frequency.
   :type f_exp: pdarray, default = None
   :param ddof: The delta degrees of freedom.
   :type ddof: int

   :rtype: arkouda.akstats.Power_divergenceResult

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.scipy import chisquare
   >>> chisquare(ak.array([10, 20, 30, 10]), ak.array([10, 30, 20, 10]))
   Power_divergenceResult(statistic=np.float64(8.333333333333334), pvalue=np.float64(0.03960235520...))

   .. seealso:: :py:obj:`scipy.stats.chisquare`, :py:obj:`arkouda.akstats.power_divergence`

   .. rubric:: References

   [1] â€œChi-squared testâ€, https://en.wikipedia.org/wiki/Chi-squared_test

   [2] "scipy.stats.chisquare",
   https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html


.. py:function:: compute_join_size(a: arkouda.numpy.pdarrayclass.pdarray, b: arkouda.numpy.pdarrayclass.pdarray) -> Tuple[int, int]

   Compute the internal size of a hypothetical join between a and b. Returns
   both the number of elements and number of bytes required for the join.


.. py:function:: date_operators(cls)

   Add common datetime operation methods to a DatetimeAccessor class.

   This class decorator dynamically attaches datetime operations (`floor`,
   `ceil`, `round`) to the given class using the `_make_op` helper.

   :param cls: The accessor class to decorate.
   :type cls: type

   :returns: The accessor class with datetime methods added.
   :rtype: type

   .. rubric:: Notes

   Used internally to implement the `.dt` accessor API.


.. py:function:: disableVerbose(logLevel: LogLevel = LogLevel.INFO)

   Deprecated alias for :func:`disable_verbose`.

   This function exists for backward compatibility only. Use
   :func:`disable_verbose` instead.

   :param logLevel: The log level to apply to all ArkoudaLoggers, disabling
                    verbose (DEBUG-level) output.
   :type logLevel: LogLevel, default LogLevel.INFO

   :rtype: None

   :Warns: **DeprecationWarning** -- Always raised. ``disableVerbose`` is deprecated and will be removed
           in a future release.

   .. seealso::

      :py:obj:`disable_verbose`
          Disable verbose logging for all loggers.

      :py:obj:`enable_verbose`
          Enable verbose logging for all loggers.


.. py:function:: disable_verbose(logLevel: LogLevel = LogLevel.INFO) -> None

   Disables verbose logging.

   Disables verbose logging (DEBUG log level) for all ArkoudaLoggers, setting
   the log level for each to the logLevel parameter.

   :param logLevel: The new log level, defaultts to LogLevel.INFO
   :type logLevel: LogLevel

   :raises TypeError: Raised if logLevel is not a LogLevel enum


.. py:function:: enableVerbose()

   Deprecated alias for :func:`enable_verbose`.

   This function exists for backward compatibility only. Use
   :func:`enable_verbose` instead.

   :rtype: None

   :Warns: **DeprecationWarning** -- Always raised. ``enableVerbose`` is deprecated and will be removed
           in a future release.

   .. seealso::

      :py:obj:`enable_verbose`
          Enable verbose (DEBUG-level) logging for all loggers.


.. py:function:: enable_verbose() -> None

   Enable verbose logging (DEBUG log level) for all ArkoudaLoggers.


.. py:function:: export(read_path: str, dataset_name: str = 'ak_data', write_file: Optional[str] = None, return_obj: bool = True, index: bool = False)

   Export data from arkouda to pandas.

   Export data from Arkouda file (Parquet/HDF5)
   to Pandas object or file formatted to be readable by Pandas.

   :param read_path: path to file where arkouda data is stored.
   :type read_path: str
   :param dataset_name: name to store dataset under
   :type dataset_name: str
   :param write_file: path to file to write pandas formatted data to. Only write the file if this is set.
                      Default is None.
   :type write_file: str
   :param return_obj: When True (default) return the Pandas DataFrame object, otherwise return None.
   :type return_obj: bool
   :param index: Default False. When True, maintain the indexes loaded from the pandas file
   :type index: bool

   :raises RuntimeError: - Unsupported file type

   :returns: When `return_obj=True`
   :rtype: pd.DataFrame

   .. seealso:: :py:obj:`pandas.DataFrame.to_parquet`, :py:obj:`pandas.DataFrame.to_hdf`, :py:obj:`pandas.DataFrame.read_parquet`, :py:obj:`pandas.DataFrame.read_hdf`, :py:obj:`ak.import_data`

   .. rubric:: Notes

   - If Arkouda file is exported for pandas, the format will not change. This mean parquet files
     will remain parquet and hdf5 will remain hdf5.
   - Export can only be performed from hdf5 or parquet files written by Arkouda. The result will be
     the same file type, but formatted to be read by Pandas.


.. py:function:: from_series(series: pandas.Series, dtype: Optional[Union[type, str]] = None) -> Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings]

   Convert a pandas ``Series`` to an Arkouda ``pdarray`` or ``Strings``.

   If ``dtype`` is not provided, the dtype is inferred from the pandas
   ``Series`` (using pandas' dtype metadata). If ``dtype`` is provided, it
   is used as an override and normalized via Arkouda's dtype resolution rules.

   In addition to the core numeric/bool types, this function supports
   datetime and timedelta Series of **any** resolution (``ns``, ``us``, ``ms``,
   etc.) by converting them to an ``int64`` pdarray of nanoseconds.

   :param series: The pandas Series to convert.
   :type series: pd.Series
   :param dtype: Optional dtype override. This may be a Python type (e.g. ``bool``),
                 a NumPy scalar type (e.g. ``np.int64``), or a dtype string.

                 String-like spellings are normalized to Arkouda string dtype, including:
                 ``"object"``, ``"str"``, ``"string"``, ``"string[python]"``,
                 and ``"string[pyarrow]"``.
   :type dtype: Optional[Union[type, str]], optional

   :returns: An Arkouda ``pdarray`` for numeric/bool/datetime/timedelta inputs, or an
             Arkouda ``Strings`` for string inputs.
   :rtype: Union[pdarray, Strings]

   :raises ValueError: Raised if the dtype cannot be interpreted or is unsupported for conversion.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> import numpy as np
   >>> import pandas as pd

   # ints
   >>> np.random.seed(1701)
   >>> ak.from_series(pd.Series(np.random.randint(0, 10, 5)))
   array([4 3 3 5 0])

   >>> ak.from_series(pd.Series(['1', '2', '3', '4', '5']), dtype=np.int64)
   array([1 2 3 4 5])

   # floats
   >>> np.random.seed(1701)
   >>> ak.from_series(pd.Series(np.random.uniform(low=0.0, high=1.0, size=3)))
   array([0.089433234324597599 0.1153776854774361 0.51874393620990389])

   # bools
   >>> np.random.seed(1864)
   >>> ak.from_series(pd.Series(np.random.choice([True, False], size=5)))
   array([True True True False False])

   # strings: pandas dtype spellings normalized to Arkouda Strings
   >>> ak.from_series(pd.Series(['a', 'b', 'c', 'd', 'e'], dtype="string"))
   array(['a', 'b', 'c', 'd', 'e'])

   >>> ak.from_series(pd.Series(['a', 'b', 'c'], dtype="string[pyarrow]"))
   array(['a', 'b', 'c'])

   # datetime: any resolution is accepted, returned as int64 nanoseconds
   >>> ak.from_series(pd.Series(pd.to_datetime(['1/1/2018', np.datetime64('2018-01-01')])))
   array([1514764800000000000 1514764800000000000])

   .. rubric:: Notes

   - Datetime and timedelta Series are converted to ``int64`` nanoseconds.
   - String-like pandas dtypes (including ``object``) are treated as string and
     converted to Arkouda ``Strings``.


.. py:function:: gen_ranges(starts, ends, stride=1, return_lengths=False)

   Generate a segmented array of variable-length, contiguous ranges between pairs of
   start- and end-points.

   :param starts: The start value of each range
   :type starts: pdarray, int64
   :param ends: The end value (exclusive) of each range
   :type ends: pdarray, int64
   :param stride: Difference between successive elements of each range
   :type stride: int
   :param return_lengths: Whether or not to return the lengths of each segment. Default False.
   :type return_lengths: bool, optional

   :returns:

             segments : pdarray, int64
                 The starting index of each range in the resulting array
             ranges : pdarray, int64
                 The actual ranges, flattened into a single array
             lengths : pdarray, int64
                 The lengths of each segment. Only returned if return_lengths=True.
   :rtype: pdarray|int64, pdarray|int64, pdarray|int64


.. py:function:: get_columns(filenames: Union[str, List[str]], col_delim: str = ',', allow_errors: bool = False) -> List[str]

   Get a list of column names from CSV file(s).


.. py:function:: get_datasets(filenames: Union[str, List[str]], allow_errors: bool = False, column_delim: str = ',', read_nested: bool = True) -> List[str]

   Get the names of the datasets in the provide files.

   :param filenames: Name of the file/s from which to return datasets
   :type filenames: str or List[str]
   :param allow_errors: Default: False
                        Whether or not to allow errors while accessing datasets
   :type allow_errors: bool
   :param column_delim: Column delimiter to be used if dataset is CSV. Otherwise, unused.
   :type column_delim: str
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Only used for Parquet Files.
   :type read_nested: bool

   :rtype: List[str] of names of the datasets

   :raises RuntimeError: - If no datasets are returned

   .. rubric:: Notes

   - This function currently supports HDF5 and Parquet formats.
   - Future updates to Parquet will deprecate this functionality on that format,
   but similar support will be added for Parquet at that time.
   - If a list of files is provided, only the datasets in the first file will be returned

   .. seealso:: :py:obj:`ls`


.. py:function:: get_filetype(filenames: Union[str, List[str]]) -> str

   Get the type of a file accessible to the server.

   Supported file types and possible return strings are 'HDF5' and 'Parquet'.

   :param filenames: A file or list of files visible to the arkouda server
   :type filenames: Union[str, List[str]]

   :returns: Type of the file returned as a string, either 'HDF5', 'Parquet' or 'CSV
   :rtype: str

   :raises ValueError: Raised if filename is empty or contains only whitespace

   .. rubric:: Notes

   - When list provided, it is assumed that all files are the same type
   - CSV Files without the Arkouda Header are not supported

   .. seealso:: :py:obj:`read_parquet`, :py:obj:`read_hdf`


.. py:function:: get_null_indices(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None) -> Union[arkouda.numpy.pdarrayclass.pdarray, Mapping[str, arkouda.numpy.pdarrayclass.pdarray]]

   Get null indices of a string column in a Parquet file.

   :param filenames: Either a list of filenames or shell expression
   :type filenames: list or str
   :param datasets: (List of) name(s) of dataset(s) to read. Each dataset must be a string
                    column. There is no default value for this function, the datasets to be
                    read must be specified.
   :type datasets: list or str or None

   :returns: Dictionary of {datasetName: pdarray}
   :rtype: returns a dictionary of Arkouda pdarrays

   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. seealso:: :py:obj:`get_datasets`, :py:obj:`ls`


.. py:function:: import_data(read_path: str, write_file: Optional[str] = None, return_obj: bool = True, index: bool = False)

   Import data from a file saved by Pandas (HDF5/Parquet).

   Import data from a file saved by Pandas (HDF5/Parquet) to Arkouda object and/or
   a file formatted to be read by Arkouda.

   :param read_path: path to file where pandas data is stored. This can be glob expression for parquet formats.
   :type read_path: str
   :param write_file: path to file to write arkouda formatted data to. Only write file if provided
   :type write_file: str, optional
   :param return_obj: If True (default), return the Arkouda DataFrame object. If False, return None.
   :type return_obj: bool
   :param index: If True, maintain the indexes loaded from the pandas file. Default is False.
   :type index: bool

   :raises RuntimeWarning: - Export attempted on Parquet file. Arkouda formatted Parquet files are readable by pandas.
   :raises RuntimeError: - Unsupported file type

   :returns: When `return_obj=True`
   :rtype: pd.DataFrame

   .. seealso:: :py:obj:`pandas.DataFrame.to_parquet`, :py:obj:`pandas.DataFrame.to_hdf`, :py:obj:`pandas.DataFrame.read_parquet`, :py:obj:`pandas.DataFrame.read_hdf`, :py:obj:`ak.export`

   .. rubric:: Notes

   - Import can only be performed from hdf5 or parquet files written by pandas.


.. py:function:: information(names: Union[List[str], str] = RegisteredSymbols) -> str

   Return a JSON formatted string containing information about the objects in names.

   :param names: names is either the name of an object or list of names of objects to retrieve info
                 if names is ak.AllSymbols, retrieves info for all symbols in the symbol table
                 if names is ak.RegisteredSymbols, retrieves info for all symbols in the registry
   :type names: Union[List[str], str]

   :returns: JSON formatted string containing a list of information for each object in names
   :rtype: str

   :raises RuntimeError: Raised if a server-side error is thrown in the process of
       retrieving information about the objects in names


.. py:function:: intersect(a, b, positions=True, unique=False)

   Find the intersection of two arkouda arrays.

   This function can be especially useful when `positions=True` so
   that the caller gets the indices of values present in both arrays.

   :param a: An array of strings.
   :type a: Strings or pdarray
   :param b: An array of strings.
   :type b: Strings or pdarray
   :param positions: Return tuple of boolean pdarrays that indicate positions in `a` and `b`
                     of the intersection values.
   :type positions: bool, default=True
   :param unique: If the number of distinct values in `a` (and `b`) is equal to the size of
                  `a` (and `b`), there is a more efficient method to compute the intersection.
   :type unique: bool, default=False

   :returns: * *(arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.pdarrayclass.pdarray) or*
             * *arkouda.numpy.pdarrayclass.pdarray* -- The indices of `a` and `b` where any element occurs at least once in both
               arrays.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> a = ak.arange(10)
   >>> print(a)
   [0 1 2 3 4 5 6 7 8 9]

   >>> b = 2 * ak.arange(10)
   >>> print(b)
   [0 2 4 6 8 10 12 14 16 18]

   >>> intersect(a,b, positions=True)
   (array([True False True False True False True False True False]),
   array([True True True True True False False False False False]))

   >>> intersect(a,b, positions=False)
   array([0 2 4 6 8])


.. py:function:: intx(a, b)

   Find all the rows that are in both dataframes.

   Columns should be in identical order.

   Note: does not work for columns of floating point values, but does work for
   Strings, pdarrays of int64 type, and Categorical *should* work.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> a = ak.DataFrame({'a':ak.arange(5),'b': 2* ak.arange(5)})
   >>> a
      a  b
   0  0  0
   1  1  2
   2  2  4
   3  3  6
   4  4  8 (5 rows x 2 columns)

   >>> b = ak.DataFrame({'a':ak.arange(5),'b':ak.array([0,3,4,7,8])})
   >>> b
      a  b
   0  0  0
   1  1  3
   2  2  4
   3  3  7
   4  4  8 (5 rows x 2 columns)

   >>> intx(a,b)
   array([True False True False True])
   >>> intersect_df = a[intx(a,b)]
   >>> intersect_df
      a  b
   0  0  0
   2  2  4
   4  4  8 (3 rows x 2 columns)


.. py:function:: invert_permutation(perm)

   Find the inverse of a permutation array.

   :param perm: The permutation array.
   :type perm: pdarray

   :returns: The inverse of the permutation array.
   :rtype: arkouda.numpy.pdarrayclass.pdarray

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.pandas.index import Index
   >>> i = Index(ak.array([1,2,0,5,4]))
   >>> perm = i.argsort()
   >>> print(perm)
   [2 0 1 4 3]
   >>> invert_permutation(perm)
   array([1 2 0 4 3])


.. py:function:: join_on_eq_with_dt(a1: arkouda.numpy.pdarrayclass.pdarray, a2: arkouda.numpy.pdarrayclass.pdarray, t1: arkouda.numpy.pdarrayclass.pdarray, t2: arkouda.numpy.pdarrayclass.pdarray, dt: Union[int, numpy.int64], pred: str, result_limit: Union[int, numpy.int64] = 1000) -> Tuple[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.pdarrayclass.pdarray]

   Inner-join on equality between two integer arrays where the time-window predicate is also true.

   :param a1: Values to join (must be int64 dtype).
   :type a1: pdarray
   :param a2: Values to join (must be int64 dtype).
   :type a2: pdarray
   :param t1: timestamps in millis corresponding to the a1 pdarray
   :type t1: pdarray
   :param t2: timestamps in millis corresponding to the a2 pdarray
   :type t2: pdarray
   :param dt: time delta
   :type dt: Union[int,np.int64]
   :param pred: time window predicate
   :type pred: str
   :param result_limit: size limit for returned result
   :type result_limit: Union[int,np.int64]

   :returns:

             result_array_one : pdarray, int64
                 a1 indices where a1 == a2
             result_array_one : pdarray, int64
                 a2 indices where a2 == a1
   :rtype: Tuple[pdarray, pdarray]

   :raises TypeError: Raised if a1, a2, t1, or t2 is not a pdarray, or if dt or
       result_limit is not an int
   :raises ValueError: if a1, a2, t1, or t2 dtype is not int64, pred is not
       'true_dt', 'abs_dt', or 'pos_dt', or result_limit is < 0


.. py:function:: list_registry(detailed: bool = False)

   Return a list containing the names of all registered objects.

   :param detailed: Default = False
                    Return details of registry objects. Currently includes object type for any objects
   :type detailed: bool

   :returns: Dict containing keys "Components" and "Objects".
   :rtype: dict

   :raises RuntimeError: Raised if there's a server-side error thrown


.. py:function:: list_symbol_table() -> List[str]

   Return a list containing the names of all objects in the symbol table.

   :returns: List of all object names in the symbol table
   :rtype: list

   :raises RuntimeError: Raised if there's a server-side error thrown


.. py:function:: load(path_prefix: str, file_format: str = 'INFER', dataset: str = 'array', calc_string_offsets: bool = False, column_delim: str = ',') -> Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray, arkouda.pandas.categorical.Categorical, arkouda.pandas.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.numpy.timeclass.Datetime, arkouda.numpy.timeclass.Timedelta, arkouda.pandas.index.Index]]]

   Load a pdarray previously saved with ``pdarray.save()``.

   :param path_prefix: Filename prefix used to save the original pdarray
   :type path_prefix: str
   :param file_format: 'INFER', 'HDF5' or 'Parquet'. Defaults to 'INFER'. Used to indicate the file type being loaded.
                       If INFER, this will be detected during processing
   :type file_format: str
   :param dataset: Dataset name where the pdarray was saved, defaults to 'array'
   :type dataset: str
   :param calc_string_offsets: If True the server will ignore Segmented Strings 'offsets' array and derive
                               it from the null-byte terminators.  Defaults to False currently
   :type calc_string_offsets: bool
   :param column_delim: Column delimiter to be used if dataset is CSV. Otherwise, unused.
   :type column_delim: str

   :returns: Dictionary of {datsetName: Union[pdarray, Strings, SegArray, Categorical]}
             with the previously saved pdarrays, Strings, SegArrays, or Categoricals
   :rtype: Mapping[str, Union[pdarray, Strings, SegArray, Categorical]]

   :raises TypeError: Raised if either path_prefix or dataset is not a str
   :raises ValueError: Raised if invalid file_format or if the dataset is not present in all hdf5 files or if the
       path_prefix does not correspond to files accessible to Arkouda
   :raises RuntimeError: Raised if the hdf5 files are present but there is an error in opening
       one or more of them

   .. seealso:: :py:obj:`to_parquet`, :py:obj:`to_hdf`, :py:obj:`load_all`, :py:obj:`read`

   .. rubric:: Notes

   If you have a previously saved Parquet file that is raising a FileNotFound error, try loading it
   with a .parquet appended to the prefix_path.
   Parquet files were previously ALWAYS stored with a ``.parquet`` extension.

   ak.load does not support loading a single file.
   For loading single HDF5 files without the _LOCALE#### suffix please use ak.read().

   CSV files without the Arkouda Header are not supported.

   .. rubric:: Examples

   >>> import arkouda as ak

   Loading from file without extension
   >>> obj = ak.load('path/prefix') # doctest: +SKIP

   Loads the array from numLocales files with the name ``cwd/path/name_prefix_LOCALE####``.
   The file type is inferred during processing.

   Loading with an extension (HDF5)
   >>> obj = ak.load('path/prefix.test') # doctest: +SKIP

   Loads the object from numLocales files with the name ``cwd/path/name_prefix_LOCALE####.test`` where
   #### is replaced by each locale numbers. Because filetype is inferred during processing,
   the extension is not required to be a specific format.


.. py:function:: load_all(path_prefix: str, file_format: str = 'INFER', column_delim: str = ',', read_nested: bool = True) -> Mapping[str, arkouda.numpy.pdarrayclass.pdarray | arkouda.numpy.strings.Strings | arkouda.numpy.segarray.SegArray | arkouda.pandas.categorical.Categorical | arkouda.pandas.dataframe.DataFrame | arkouda.client_dtypes.IPv4 | arkouda.numpy.timeclass.Datetime | arkouda.numpy.timeclass.Timedelta | arkouda.pandas.index.Index]

   Load multiple pdarrays, Strings, SegArrays, or Categoricals previously saved with ``save_all()``.

   :param path_prefix: Filename prefix used to save the original pdarray
   :type path_prefix: str
   :param file_format: 'INFER', 'HDF5', 'Parquet', or 'CSV'. Defaults to 'INFER'. Indicates the format being loaded.
                       When 'INFER' the processing will detect the format
                       Defaults to 'INFER'
   :type file_format: str
   :param column_delim: Column delimiter to be used if dataset is CSV. Otherwise, unused.
   :type column_delim: str
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Parquet files only
   :type read_nested: bool

   :returns: Dictionary of {datsetName: Union[pdarray, Strings, SegArray, Categorical]}
             with the previously saved pdarrays, Strings, SegArrays, or Categoricals
   :rtype: Mapping[str, Union[pdarray, Strings, SegArray, Categorical]]

   :raises TypeError: Raised if path_prefix is not a str
   :raises ValueError: Raised if file_format/extension is encountered that is not hdf5 or parquet or
       if all datasets are not present in all hdf5/parquet files or if the
       path_prefix does not correspond to files accessible to Arkouda
   :raises RuntimeError: Raised if the hdf5 files are present but there is an error in opening
       one or more of them

   .. seealso:: :py:obj:`to_parquet`, :py:obj:`to_hdf`, :py:obj:`load`, :py:obj:`read`

   .. rubric:: Notes

   This function has been updated to determine the file extension based on the file format variable

   This function will be deprecated when glob flags are added to read_* methods

   CSV files without the Arkouda Header are not supported.


.. py:function:: load_checkpoint(name, path='.akdata')

   Load server's state.

   The server metadata must match the current
   configuration (e.g. same number of locales must be used).

   :param name: Name of the checkpoint. ``<path>/<name>`` must be a directory.
   :type name: str
   :param path: The directory to save the checkpoint.
   :type path: str

   :returns: The checkpoint name, which will be the same as the ``name`` argument.
   :rtype: str

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> arr = ak.zeros(10, int)
   >>> arr[2] = 2
   >>> arr[2]
   np.int64(2)
   >>> cp_name = ak.save_checkpoint()
   >>> arr[2] = 3
   >>> arr[2]
   np.int64(3)
   >>> ak.load_checkpoint(cp_name) # doctest: +SKIP
   >>> arr[2]
   np.int64(3)

   .. seealso:: :py:obj:`save_checkpoint`


.. py:function:: ls(filename: str, col_delim: str = ',', read_nested: bool = True) -> List[str]

   List the contents of an HDF5 or Parquet file on the Arkouda server.

   This function invokes the HDF5 `h5ls` utility on a file visible to the
   Arkouda server, or simulates a similar listing for Parquet files. For CSV
   files without headers, see `ls_csv`.

   :param filename: Path to the file on the Arkouda server. Must be a non-empty string.
   :type filename: str
   :param col_delim: Delimiter to use when interpreting CSV files. Default is ",".
   :type col_delim: str
   :param read_nested: If True, include nested Parquet columns (e.g., `SegArray`). If False,
                       nested columns are ignored. Only applies to Parquet files.
                       Default is True.
   :type read_nested: bool

   :returns: A list of lines describing each dataset or column in the file.
   :rtype: List[str]

   :raises TypeError: If `filename` is not a string.
   :raises ValueError: If `filename` is empty or contains only whitespace.
   :raises RuntimeError: If an error occurs when running `h5ls` or simulating the Parquet listing.

   .. rubric:: Notes

   - Parquet support is limited and may change in future releases.
   - Output lines mirror the format of the HDF5 `h5ls` output.
   - For CSV files lacking headers, use `ls_csv`.

   .. seealso::

      :py:obj:`ls_csv`
          List the contents of CSV files without headers.


.. py:function:: ls_csv(filename: str, col_delim: str = ',') -> List[str]

   List the datasets within a file when a CSV does not have a header.

   :param filename: The name of the file to pass to the server
   :type filename: str
   :param col_delim: The delimiter used to separate columns if the file is a csv
   :type col_delim: str

   :returns: The string output of the datasets from the server
   :rtype: str

   .. seealso:: :py:obj:`ls`


.. py:function:: merge(left: DataFrame, right: DataFrame, on: Optional[Union[str, List[str]]] = None, left_on: Optional[Union[str, List[str]]] = None, right_on: Optional[Union[str, List[str]]] = None, how: str = 'inner', left_suffix: str = '_x', right_suffix: str = '_y', convert_ints: bool = True, sort: bool = True) -> DataFrame

   Merge Arkouda DataFrames with a database-style join.

   The resulting dataframe contains rows from both DataFrames as specified by
   the merge condition (based on the "how" and "on" parameters).

   Based on pandas merge functionality.
   https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html

   :param left: The Left DataFrame to be joined.
   :type left: DataFrame
   :param right: The Right DataFrame to be joined.
   :type right: DataFrame
   :param on: The name or list of names of the DataFrame column(s) to join on.
              If on is None, this defaults to the intersection of the columns in both DataFrames.
   :type on: Optional[Union[str, List[str]]] = None
   :param left_on: Column name or names to join on in the left DataFrame. If this is not None, then right_on
                   must also not be None, and this will override `on`.
   :type left_on: str or List of str, optional
   :param right_on: Column name or names to join on in the right DataFrame. If this is not None, then left_on
                    must also not be None, and this will override `on`.
   :type right_on: str or List of str, optional
   :param how: The merge condition.
               Must be one of "inner", "left", "right", or "outer".
   :type how: str, default = "inner"
   :param left_suffix: A string indicating the suffix to add to columns from the left dataframe for overlapping
                       column names in both left and right. Defaults to "_x". Only used when how is "inner".
   :type left_suffix: str, default = "_x"
   :param right_suffix: A string indicating the suffix to add to columns from the right dataframe for overlapping
                        column names in both left and right. Defaults to "_y". Only used when how is "inner".
   :type right_suffix: str, default = "_y"
   :param convert_ints: If True, convert columns with missing int values (due to the join) to float64.
                        This is to match pandas.
                        If False, do not convert the column dtypes.
                        This has no effect when how = "inner".
   :type convert_ints: bool = True
   :param sort: If True, DataFrame is returned sorted by "on".
                Otherwise, the DataFrame is not sorted.
   :type sort: bool = True

   :returns: Joined Arkouda DataFrame.
   :rtype: DataFrame

   .. note:: Multiple column joins are only supported for integer columns.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda import merge
   >>> left_df = ak.DataFrame({'col1': ak.arange(5), 'col2': -1 * ak.arange(5)})
   >>> left_df
      col1  col2
   0     0     0
   1     1    -1
   2     2    -2
   3     3    -3
   4     4    -4 (5 rows x 2 columns)

   >>> right_df = ak.DataFrame({'col1': 2 * ak.arange(5), 'col2': 2 * ak.arange(5)})
   >>> right_df
      col1  col2
   0     0     0
   1     2     2
   2     4     4
   3     6     6
   4     8     8 (5 rows x 2 columns)

   >>> merge(left_df, right_df, on = "col1")
      col1  col2_x  col2_y
   0     0       0       0
   1     2      -2       2
   2     4      -4       4 (3 rows x 3 columns)

   >>> merge(left_df, right_df, on = "col1", how = "left")
      col1  col2_x  col2_y
   0     0       0     0.0
   1     1      -1     NaN
   2     2      -2     2.0
   3     3      -3     NaN
   4     4      -4     4.0 (5 rows x 3 columns)

   >>> merge(left_df, right_df, on = "col1", how = "right")
      col1  col2_x  col2_y
   0     0     0.0       0
   1     2    -2.0       2
   2     4    -4.0       4
   3     6     NaN       6
   4     8     NaN       8 (5 rows x 3 columns)

   >>> merge(left_df, right_df, on = "col1", how = "outer")
      col1  col2_x  col2_y
   0     0     0.0     0.0
   1     1    -1.0     NaN
   2     2    -2.0     2.0
   3     3    -3.0     NaN
   4     4    -4.0     4.0
   5     6     NaN     6.0
   6     8     NaN     8.0 (7 rows x 3 columns)


.. py:function:: power_divergence(f_obs, f_exp=None, ddof=0, lambda_=None)

   Computes the power divergence statistic and p-value.

   :param f_obs: The observed frequency.
   :type f_obs: pdarray
   :param f_exp: The expected frequency.
   :type f_exp: pdarray, default = None
   :param ddof: The delta degrees of freedom.
   :type ddof: int
   :param lambda_: The power in the Cressie-Read power divergence statistic.
                   Allowed values: "pearson", "log-likelihood", "freeman-tukey", "mod-log-likelihood",
                   "neyman", "cressie-read"

                   Powers correspond as follows:

                   "pearson": 1

                   "log-likelihood": 0

                   "freeman-tukey": -0.5

                   "mod-log-likelihood": -1

                   "neyman": -2

                   "cressie-read": 2 / 3
   :type lambda_: string, default = "pearson"

   :rtype: arkouda.akstats.Power_divergenceResult

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.scipy import power_divergence
   >>> x = ak.array([10, 20, 30, 10])
   >>> y = ak.array([10, 30, 20, 10])
   >>> power_divergence(x, y, lambda_="pearson")
   Power_divergenceResult(statistic=np.float64(8.333333333333334), pvalue=np.float64(0.03960235520...))
   >>> power_divergence(x, y, lambda_="log-likelihood")
   Power_divergenceResult(statistic=np.float64(8.109302162163285), pvalue=np.float64(0.04380595350...))

   .. seealso:: :py:obj:`scipy.stats.power_divergence`, :py:obj:`arkouda.akstats.chisquare`

   .. rubric:: Notes

   This is a modified version of scipy.stats.power_divergence [2]
   in order to scale using arkouda pdarrays.

   .. rubric:: References

   [1] "scipy.stats.power_divergence",
   https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.power_divergence.html

   [2] Scipy contributors (2024) scipy (Version v1.12.0) [Source code].
   https://github.com/scipy/scipy


.. py:function:: pretty_print_information(names: Union[List[str], str] = RegisteredSymbols) -> None

   Print verbose information for each object in names in a human readable format.

   :param names: names is either the name of an object or list of names of objects to retrieve info
                 if names is ak.AllSymbols, retrieves info for all symbols in the symbol table
                 if names is ak.RegisteredSymbols, retrieves info for all symbols in the registry
   :type names: Union[List[str], str]

   :raises RuntimeError: Raised if a server-side error is thrown in the process of
       retrieving information about the objects in names


.. py:function:: read(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, iterative: bool = False, strictTypes: bool = True, allow_errors: bool = False, calc_string_offsets=False, column_delim: str = ',', read_nested: bool = True, has_non_float_nulls: bool = False, fixed_len: int = -1) -> Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray, arkouda.pandas.categorical.Categorical, arkouda.pandas.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.numpy.timeclass.Datetime, arkouda.numpy.timeclass.Timedelta, arkouda.pandas.index.Index]]]

   Read datasets from files.

   File Type is determined automatically.

   :param filenames: Either a list of filenames or shell expression
   :type filenames: list or str
   :param datasets: (List of) name(s) of dataset(s) to read (default: all available)
   :type datasets: list or str or None
   :param iterative: Iterative (True) or Single (False) function call(s) to server
   :type iterative: bool
   :param strictTypes: If True (default), require all dtypes of a given dataset to have the
                       same precision and sign. If False, allow dtypes of different
                       precision and sign across different files. For example, if one
                       file contains a uint32 dataset and another contains an int64
                       dataset with the same name, the contents of both will be read
                       into an int64 pdarray.
   :type strictTypes: bool
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool
   :param calc_string_offsets: Default False, if True this will tell the server to calculate the
                               offsets/segments array on the server versus loading them from HDF5 files.
                               In the future this option may be set to True as the default.
   :type calc_string_offsets: bool
   :param column_delim: Column delimiter to be used if dataset is CSV. Otherwise, unused.
   :type column_delim: str
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Ignored if datasets is not None
                       Parquet Files only.
   :type read_nested: bool
   :param has_non_float_nulls: Default False. This flag must be set to True to read non-float parquet columns
                               that contain null values.
   :type has_non_float_nulls: bool
   :param fixed_len: Default -1. This value can be set for reading Parquet string columns when the
                     length of each string is known at runtime. This can allow for skipping byte
                     calculation, which can have an impact on performance.
   :type fixed_len: int

   :returns: Dictionary of {datasetName: pdarray, String, or SegArray}
   :rtype: Returns a dictionary of Arkouda pdarrays, Arkouda Strings, or Arkouda Segarrays.

   :raises RuntimeError: If invalid filetype is detected

   .. seealso:: :py:obj:`get_datasets`, :py:obj:`ls`, :py:obj:`read_parquet`, :py:obj:`read_hdf`

   .. rubric:: Notes

   If filenames is a string, it is interpreted as a shell expression
   (a single filename is a valid expression, so it will work) and is
   expanded with glob to read all matching files.

   If iterative == True each dataset name and file names are passed to
   the server as independent sequential strings while if iterative == False
   all dataset names and file names are passed to the server in a single
   string.

   If datasets is None, infer the names of datasets from the first file
   and read all of them. Use ``get_datasets`` to show the names of datasets
   to HDF5/Parquet files.

   CSV files without the Arkouda Header are not supported.

   .. rubric:: Examples

   >>> import arkouda as ak

   Read with file Extension
   load HDF5 - processing determines file type not extension
   >>> x = ak.read('path/name_prefix.h5')  # doctest: +SKIP

   Read without file Extension
   load Parquet
   >>> x = ak.read('path/name_prefix.parquet') # doctest: +SKIP

   Read Glob Expression
   Reads HDF5
   >>> x = ak.read('path/name_prefix*') # doctest: +SKIP


.. py:function:: read_csv(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, column_delim: str = ',', allow_errors: bool = False) -> Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray, arkouda.pandas.categorical.Categorical, arkouda.pandas.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.numpy.timeclass.Datetime, arkouda.numpy.timeclass.Timedelta, arkouda.pandas.index.Index]]]

   Read CSV file(s) into Arkouda objects.

   If more than one dataset is found, the objects
   will be returned in a dictionary mapping the dataset name to the Arkouda object
   containing the data. If the file contains the appropriately formatted header, typed
   data will be returned. Otherwise, all data will be returned as a Strings object.

   :param filenames: The filenames to read data from
   :type filenames: str or List[str]
   :param datasets: names of the datasets to read. When `None`, all datasets will be read.
   :type datasets: str or List[str] (Optional)
   :param column_delim: The delimiter for column names and data. Defaults to ",".
   :type column_delim: str
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool

   :returns: Dictionary of {datasetName: pdarray, String, or SegArray}
   :rtype: Returns a dictionary of Arkouda pdarrays, Arkouda Strings, or Arkouda Segarrays.

   :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
       more of the specified files do not exist
   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
       If `allow_errors` is true this may be raised if no values are returned
       from the server.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. seealso:: :py:obj:`to_csv`

   .. rubric:: Notes

   - CSV format is not currently supported by load/load_all operations
   - The column delimiter is expected to be the same for column names and data
   - Be sure that column delimiters are not found within your data.
   - All CSV files must delimit rows using newline (``\\n``) at this time.
   - Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
     bytes as uint(8).


.. py:function:: read_hdf(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, iterative: bool = False, strict_types: bool = True, allow_errors: bool = False, calc_string_offsets: bool = False, tag_data=False) -> Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray, arkouda.pandas.categorical.Categorical, arkouda.pandas.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.numpy.timeclass.Datetime, arkouda.numpy.timeclass.Timedelta, arkouda.pandas.index.Index]]]

   Read Arkouda objects from HDF5 file/s.

   :param filenames: Filename/s to read objects from
   :type filenames: str, List[str]
   :param datasets: datasets to read from the provided files
   :type datasets: Optional str, List[str]
   :param iterative: Iterative (True) or Single (False) function call(s) to server
   :type iterative: bool
   :param strict_types: If True (default), require all dtypes of a given dataset to have the
                        same precision and sign. If False, allow dtypes of different
                        precision and sign across different files. For example, if one
                        file contains a uint32 dataset and another contains an int64
                        dataset with the same name, the contents of both will be read
                        into an int64 pdarray.
   :type strict_types: bool
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool
   :param calc_string_offsets: Default False, if True this will tell the server to calculate the
                               offsets/segments array on the server versus loading them from HDF5 files.
                               In the future this option may be set to True as the default.
   :type calc_string_offsets: bool
   :param tag_data: Default False, if True tag the data with the code associated with the filename
                    that the data was pulled from.
   :type tag_data: bool

   :returns: Dictionary of {datasetName: pdarray, String, SegArray}
   :rtype: Returns a dictionary of Arkouda pdarrays, Arkouda Strings, or Arkouda Segarrays.

   :raises ValueError: Raised if all datasets are not present in all hdf5 files or if one or
       more of the specified files do not exist
   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
       If `allow_errors` is true this may be raised if no values are returned
       from the server.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. rubric:: Notes

   If filenames is a string, it is interpreted as a shell expression
   (a single filename is a valid expression, so it will work) and is
   expanded with glob to read all matching files.

   If iterative == True each dataset name and file names are passed to
   the server as independent sequential strings while if iterative == False
   all dataset names and file names are passed to the server in a single
   string.

   If datasets is None, infer the names of datasets from the first file
   and read all of them. Use ``get_datasets`` to show the names of datasets
   to HDF5 files.

   .. seealso:: :py:obj:`read_tagged_data`

   .. rubric:: Examples

   >>> import arkouda as ak

   Read with file Extension
   >>> x = ak.read_hdf('path/name_prefix.h5')  # doctest: +SKIP

   Read Glob Expression
   >>> x = ak.read_hdf('path/name_prefix*')  # doctest: +SKIP


.. py:function:: read_parquet(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, iterative: bool = False, strict_types: bool = True, allow_errors: bool = False, tag_data: bool = False, read_nested: bool = True, has_non_float_nulls: bool = False, null_handling: Optional[str] = None, fixed_len: int = -1) -> Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray, arkouda.pandas.categorical.Categorical, arkouda.pandas.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.numpy.timeclass.Datetime, arkouda.numpy.timeclass.Timedelta, arkouda.pandas.index.Index]]]

   Read Arkouda objects from Parquet file/s.

   :param filenames: Filename/s to read objects from
   :type filenames: str, List[str]
   :param datasets: datasets to read from the provided files
   :type datasets: Optional str, List[str]
   :param iterative: Iterative (True) or Single (False) function call(s) to server
   :type iterative: bool
   :param strict_types: If True (default), require all dtypes of a given dataset to have the
                        same precision and sign. If False, allow dtypes of different
                        precision and sign across different files. For example, if one
                        file contains a uint32 dataset and another contains an int64
                        dataset with the same name, the contents of both will be read
                        into an int64 pdarray.
   :type strict_types: bool
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool
   :param tag_data: Default False, if True tag the data with the code associated with the filename
                    that the data was pulled from.
   :type tag_data: bool
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       If datasets is not None, this will be ignored.
   :type read_nested: bool
   :param has_non_float_nulls: Deprecated. Please use null_handling.
                               Default False. This flag must be set to True to read non-float parquet columns
                               that contain null values.
   :type has_non_float_nulls: bool
   :param null_handling: Defaults to "only floats".
                         Supported values are "none", "only floats", "all".
                         If "none", the data is assumed to be free of nulls.  This results in the
                         fastest performance. However, if there is nulls in the data, the
                         behavior is undefined. If "only floats", only floating point typed
                         columns may contain nulls. This makes reading other data types faster.
                         If "all", any column can contain nulls. This is the most generally
                         applicable mode, though results in slower performance across the board.
   :type null_handling: Optional str
   :param fixed_len: Default -1. This value can be set for reading Parquet string columns when the
                     length of each string is known at runtime. This can allow for skipping byte
                     calculation, which can have an impact on performance.
   :type fixed_len: int

   :returns: Dictionary of {datasetName: pdarray, String, or SegArray}
   :rtype: Returns a dictionary of Arkouda pdarrays, Arkouda Strings, or Arkouda Segarrays.

   :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
       more of the specified files do not exist
   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
       If `allow_errors` is true this may be raised if no values are returned
       from the server.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. rubric:: Notes

   If filenames is a string, it is interpreted as a shell expression
   (a single filename is a valid expression, so it will work) and is
   expanded with glob to read all matching files.

   If iterative == True each dataset name and file names are passed to
   the server as independent sequential strings while if iterative == False
   all dataset names and file names are passed to the server in a single
   string.

   If datasets is None, infer the names of datasets from the first file
   and read all of them. Use ``get_datasets`` to show the names of datasets
   to Parquet files.

   Parquet always recomputes offsets at this time
   This will need to be updated once parquets workflow is updated

   .. seealso:: :py:obj:`read_tagged_data`

   .. rubric:: Examples

   >>> import arkouda as ak

   Read without file Extension
   load Parquet
   >>> x = ak.read_parquet('path/name_prefix.parquet')  # doctest: +SKIP

   Read Glob Expression
   Reads Parquet
   >>> x = ak.read_parquet('path/name_prefix*')  # doctest: +SKIP


.. py:function:: read_tagged_data(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, strictTypes: bool = True, allow_errors: bool = False, calc_string_offsets=False, read_nested: bool = True, has_non_float_nulls: bool = False)

   Read datasets from files and tag each record to the file it was read from.

   File Type is determined automatically.

   :param filenames: Either a list of filenames or shell expression
   :type filenames: list or str
   :param datasets: (List of) name(s) of dataset(s) to read (default: all available)
   :type datasets: list or str or None
   :param strictTypes: If True (default), require all dtypes of a given dataset to have the
                       same precision and sign. If False, allow dtypes of different
                       precision and sign across different files. For example, if one
                       file contains a uint32 dataset and another contains an int64
                       dataset with the same name, the contents of both will be read
                       into an int64 pdarray.
   :type strictTypes: bool
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool
   :param calc_string_offsets: Default False, if True this will tell the server to calculate the
                               offsets/segments array on the server versus loading them from HDF5 files.
                               In the future this option may be set to True as the default.
   :type calc_string_offsets: bool
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Ignored if datasets is not `None`
                       Parquet Files only.
   :type read_nested: bool
   :param has_non_float_nulls: Default False. This flag must be set to True to read non-float parquet columns
                               that contain null values.
   :type has_non_float_nulls: bool

   .. rubric:: Notes

   Not currently supported for Categorical or GroupBy datasets

   .. rubric:: Examples

   >>> import arkouda as ak

   Read files and return data with tagging corresponding to the Categorical returned
   cat.codes will link the codes in data to the filename. Data will contain the code `Filename_Codes`
   >>> data, cat = ak.read_tagged_data('path/name') # doctest: +SKIP
   >>> data # doctest: +SKIP
   {'Filname_Codes': array([0 3 6 9 12]), 'col_name': array([0 0 0 1])}


.. py:function:: read_zarr(store_path: str, ndim: int, dtype)

   Read a Zarr store from disk into a pdarray.

   Supports multi-dimensional pdarrays of numeric types.
   To use this function, ensure you have installed the blosc dependency (`make install-blosc`)
   and have included `ZarrMsg.chpl` in the `ServerModules.cfg` file.

   :param store_path: The path to the Zarr store. The path must be to a directory that contains a `.zarray`
                      file containing the Zarr store metadata.
   :type store_path: str
   :param ndim: The number of dimensions in the array
   :type ndim: int
   :param dtype: The data type of the array
   :type dtype: str

   :returns: The pdarray read from the Zarr store.
   :rtype: pdarray


.. py:function:: receive(hostname: str, port)

   Receive a pdarray sent by `pdarray.transfer()`.

   :param hostname: The hostname of the pdarray that sent the array
   :type hostname: str
   :param port: The port to send the array over. This needs to be an
                open port (i.e., not one that the Arkouda server is
                running on). This will open up `numLocales` ports,
                each of which in succession, so will use ports of the
                range {port..(port+numLocales)} (e.g., running an
                Arkouda server of 4 nodes, port 1234 is passed as
                `port`, Arkouda will use ports 1234, 1235, 1236,
                and 1237 to send the array data).
                This port much match the port passed to the call to
                `pdarray.transfer()`.
   :type port: int_scalars

   :returns: The pdarray sent from the sending server to the current
             receiving server.
   :rtype: pdarray

   :raises ValueError: Raised if the op is not within the pdarray.BinOps set
   :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
       a supported dtype


.. py:function:: receive_dataframe(hostname: str, port)

   Receive a pdarray sent by `dataframe.transfer()`.

   :param hostname: The hostname of the dataframe that sent the array
   :type hostname: str
   :param port: The port to send the dataframe over. This needs to be an
                open port (i.e., not one that the Arkouda server is
                running on). This will open up `numLocales` ports,
                each of which in succession, so will use ports of the
                range {port..(port+numLocales)} (e.g., running an
                Arkouda server of 4 nodes, port 1234 is passed as
                `port`, Arkouda will use ports 1234, 1235, 1236,
                and 1237 to send the array data).
                This port much match the port passed to the call to
                `pdarray.send_array()`.
   :type port: int_scalars

   :returns: The dataframe sent from the sending server to the
             current receiving server.
   :rtype: pdarray

   :raises ValueError: Raised if the op is not within the pdarray.BinOps set
   :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
       a supported dtype


.. py:function:: restore(filename)

   Return data saved using `ak.snapshot`.

   :param filename: Name used to create snapshot to be read
   :type filename: str

   :rtype: Dict

   .. rubric:: Notes

   Unlike other save/load methods using snapshot restore will save DataFrames alongside other
   objects in HDF5. Thus, they are returned within the dictionary as a dataframe.


.. py:function:: save_checkpoint(name='', path='.akdata', mode: Literal['overwrite', 'preserve_previous', 'error'] = 'overwrite')

   Save the server's state.

   Records some metadata about the server, and saves
   all pdarrays into parquet files.

   :param name: Name of the checkpoint. The default will be the server session ID, which
                is typically in format ``id_<hash>_``. A directory will be created in
                ``path`` with this name.
   :type name: str
   :param path: The directory to save the checkpoint. If the directory doesn't exist, it
                will be created. If it exists, a new directory for the checkpoint
                instance will be created inside this directory.
   :type path: str
   :param mode: How to handle an existing checkpoint with the same name.
                - ``'overwrite'`` (default): overwrite the checkpoint files.
                - ``'preserve_previous'``: rename existing checkpoint to ``<name>.prev``,
                  overwriting that if it exists.
                - ``'error'``: raise an error if the checkpoint exists.
   :type mode: {'overwrite', 'preserve_previous', 'error'}

   .. rubric:: Notes

   Only ``pdarray``s are saved. Other data structures will not be recorded. We
   expect to expand the coverage in the future.

   :returns: The checkpoint name, which will be the same as the ``name`` argument if
             it was passed.
   :rtype: str

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> arr = ak.zeros(10, int)
   >>> arr[2] = 2
   >>> arr[2]
   np.int64(2)
   >>> cp_name = ak.save_checkpoint()
   >>> arr[2] = 3
   >>> arr[2]
   np.int64(3)
   >>> ak.load_checkpoint(cp_name) # doctest: +SKIP
   >>> arr[2]
   np.int64(3)

   .. seealso:: :py:obj:`load_checkpoint`


.. py:function:: snapshot(filename)

   Create a snapshot of the current Arkouda namespace.

   All currently accessible variables containing
   Arkouda objects will be written to an HDF5 file.

   Unlike other save/load functions, this maintains the integrity of dataframes.

   Current Variable names are used as the dataset name when saving.

   :param filename: Name to use when storing file
   :type filename: str

   .. seealso:: :py:obj:`ak.restore`


.. py:function:: string_operators(cls)

   Add common string operation methods to a StringAccessor class.

   This class decorator dynamically attaches string operations (`contains`,
   `startswith`, `endswith`) to the given class using the `_make_op` helper.

   :param cls: The accessor class to decorate.
   :type cls: type

   :returns: The accessor class with string methods added.
   :rtype: type

   .. rubric:: Notes

   Used internally to implement the `.str` accessor API.


.. py:function:: to_csv(columns: Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings]], List[Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings]]], prefix_path: str, names: Optional[List[str]] = None, col_delim: str = ',', overwrite: bool = False)

   Write Arkouda object(s) to CSV file(s).

   All CSV Files written by Arkouda
   include a header denoting data types of the columns.

   :param columns: The objects to be written to CSV file. If a mapping is used and `names` is None
                   the keys of the mapping will be used as the dataset names.
   :type columns: Mapping[str, pdarray] or List[pdarray]
   :param prefix_path: The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                       when they are written to disk.
   :type prefix_path: str
   :param names: names of dataset to be written. Order should correspond to the order of data
                 provided in `columns`.
   :type names: List[str] (Optional)
   :param col_delim: Defaults to ",". Value to be used to separate columns within the file.
                     Please be sure that the value used DOES NOT appear in your dataset.
   :type col_delim: str
   :param overwrite: Defaults to False. If True, any existing files matching your provided prefix_path will
                     be overwritten. If False, an error will be returned if existing files are found.
   :type overwrite: bool

   :raises ValueError: Raised if any datasets are present in all csv files or if one or
       more of the specified files do not exist
   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
       If `allow_errors` is true this may be raised if no values are returned
       from the server.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. seealso:: :py:obj:`read_csv`

   .. rubric:: Notes

   - CSV format is not currently supported by load/load_all operations
   - The column delimiter is expected to be the same for column names and data
   - Be sure that column delimiters are not found within your data.
   - All CSV files must delimit rows using newline (``\\n``) at this time.
   - Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
     bytes as uint(8).


.. py:function:: to_hdf(columns: Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray]], List[Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray]]], prefix_path: str, names: Optional[List[str]] = None, mode: Literal['truncate', 'append'] = 'truncate', file_type: Literal['single', 'distribute'] = 'distribute') -> None

   Save multiple named pdarrays to HDF5 files.

   :param columns: Collection of arrays to save
   :type columns: dict or list of pdarrays
   :param prefix_path: Directory and filename prefix for output files
   :type prefix_path: str
   :param names: Dataset names for the pdarrays
   :type names: list of str
   :param mode: By default, truncate (overwrite) the output files if they exist.
                If 'append', attempt to create new dataset in existing files.
   :type mode: {"truncate", "append"}
   :param file_type: Default: distribute
                     Single writes the dataset to a single file
                     Distribute writes the dataset to a file per locale.
   :type file_type: {"single", "distribute"}

   :raises ValueError: Raised if (1) the lengths of columns and values differ or (2) the mode
       is not 'truncate' or 'append'
   :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

   .. seealso:: :py:obj:`to_parquet`, :py:obj:`load`, :py:obj:`load_all`, :py:obj:`read`

   .. rubric:: Notes

   Creates one file per locale containing that locale's chunk of each pdarray.
   If columns is a dictionary, the keys are used as the HDF5 dataset names.
   Otherwise, if no names are supplied, 0-up integers are used. By default,
   any existing files at path_prefix will be overwritten, unless the user
   specifies the 'append' mode, in which case arkouda will attempt to add
   <columns> as new datasets to existing files. If the wrong number of files
   is present or dataset names already exist, a RuntimeError is raised.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> a = ak.arange(25)
   >>> b = ak.arange(25)

   Save with mapping defining dataset names
   >>> ak.to_hdf({'a': a, 'b': b}, 'path/name_prefix') # doctest: +SKIP

   Save using names instead of mapping
   >>> ak.to_hdf([a, b], 'path/name_prefix', names=['a', 'b']) # doctest: +SKIP


.. py:function:: to_parquet(columns: Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray]], List[Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray]]], prefix_path: str, names: Optional[List[str]] = None, mode: Literal['truncate', 'append'] = 'truncate', compression: Optional[str] = None, convert_categoricals: bool = False) -> None

   Save multiple named pdarrays to Parquet files.

   :param columns: Collection of arrays to save
   :type columns: dict or list of pdarrays
   :param prefix_path: Directory and filename prefix for output files
   :type prefix_path: str
   :param names: Dataset names for the pdarrays
   :type names: list of str
   :param mode: By default, truncate (overwrite) the output files if they exist.
                If 'append', attempt to create new dataset in existing files.
                'append' is deprecated, please use the multi-column write.
   :type mode: {"truncate", "append"}
   :param compression:     Default None.
                           Provide the compression type to use when writing the file.
                           Supported values: snappy, gzip, brotli, zstd, lz4
                       convert_categoricals: bool
                           Defaults to False
                           Parquet requires all columns to be the same size and Categoricals
                           don't satisfy that requirement.
                           if set, write the equivalent Strings in place of any Categorical columns.
   :type compression: str

   :raises ValueError: Raised if (1) the lengths of columns and values differ or (2) the mode
       is not 'truncate' or 'append'
   :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

   .. seealso:: :py:obj:`to_hdf`, :py:obj:`load`, :py:obj:`load_all`, :py:obj:`read`

   .. rubric:: Notes

   Creates one file per locale containing that locale's chunk of each pdarray.
   If columns is a dictionary, the keys are used as the Parquet column names.
   Otherwise, if no names are supplied, 0-up integers are used. By default,
   any existing files at path_prefix will be deleted
   (regardless of whether they would be overwritten), unless the user
   specifies the 'append' mode, in which case arkouda will attempt to add
   <columns> as new datasets to existing files. If the wrong number of files
   is present or dataset names already exist, a RuntimeError is raised.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> a = ak.arange(25)
   >>> b = ak.arange(25)

   Save with mapping defining dataset names
   >>> ak.to_parquet({'a': a, 'b': b}, 'path/name_prefix') # doctest: +SKIP

   Save using names instead of mapping
   >>> ak.to_parquet([a, b], 'path/name_prefix', names=['a', 'b']) # doctest: +SKIP


.. py:function:: to_zarr(store_path: str, arr: arkouda.numpy.pdarrayclass.pdarray, chunk_shape)

   Write a pdarray to disk as a Zarr store.

   Supports multi-dimensional pdarrays of numeric types.
   To use this function, ensure you have installed the blosc dependency (`make install-blosc`)
   and have included `ZarrMsg.chpl` in the `ServerModules.cfg` file.

   :param store_path: The path at which Zarr store should be written
   :type store_path: str
   :param arr: The pdarray to be written to disk
   :type arr: pdarray
   :param chunk_shape: The shape of the chunks to be used in the Zarr store
   :type chunk_shape: tuple

   :raises ValueError: Raised if the number of dimensions in the chunk shape does not match
       the number of dimensions in the array or if the array is not a 32 or 64 bit numeric type


.. py:function:: update_hdf(columns: Union[Mapping[str, Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray]], List[Union[arkouda.numpy.pdarrayclass.pdarray, arkouda.numpy.strings.Strings, arkouda.numpy.segarray.SegArray]]], prefix_path: str, names: Optional[List[str]] = None, repack: bool = True)

   Overwrite the datasets with name appearing in names or keys in columns if columns is a dictionary.

   :param columns: Collection of arrays to save
   :type columns: dict or list of pdarrays
   :param prefix_path: Directory and filename prefix for output files
   :type prefix_path: str
   :param names: Dataset names for the pdarrays
   :type names: list of str
   :param repack: Default: True
                  HDF5 does not release memory on delete. When True, the inaccessible
                  data (that was overwritten) is removed. When False, the data remains, but is
                  inaccessible. Setting to false will yield better performance, but will cause
                  file sizes to expand.
   :type repack: bool

   :raises RuntimeError: Raised if a server-side error is thrown saving the datasets

   .. rubric:: Notes

   - If file does not contain File_Format attribute to indicate how it was saved,
     the file name is checked for _LOCALE#### to determine if it is distributed.
   - If the datasets provided do not exist, they will be added
   - Because HDF5 deletes do not release memory, this will create a copy of the
     file with the new data
   - This workflow is slightly different from `to_hdf` to prevent reading and
     creating a copy of the file for each dataset


.. py:function:: write_log(log_msg: str, tag: str = 'ClientGeneratedLog', log_lvl: LogLevel = LogLevel.INFO)

   Allow the user to write custom logs.

   :param log_msg: The message to be added to the server log
   :type log_msg: str
   :param tag: The tag to use in the log. This takes the place of the server function name.
               Allows for easy identification of custom logs.
               Defaults to "ClientGeneratedLog"
   :type tag: str
   :param log_lvl: The type of log to be written
                   Defaults to LogLevel.INFO
   :type log_lvl: LogLevel

   .. seealso:: :py:obj:`LogLevel`


.. py:function:: xlogy(x: Union[arkouda.numpy.pdarrayclass.pdarray, numpy.float64], y: arkouda.numpy.pdarrayclass.pdarray)

   Computes x * log(y).

   :param x: x must have a datatype that is castable to float64
   :type x: pdarray or np.float64
   :param y:
   :type y: pdarray

   :rtype: arkouda.numpy.pdarrayclass.pdarray

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.scipy.special import xlogy
   >>> xlogy( ak.array([1, 2, 3, 4]),  ak.array([5,6,7,8]))
   array([1.6094379124341003 3.5835189384561099 5.8377304471659395 8.317766166719343])
   >>> xlogy( 5.0, ak.array([1, 2, 3, 4]))
   array([0.00000000000000000 3.4657359027997265 5.4930614433405491 6.9314718055994531])


