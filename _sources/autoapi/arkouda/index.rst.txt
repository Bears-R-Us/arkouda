arkouda
=======

.. py:module:: arkouda


Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/arkouda/accessor/index
   /autoapi/arkouda/alignment/index
   /autoapi/arkouda/apply/index
   /autoapi/arkouda/array_api/index
   /autoapi/arkouda/categorical/index
   /autoapi/arkouda/client/index
   /autoapi/arkouda/client_dtypes/index
   /autoapi/arkouda/comm_diagnostics/index
   /autoapi/arkouda/dataframe/index
   /autoapi/arkouda/dtypes/index
   /autoapi/arkouda/groupbyclass/index
   /autoapi/arkouda/history/index
   /autoapi/arkouda/index/index
   /autoapi/arkouda/infoclass/index
   /autoapi/arkouda/io/index
   /autoapi/arkouda/io_util/index
   /autoapi/arkouda/join/index
   /autoapi/arkouda/logger/index
   /autoapi/arkouda/match/index
   /autoapi/arkouda/matcher/index
   /autoapi/arkouda/numeric/index
   /autoapi/arkouda/numpy/index
   /autoapi/arkouda/pdarrayclass/index
   /autoapi/arkouda/pdarraycreation/index
   /autoapi/arkouda/pdarraymanipulation/index
   /autoapi/arkouda/pdarraysetops/index
   /autoapi/arkouda/plotting/index
   /autoapi/arkouda/random/index
   /autoapi/arkouda/row/index
   /autoapi/arkouda/scipy/index
   /autoapi/arkouda/security/index
   /autoapi/arkouda/segarray/index
   /autoapi/arkouda/series/index
   /autoapi/arkouda/sorting/index
   /autoapi/arkouda/sparrayclass/index
   /autoapi/arkouda/sparsematrix/index
   /autoapi/arkouda/strings/index
   /autoapi/arkouda/testing/index
   /autoapi/arkouda/timeclass/index
   /autoapi/arkouda/util/index


Attributes
----------

.. autoapisummary::

   arkouda.AllSymbols
   arkouda.Inf
   arkouda.Infinity
   arkouda.LEN_SUFFIX
   arkouda.NAN
   arkouda.NINF
   arkouda.NZERO
   arkouda.NaN
   arkouda.PINF
   arkouda.PZERO
   arkouda.RegisteredSymbols
   arkouda.SEG_SUFFIX
   arkouda.SortingAlgorithm
   arkouda.VAL_SUFFIX
   arkouda.e
   arkouda.euler_gamma
   arkouda.inf
   arkouda.infty
   arkouda.nan
   arkouda.pi


Exceptions
----------

.. autoapisummary::

   arkouda.NonUniqueError
   arkouda.RegistrationError
   arkouda.RegistrationError
   arkouda.RegistrationError
   arkouda.RegistrationError
   arkouda.RegistrationError


Classes
-------

.. autoapisummary::

   arkouda.ARKOUDA_SUPPORTED_DTYPES
   arkouda.ARKOUDA_SUPPORTED_INTS
   arkouda.BitVector
   arkouda.BoolDType
   arkouda.ByteDType
   arkouda.BytesDType
   arkouda.CLongDoubleDType
   arkouda.CachedAccessor
   arkouda.Categorical
   arkouda.Categorical
   arkouda.Categorical
   arkouda.Complex128DType
   arkouda.Complex64DType
   arkouda.DType
   arkouda.DTypeObjects
   arkouda.DTypes
   arkouda.DataFrame
   arkouda.DataFrame
   arkouda.DataFrameGroupBy
   arkouda.DataSource
   arkouda.DateTime64DType
   arkouda.Datetime
   arkouda.Datetime
   arkouda.Datetime
   arkouda.DatetimeAccessor
   arkouda.DiffAggregate
   arkouda.ErrorMode
   arkouda.False_
   arkouda.Fields
   arkouda.Float16DType
   arkouda.Float32DType
   arkouda.Float64DType
   arkouda.GROUPBY_REDUCTION_TYPES
   arkouda.GroupBy
   arkouda.GroupBy
   arkouda.GroupBy
   arkouda.GroupBy
   arkouda.GroupBy
   arkouda.GroupBy
   arkouda.IPv4
   arkouda.Index
   arkouda.Int16DType
   arkouda.Int32DType
   arkouda.Int64DType
   arkouda.Int8DType
   arkouda.IntDType
   arkouda.LogLevel
   arkouda.LongDType
   arkouda.LongDoubleDType
   arkouda.LongLongDType
   arkouda.MultiIndex
   arkouda.NUMBER_FORMAT_STRINGS
   arkouda.NumericDTypes
   arkouda.ObjectDType
   arkouda.Power_divergenceResult
   arkouda.Properties
   arkouda.RankWarning
   arkouda.Row
   arkouda.ScalarDTypes
   arkouda.ScalarType
   arkouda.SegArray
   arkouda.Series
   arkouda.SeriesDTypes
   arkouda.ShortDType
   arkouda.StrDType
   arkouda.StringAccessor
   arkouda.TimeDelta64DType
   arkouda.Timedelta
   arkouda.Timedelta
   arkouda.TooHardError
   arkouda.True_
   arkouda.UByteDType
   arkouda.UInt16DType
   arkouda.UInt32DType
   arkouda.UInt64DType
   arkouda.UInt8DType
   arkouda.UIntDType
   arkouda.ULongDType
   arkouda.ULongLongDType
   arkouda.UShortDType
   arkouda.VoidDType
   arkouda.akbool
   arkouda.akbool
   arkouda.akfloat64
   arkouda.akfloat64
   arkouda.akint64
   arkouda.akint64
   arkouda.akint64
   arkouda.akuint64
   arkouda.akuint64
   arkouda.akuint64
   arkouda.all_scalars
   arkouda.bigint
   arkouda.bigint
   arkouda.bitType
   arkouda.bitType
   arkouda.bool_
   arkouda.bool_scalars
   arkouda.bool_scalars
   arkouda.byte
   arkouda.bytes_
   arkouda.cdouble
   arkouda.cfloat
   arkouda.character
   arkouda.clongdouble
   arkouda.clongfloat
   arkouda.complex128
   arkouda.complex64
   arkouda.csingle
   arkouda.datetime64
   arkouda.double
   arkouda.finfo
   arkouda.flexible
   arkouda.float16
   arkouda.float32
   arkouda.float64
   arkouda.float_
   arkouda.float_scalars
   arkouda.floating
   arkouda.format_parser
   arkouda.half
   arkouda.iinfo
   arkouda.inexact
   arkouda.int16
   arkouda.int32
   arkouda.int64
   arkouda.int64
   arkouda.int8
   arkouda.intTypes
   arkouda.intTypes
   arkouda.intTypes
   arkouda.int_
   arkouda.int_scalars
   arkouda.int_scalars
   arkouda.int_scalars
   arkouda.intc
   arkouda.integer
   arkouda.intp
   arkouda.longdouble
   arkouda.longfloat
   arkouda.longlong
   arkouda.number
   arkouda.numeric_and_bool_scalars
   arkouda.numeric_scalars
   arkouda.numpy_scalars
   arkouda.object_
   arkouda.pdarray
   arkouda.pdarray
   arkouda.pdarray
   arkouda.pdarray
   arkouda.pdarray
   arkouda.pdarray
   arkouda.sctypeDict
   arkouda.sctypes
   arkouda.short
   arkouda.signedinteger
   arkouda.single
   arkouda.sparray
   arkouda.str_
   arkouda.str_
   arkouda.str_scalars
   arkouda.timedelta64
   arkouda.ubyte
   arkouda.uint
   arkouda.uint16
   arkouda.uint32
   arkouda.uint64
   arkouda.uint8
   arkouda.uintc
   arkouda.uintp
   arkouda.ulonglong
   arkouda.unsignedinteger
   arkouda.ushort
   arkouda.void


Functions
---------

.. autoapisummary::

   arkouda.BitVectorizer
   arkouda.abs
   arkouda.add_newdoc
   arkouda.akabs
   arkouda.akcast
   arkouda.akcast
   arkouda.align
   arkouda.apply
   arkouda.arange
   arkouda.arange
   arkouda.arange
   arkouda.arange
   arkouda.arange
   arkouda.arange
   arkouda.arccos
   arkouda.arccosh
   arkouda.arcsin
   arkouda.arcsinh
   arkouda.arctan
   arkouda.arctan2
   arkouda.arctanh
   arkouda.argmaxk
   arkouda.argmink
   arkouda.argsort
   arkouda.argsort
   arkouda.argsort
   arkouda.array
   arkouda.array
   arkouda.array
   arkouda.array
   arkouda.array_equal
   arkouda.assert_almost_equal
   arkouda.assert_almost_equivalent
   arkouda.assert_arkouda_array_equal
   arkouda.assert_arkouda_array_equivalent
   arkouda.assert_arkouda_pdarray_equal
   arkouda.assert_arkouda_segarray_equal
   arkouda.assert_arkouda_strings_equal
   arkouda.assert_attr_equal
   arkouda.assert_categorical_equal
   arkouda.assert_class_equal
   arkouda.assert_contains_all
   arkouda.assert_copy
   arkouda.assert_dict_equal
   arkouda.assert_equal
   arkouda.assert_equivalent
   arkouda.assert_frame_equal
   arkouda.assert_frame_equivalent
   arkouda.assert_index_equal
   arkouda.assert_index_equivalent
   arkouda.assert_is_sorted
   arkouda.assert_series_equal
   arkouda.assert_series_equivalent
   arkouda.attach
   arkouda.attach_all
   arkouda.attach_pdarray
   arkouda.base_repr
   arkouda.bigint_from_uint_arrays
   arkouda.binary_repr
   arkouda.broadcast
   arkouda.broadcast
   arkouda.broadcast
   arkouda.broadcast
   arkouda.broadcast_dims
   arkouda.broadcast_to_shape
   arkouda.cast
   arkouda.cast
   arkouda.ceil
   arkouda.chisquare
   arkouda.clear
   arkouda.clip
   arkouda.clz
   arkouda.coargsort
   arkouda.coargsort
   arkouda.coargsort
   arkouda.compute_join_size
   arkouda.concatenate
   arkouda.concatenate
   arkouda.concatenate
   arkouda.convert_if_categorical
   arkouda.corr
   arkouda.cos
   arkouda.cosh
   arkouda.count_nonzero
   arkouda.cov
   arkouda.create_pdarray
   arkouda.create_pdarray
   arkouda.create_pdarray
   arkouda.create_pdarray
   arkouda.create_pdarray
   arkouda.create_sparray
   arkouda.create_sparse_matrix
   arkouda.ctz
   arkouda.cumprod
   arkouda.cumsum
   arkouda.cumsum
   arkouda.cumsum
   arkouda.date_operators
   arkouda.date_range
   arkouda.date_range
   arkouda.deg2rad
   arkouda.delete
   arkouda.deprecate
   arkouda.deprecate_with_doc
   arkouda.disableVerbose
   arkouda.disp
   arkouda.divmod
   arkouda.dot
   arkouda.dtype
   arkouda.enableVerbose
   arkouda.exp
   arkouda.expm1
   arkouda.export
   arkouda.eye
   arkouda.find
   arkouda.flip
   arkouda.floor
   arkouda.fmod
   arkouda.format_float_positional
   arkouda.format_float_scientific
   arkouda.from_series
   arkouda.from_series
   arkouda.full
   arkouda.full
   arkouda.full_like
   arkouda.gen_ranges
   arkouda.gen_ranges
   arkouda.generic_concat
   arkouda.getArkoudaLogger
   arkouda.get_byteorder
   arkouda.get_callback
   arkouda.get_columns
   arkouda.get_datasets
   arkouda.get_filetype
   arkouda.get_null_indices
   arkouda.get_server_byteorder
   arkouda.hash
   arkouda.hist_all
   arkouda.histogram
   arkouda.histogram
   arkouda.histogram2d
   arkouda.histogramdd
   arkouda.import_data
   arkouda.in1d
   arkouda.in1d
   arkouda.in1d
   arkouda.in1d_intervals
   arkouda.indexof1d
   arkouda.information
   arkouda.intersect
   arkouda.intersect1d
   arkouda.interval_lookup
   arkouda.intx
   arkouda.invert_permutation
   arkouda.ip_address
   arkouda.isSupportedBool
   arkouda.isSupportedDType
   arkouda.isSupportedFloat
   arkouda.isSupportedInt
   arkouda.isSupportedInt
   arkouda.isSupportedInt
   arkouda.isSupportedInt
   arkouda.isSupportedNumber
   arkouda.is_cosorted
   arkouda.is_ipv4
   arkouda.is_ipv6
   arkouda.is_registered
   arkouda.isfinite
   arkouda.isinf
   arkouda.isnan
   arkouda.isnan
   arkouda.isscalar
   arkouda.issctype
   arkouda.issubclass_
   arkouda.issubdtype
   arkouda.join_on_eq_with_dt
   arkouda.left_align
   arkouda.linspace
   arkouda.list_registry
   arkouda.list_symbol_table
   arkouda.load
   arkouda.load_all
   arkouda.load_checkpoint
   arkouda.log
   arkouda.log10
   arkouda.log1p
   arkouda.log2
   arkouda.lookup
   arkouda.ls
   arkouda.ls_csv
   arkouda.matmul
   arkouda.maximum_sctype
   arkouda.maxk
   arkouda.mean
   arkouda.median
   arkouda.merge
   arkouda.mink
   arkouda.mod
   arkouda.ones
   arkouda.ones
   arkouda.ones
   arkouda.ones
   arkouda.ones_like
   arkouda.parity
   arkouda.plot_dist
   arkouda.popcount
   arkouda.power
   arkouda.power_divergence
   arkouda.pretty_print_information
   arkouda.promote_to_common_dtype
   arkouda.putmask
   arkouda.rad2deg
   arkouda.randint
   arkouda.random_sparse_matrix
   arkouda.random_strings_lognormal
   arkouda.random_strings_uniform
   arkouda.read
   arkouda.read_csv
   arkouda.read_hdf
   arkouda.read_parquet
   arkouda.read_tagged_data
   arkouda.read_zarr
   arkouda.receive
   arkouda.receive_dataframe
   arkouda.register_all
   arkouda.resolve_scalar_dtype
   arkouda.restore
   arkouda.right_align
   arkouda.rotl
   arkouda.rotr
   arkouda.round
   arkouda.save_all
   arkouda.save_checkpoint
   arkouda.scalar_array
   arkouda.search_intervals
   arkouda.segarray
   arkouda.setdiff1d
   arkouda.setxor1d
   arkouda.shape
   arkouda.sign
   arkouda.sin
   arkouda.sinh
   arkouda.skew
   arkouda.snapshot
   arkouda.sort
   arkouda.sparse_matrix_matrix_mult
   arkouda.sqrt
   arkouda.square
   arkouda.squeeze
   arkouda.standard_normal
   arkouda.std
   arkouda.string_operators
   arkouda.tan
   arkouda.tanh
   arkouda.timedelta_range
   arkouda.timedelta_range
   arkouda.to_csv
   arkouda.to_hdf
   arkouda.to_parquet
   arkouda.to_zarr
   arkouda.transpose
   arkouda.tril
   arkouda.triu
   arkouda.trunc
   arkouda.typename
   arkouda.uniform
   arkouda.union1d
   arkouda.unique
   arkouda.unique
   arkouda.unique
   arkouda.unregister
   arkouda.unregister_all
   arkouda.unregister_pdarray_by_name
   arkouda.unsqueeze
   arkouda.update_hdf
   arkouda.value_counts
   arkouda.var
   arkouda.vecdot
   arkouda.vstack
   arkouda.where
   arkouda.where
   arkouda.where
   arkouda.write_log
   arkouda.xlogy
   arkouda.zero_up
   arkouda.zeros
   arkouda.zeros
   arkouda.zeros
   arkouda.zeros
   arkouda.zeros_like


Package Contents
----------------

.. py:class:: ARKOUDA_SUPPORTED_DTYPES

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: ARKOUDA_SUPPORTED_INTS

   Built-in immutable sequence.

   If no argument is given, the constructor returns an empty tuple.
   If iterable is specified the tuple is initialized from iterable's items.

   If the argument is a tuple, the return value is the same object.



   .. py:method:: count(value, /)

      Return number of occurrences of value.




   .. py:method:: index(value, start=0, stop=9223372036854775807, /)

      Return first index of value.

      Raises ValueError if the value is not present.




.. py:data:: AllSymbols
   :value: '__AllSymbols__'


.. py:class:: BitVector(values, width=64, reverse=False)

   Bases: :py:obj:`arkouda.pdarrayclass.pdarray`


   Represent integers as bit vectors, e.g. a set of flags.

   :param values: The integers to represent as bit vectors
   :type values: pdarray, int64
   :param width: The number of bit fields in the vector
   :type width: int
   :param reverse: If True, display bits from least significant (left) to most
                   significant (right). By default, the most significant bit
                   is the left-most bit.
   :type reverse: bool

   :returns: **bitvectors** -- The array of binary vectors
   :rtype: BitVector

   .. rubric:: Notes

   This class is a thin wrapper around pdarray that mostly affects
   how values are displayed to the user. Operators and methods will
   typically treat this class like a uint64 pdarray.


   .. py:attribute:: conserves


   .. py:method:: format(x)

      Format a single binary vector as a string.



   .. py:method:: from_return_msg(rep_msg)
      :classmethod:



   .. py:method:: opeq(other, op)


   .. py:method:: register(user_defined_name)

      Register this BitVector object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the BitVector is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same BitVector which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different BitVectors with the same name.
      :rtype: BitVector

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the BitVector with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :value: None



   .. py:attribute:: reverse
      :value: False



   .. py:attribute:: special_objType
      :value: 'BitVector'



   .. py:method:: to_list()

      Export data to a list of string-formatted bit vectors.



   .. py:method:: to_ndarray()

      Export data to a numpy array of string-formatted bit vectors.



   .. py:attribute:: values


   .. py:attribute:: width
      :value: 64



.. py:function:: BitVectorizer(width=64, reverse=False)

   Make a callback (i.e. function) that can be called on an
   array to create a BitVector.

   :param width: The number of bit fields in the vector
   :type width: int
   :param reverse: If True, display bits from least significant (left) to most
                   significant (right). By default, the most significant bit
                   is the left-most bit.
   :type reverse: bool

   :returns: **bitvectorizer** -- A function that takes an array and returns a BitVector instance
   :rtype: callable


.. py:class:: BoolDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: ByteDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: BytesDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: CLongDoubleDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: CachedAccessor(name: str, accessor)

   Custom property-like object.
   A descriptor for caching accessors.
   :param name: Namespace that will be accessed under, e.g. ``df.foo``.
   :type name: str
   :param accessor: Class with the extension methods.
   :type accessor: cls

   .. rubric:: Notes

   For accessor, The class's __init__ method assumes that one of
   ``Series``, ``DataFrame`` or ``Index`` as the
   single argument ``data``.


.. py:class:: Categorical(values, **kwargs)

   Represents an array of values belonging to named categories. Converting a
   Strings object to Categorical often saves memory and speeds up operations,
   especially if there are many repeated values, at the cost of some one-time
   work in initialization.

   :param values: Values to convert to categories
   :type values: Strings, Categorical, pd.Categorical
   :param NAvalue: The value to use to represent missing/null data
   :type NAvalue: str scalar

   .. attribute:: categories

      The set of category labels (determined automatically)

      :type: Strings

   .. attribute:: codes

      The category indices of the values or -1 for N/A

      :type: pdarray, int64

   .. attribute:: permutation

      The permutation that groups the values in the same order as categories

      :type: pdarray, int64

   .. attribute:: segments

      When values are grouped, the starting offset of each group

      :type: pdarray, int64

   .. attribute:: size

      The number of items in the array

      :type: Union[int,np.int64]

   .. attribute:: nlevels

      The number of distinct categories

      :type: Union[int,np.int64]

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: Union[int,np.int64]

   .. attribute:: shape

      The sizes of each dimension of the array

      :type: tuple


   .. py:attribute:: BinOps


   .. py:attribute:: RegisterablePieces


   .. py:attribute:: RequiredPieces


   .. py:method:: argsort()


   .. py:method:: attach(user_defined_name: str) -> Categorical
      :staticmethod:


      DEPRECATED
      Function to return a Categorical object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which Categorical object was registered under
      :type user_defined_name: str

      :returns: The Categorical object created by re-attaching to the corresponding server components
      :rtype: Categorical

      :raises TypeError: if user_defined_name is not a string

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_categorical_by_name`



   .. py:method:: concatenate(others: Sequence[Categorical], ordered: bool = True) -> Categorical

      Merge this Categorical with other Categorical objects in the array,
      concatenating the arrays and synchronizing the categories.

      :param others: The Categorical arrays to concatenate and merge with this one
      :type others: Sequence[Categorical]
      :param ordered: If True (default), the arrays will be appended in the
                      order given. If False, array data may be interleaved
                      in blocks, which can greatly improve performance but
                      results in non-deterministic ordering of elements.
      :type ordered: bool

      :returns: The merged Categorical object
      :rtype: Categorical

      :raises TypeError: Raised if any others array objects are not Categorical objects

      .. rubric:: Notes

      This operation can be expensive -- slower than concatenating Strings.



   .. py:method:: contains(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element contains the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that contain substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.startswith`, :obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:attribute:: dtype


   .. py:method:: endswith(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element ends with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that end with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.startswith`, :obj:`Categorical.contains`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether Categoricals are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the Categoricals are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> c = Categorical(ak.array(["a", "b", "c"]))
      >>> c_cpy = Categorical(ak.array(["a", "b", "c"]))
      >>> c.equals(c_cpy)
      True
      >>> c2 = Categorical(ak.array(["a", "x", "c"]))
      >>> c.equals(c2)
      False



   .. py:method:: from_codes(codes: arkouda.pdarrayclass.pdarray, categories: arkouda.strings.Strings, permutation=None, segments=None, **kwargs) -> Categorical
      :classmethod:


      Make a Categorical from codes and categories arrays. If codes and
      categories have already been pre-computed, this constructor saves
      time. If not, please use the normal constructor.

      :param codes: Category indices of each value
      :type codes: pdarray, int64
      :param categories: Unique category labels
      :type categories: Strings
      :param permutation: The permutation that groups the values in the same order
                          as categories
      :type permutation: pdarray, int64
      :param segments: When values are grouped, the starting offset of each group
      :type segments: pdarray, int64

      :returns: The Categorical object created from the input parameters
      :rtype: Categorical

      :raises TypeError: Raised if codes is not a pdarray of int64 objects or if
          categories is not a Strings object



   .. py:method:: from_return_msg(rep_msg) -> Categorical
      :classmethod:


      Create categorical from return message from server

      .. rubric:: Notes

      This is currently only used when reading a Categorical from HDF5 files.



   .. py:method:: group() -> arkouda.pdarrayclass.pdarray

      Return the permutation that groups the array, placing equivalent
      categories together. All instances of the same category are guaranteed
      to lie in one contiguous block of the permuted array, but the blocks
      are not necessarily ordered.

      :returns: The permutation that groups the array by value
      :rtype: pdarray

      .. seealso:: :obj:`GroupBy`, :obj:`unique`

      .. rubric:: Notes

      This method is faster than the corresponding Strings method. If the
      Categorical was created from a Strings object, then this function
      simply returns the cached permutation. Even if the Categorical was
      created using from_codes(), this function will be faster than
      Strings.group() because it sorts dense integer values, rather than
      128-bit hash values.



   .. py:method:: hash() -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

      Compute a 128-bit hash of each element of the Categorical.

      :returns: A tuple of two int64 pdarrays. The ith hash value is the concatenation
                of the ith values from each array.
      :rtype: Tuple[pdarray,pdarray]

      .. rubric:: Notes

      The implementation uses SipHash128, a fast and balanced hash function (used
      by Python for dictionaries and sets). For realistic numbers of strings (up
      to about 10**15), the probability of a collision between two 128-bit hash
      values is negligible.



   .. py:method:: in1d(test: Union[arkouda.strings.Strings, Categorical]) -> arkouda.pdarrayclass.pdarray

      Test whether each element of the Categorical object is
      also present in the test Strings or Categorical object.

      Returns a boolean array the same length as `self` that is True
      where an element of `self` is in `test` and False otherwise.

      :param test: The values against which to test each value of 'self`.
      :type test: Union[Strings,Categorical]

      :returns: The values `self[in1d]` are in the `test` Strings or Categorical object.
      :rtype: pdarray, bool

      :raises TypeError: Raised if test is not a Strings or Categorical object

      .. seealso:: :obj:`unique`, :obj:`intersect1d`, :obj:`union1d`

      .. rubric:: Notes

      `in1d` can be considered as an element-wise function version of the
      python keyword `in`, for 1-D sequences. ``in1d(a, b)`` is logically
      equivalent to ``ak.array([item in b for item in a])``, but is much
      faster and scales to arbitrarily large ``a``.

      .. rubric:: Examples

      >>> strings = ak.array([f'String {i}' for i in range(0,5)])
      >>> cat = ak.Categorical(strings)
      >>> ak.in1d(cat,strings)
      array([True True True True True])
      >>> strings = ak.array([f'String {i}' for i in range(5,9)])
      >>> catTwo = ak.Categorical(strings)
      >>> ak.in1d(cat,catTwo)
      array([False False False False False])



   .. py:property:: inferred_type
      :type: str


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_categorical_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isna()

      Find where values are missing or null (as defined by self.NAvalue)



   .. py:attribute:: logger


   .. py:property:: nbytes

      The size of the Categorical in bytes.

      :returns: The size of the Categorical in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: nlevels


   .. py:attribute:: objType
      :value: 'Categorical'



   .. py:method:: parse_hdf_categoricals(d: Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]]) -> Tuple[List[str], Dict[str, Categorical]]
      :staticmethod:


      This function should be used in conjunction with the load_all function which reads hdf5 files
      and reconstitutes Categorical objects.
      Categorical objects use a naming convention and HDF5 structure so they can be identified and
      constructed for the user.

      In general you should not call this method directly

      :param d:
      :type d: Dictionary of String to either Pdarray or Strings object

      :returns: * *2-Tuple of List of strings containing key names which should be removed and Dictionary of*
                * *base name to Categorical object*

      .. seealso:: :obj:`Categorical.save`, :obj:`load_all`



   .. py:attribute:: permutation
      :type:  Union[arkouda.pdarrayclass.pdarray, None]
      :value: None



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: register(user_defined_name: str) -> Categorical

      Register this Categorical object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Categorical is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Categorical which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Categoricals with the same name.
      :rtype: Categorical

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Categorical with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_categorical_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reset_categories() -> Categorical

      Recompute the category labels, discarding any unused labels. This
      method is often useful after slicing or indexing a Categorical array,
      when the resulting array only contains a subset of the original
      categories. In this case, eliminating unused categories can speed up
      other operations.

      :returns: A Categorical object generated from the current instance
      :rtype: Categorical



   .. py:method:: save(prefix_path: str, dataset: str = 'categorical_array', file_format: str = 'HDF5', mode: str = 'truncate', file_type: str = 'distribute', compression: Optional[str] = None) -> str

      DEPRECATED
      Save the Categorical object to HDF5 or Parquet. The result is a collection of HDF5/Parquet files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path and dataset. Each locale saves its chunk of the Strings array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in HDF5 files (must not already exist)
      :type dataset: str
      :param file_format: The format to save the file to.
      :type file_format: str {'HDF5 | 'Parquet'}
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Categorical dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")
      :param compression: {None | 'snappy' | 'gzip' | 'brotli' | 'zstd' | 'lz4'}
                          The compression type to use when writing.
                          This is only supported for Parquet files and will not be used with HDF5.
      :type compression: str (Optional)

      :rtype: String message indicating result of save operation

      :raises ValueError: Raised if the lengths of columns and values differ, or the mode is
          neither 'truncate' nor 'append'
      :raises TypeError: Raised if prefix_path, dataset, or mode is not a str

      .. rubric:: Notes

      Important implementation notes: (1) Strings state is saved as two datasets
      within an hdf5 group: one for the string characters and one for the
      segments corresponding to the start of each string, (2) the hdf5 group is named
      via the dataset parameter.

      .. seealso:: :obj:`-`, :obj:`-`



   .. py:attribute:: segments
      :value: None



   .. py:method:: set_categories(new_categories, NAvalue=None)

      Set categories to user-defined values.

      :param new_categories: The array of new categories to use. Must be unique.
      :type new_categories: Strings
      :param NAvalue: The value to use to represent missing/null data
      :type NAvalue: str scalar

      :returns: A new Categorical with the user-defined categories. Old values present
                in new categories will appear unchanged. Old values not present will
                be assigned the NA value.
      :rtype: Categorical



   .. py:attribute:: shape


   .. py:attribute:: size
      :type:  arkouda.numpy.dtypes.int_scalars


   .. py:method:: sort_values()


   .. py:method:: standardize_categories(arrays, NAvalue='N/A')
      :classmethod:


      Standardize an array of Categoricals so that they share the same categories.

      :param arrays: The Categoricals to standardize
      :type arrays: sequence of Categoricals
      :param NAvalue: The value to use to represent missing/null data
      :type NAvalue: str scalar

      :returns: A list of the original Categoricals remapped to the shared categories.
      :rtype: List of Categoricals



   .. py:method:: startswith(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element starts with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that start with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.contains`, :obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding
      method on Strings objects, because it searches the unique category
      labels instead of the full array.



   .. py:method:: to_hdf(prefix_path, dataset='categorical_array', mode='truncate', file_type='distribute')

      Save the Categorical to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
      :type file_type: str ("single" | "distribute")

      :rtype: None

      .. seealso:: :obj:`load`



   .. py:method:: to_list() -> List

      Convert the Categorical to a list, transferring data from
      the arkouda server to Python. This conversion discards category
      information and produces a list of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A list of strings corresponding to the values in
                this Categorical
      :rtype: list

      .. rubric:: Notes

      The number of bytes in the Categorical cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from
      the arkouda server to Python. This conversion discards category
      information and produces an ndarray of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A numpy ndarray of strings corresponding to the values in
                this array
      :rtype: np.ndarray

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_pandas() -> pandas.Categorical

      Return the equivalent Pandas Categorical.



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'categorical_array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      This functionality is currently not supported and will also raise a RuntimeError.
      Support is in development.
      Save the Categorical to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in HDF5 files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Categorical dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: Default None
                          Provide the compression type to use when writing the file.
                          Supported values: snappy, gzip, brotli, zstd, lz4
      :type compression: str (Optional)

      :rtype: String message indicating result of save operation

      :raises RuntimeError: On run due to compatability issues of Categorical with Parquet.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. seealso:: :obj:`to_hdf`



   .. py:method:: to_strings() -> List

      Convert the Categorical to Strings.

      :returns: A Strings object corresponding to the values in
                this Categorical.
      :rtype: arkouda.strings.Strings

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array(["a","b","c"])
      >>> a
      array(['a', 'b', 'c'])
      >>> c = ak.Categorical(a)
      >>> c.to_strings()
      array(['a', 'b', 'c'])

      >>> isinstance(c.to_strings(), ak.Strings)
      True



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a Categorical object to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the Categorical is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unique() -> Categorical


   .. py:method:: unregister() -> None

      Unregister this Categorical object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_categorical_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_categorical_by_name(user_defined_name: str) -> None
      :staticmethod:


      Function to unregister Categorical object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the Categorical object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path, dataset='categorical_array', repack=True)

      Overwrite the dataset with the name provided with this Categorical object. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the Categorical

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, the repack option allows for
        automatic creation of a file without the inaccessible data.



.. py:class:: Categorical(values, **kwargs)

   Represents an array of values belonging to named categories. Converting a
   Strings object to Categorical often saves memory and speeds up operations,
   especially if there are many repeated values, at the cost of some one-time
   work in initialization.

   :param values: Values to convert to categories
   :type values: Strings, Categorical, pd.Categorical
   :param NAvalue: The value to use to represent missing/null data
   :type NAvalue: str scalar

   .. attribute:: categories

      The set of category labels (determined automatically)

      :type: Strings

   .. attribute:: codes

      The category indices of the values or -1 for N/A

      :type: pdarray, int64

   .. attribute:: permutation

      The permutation that groups the values in the same order as categories

      :type: pdarray, int64

   .. attribute:: segments

      When values are grouped, the starting offset of each group

      :type: pdarray, int64

   .. attribute:: size

      The number of items in the array

      :type: Union[int,np.int64]

   .. attribute:: nlevels

      The number of distinct categories

      :type: Union[int,np.int64]

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: Union[int,np.int64]

   .. attribute:: shape

      The sizes of each dimension of the array

      :type: tuple


   .. py:attribute:: BinOps


   .. py:attribute:: RegisterablePieces


   .. py:attribute:: RequiredPieces


   .. py:method:: argsort()


   .. py:method:: attach(user_defined_name: str) -> Categorical
      :staticmethod:


      DEPRECATED
      Function to return a Categorical object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which Categorical object was registered under
      :type user_defined_name: str

      :returns: The Categorical object created by re-attaching to the corresponding server components
      :rtype: Categorical

      :raises TypeError: if user_defined_name is not a string

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_categorical_by_name`



   .. py:method:: concatenate(others: Sequence[Categorical], ordered: bool = True) -> Categorical

      Merge this Categorical with other Categorical objects in the array,
      concatenating the arrays and synchronizing the categories.

      :param others: The Categorical arrays to concatenate and merge with this one
      :type others: Sequence[Categorical]
      :param ordered: If True (default), the arrays will be appended in the
                      order given. If False, array data may be interleaved
                      in blocks, which can greatly improve performance but
                      results in non-deterministic ordering of elements.
      :type ordered: bool

      :returns: The merged Categorical object
      :rtype: Categorical

      :raises TypeError: Raised if any others array objects are not Categorical objects

      .. rubric:: Notes

      This operation can be expensive -- slower than concatenating Strings.



   .. py:method:: contains(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element contains the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that contain substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.startswith`, :obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:attribute:: dtype


   .. py:method:: endswith(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element ends with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that end with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.startswith`, :obj:`Categorical.contains`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether Categoricals are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the Categoricals are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> c = Categorical(ak.array(["a", "b", "c"]))
      >>> c_cpy = Categorical(ak.array(["a", "b", "c"]))
      >>> c.equals(c_cpy)
      True
      >>> c2 = Categorical(ak.array(["a", "x", "c"]))
      >>> c.equals(c2)
      False



   .. py:method:: from_codes(codes: arkouda.pdarrayclass.pdarray, categories: arkouda.strings.Strings, permutation=None, segments=None, **kwargs) -> Categorical
      :classmethod:


      Make a Categorical from codes and categories arrays. If codes and
      categories have already been pre-computed, this constructor saves
      time. If not, please use the normal constructor.

      :param codes: Category indices of each value
      :type codes: pdarray, int64
      :param categories: Unique category labels
      :type categories: Strings
      :param permutation: The permutation that groups the values in the same order
                          as categories
      :type permutation: pdarray, int64
      :param segments: When values are grouped, the starting offset of each group
      :type segments: pdarray, int64

      :returns: The Categorical object created from the input parameters
      :rtype: Categorical

      :raises TypeError: Raised if codes is not a pdarray of int64 objects or if
          categories is not a Strings object



   .. py:method:: from_return_msg(rep_msg) -> Categorical
      :classmethod:


      Create categorical from return message from server

      .. rubric:: Notes

      This is currently only used when reading a Categorical from HDF5 files.



   .. py:method:: group() -> arkouda.pdarrayclass.pdarray

      Return the permutation that groups the array, placing equivalent
      categories together. All instances of the same category are guaranteed
      to lie in one contiguous block of the permuted array, but the blocks
      are not necessarily ordered.

      :returns: The permutation that groups the array by value
      :rtype: pdarray

      .. seealso:: :obj:`GroupBy`, :obj:`unique`

      .. rubric:: Notes

      This method is faster than the corresponding Strings method. If the
      Categorical was created from a Strings object, then this function
      simply returns the cached permutation. Even if the Categorical was
      created using from_codes(), this function will be faster than
      Strings.group() because it sorts dense integer values, rather than
      128-bit hash values.



   .. py:method:: hash() -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

      Compute a 128-bit hash of each element of the Categorical.

      :returns: A tuple of two int64 pdarrays. The ith hash value is the concatenation
                of the ith values from each array.
      :rtype: Tuple[pdarray,pdarray]

      .. rubric:: Notes

      The implementation uses SipHash128, a fast and balanced hash function (used
      by Python for dictionaries and sets). For realistic numbers of strings (up
      to about 10**15), the probability of a collision between two 128-bit hash
      values is negligible.



   .. py:method:: in1d(test: Union[arkouda.strings.Strings, Categorical]) -> arkouda.pdarrayclass.pdarray

      Test whether each element of the Categorical object is
      also present in the test Strings or Categorical object.

      Returns a boolean array the same length as `self` that is True
      where an element of `self` is in `test` and False otherwise.

      :param test: The values against which to test each value of 'self`.
      :type test: Union[Strings,Categorical]

      :returns: The values `self[in1d]` are in the `test` Strings or Categorical object.
      :rtype: pdarray, bool

      :raises TypeError: Raised if test is not a Strings or Categorical object

      .. seealso:: :obj:`unique`, :obj:`intersect1d`, :obj:`union1d`

      .. rubric:: Notes

      `in1d` can be considered as an element-wise function version of the
      python keyword `in`, for 1-D sequences. ``in1d(a, b)`` is logically
      equivalent to ``ak.array([item in b for item in a])``, but is much
      faster and scales to arbitrarily large ``a``.

      .. rubric:: Examples

      >>> strings = ak.array([f'String {i}' for i in range(0,5)])
      >>> cat = ak.Categorical(strings)
      >>> ak.in1d(cat,strings)
      array([True True True True True])
      >>> strings = ak.array([f'String {i}' for i in range(5,9)])
      >>> catTwo = ak.Categorical(strings)
      >>> ak.in1d(cat,catTwo)
      array([False False False False False])



   .. py:property:: inferred_type
      :type: str


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_categorical_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isna()

      Find where values are missing or null (as defined by self.NAvalue)



   .. py:attribute:: logger


   .. py:property:: nbytes

      The size of the Categorical in bytes.

      :returns: The size of the Categorical in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: nlevels


   .. py:attribute:: objType
      :value: 'Categorical'



   .. py:method:: parse_hdf_categoricals(d: Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]]) -> Tuple[List[str], Dict[str, Categorical]]
      :staticmethod:


      This function should be used in conjunction with the load_all function which reads hdf5 files
      and reconstitutes Categorical objects.
      Categorical objects use a naming convention and HDF5 structure so they can be identified and
      constructed for the user.

      In general you should not call this method directly

      :param d:
      :type d: Dictionary of String to either Pdarray or Strings object

      :returns: * *2-Tuple of List of strings containing key names which should be removed and Dictionary of*
                * *base name to Categorical object*

      .. seealso:: :obj:`Categorical.save`, :obj:`load_all`



   .. py:attribute:: permutation
      :type:  Union[arkouda.pdarrayclass.pdarray, None]
      :value: None



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: register(user_defined_name: str) -> Categorical

      Register this Categorical object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Categorical is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Categorical which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Categoricals with the same name.
      :rtype: Categorical

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Categorical with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_categorical_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reset_categories() -> Categorical

      Recompute the category labels, discarding any unused labels. This
      method is often useful after slicing or indexing a Categorical array,
      when the resulting array only contains a subset of the original
      categories. In this case, eliminating unused categories can speed up
      other operations.

      :returns: A Categorical object generated from the current instance
      :rtype: Categorical



   .. py:method:: save(prefix_path: str, dataset: str = 'categorical_array', file_format: str = 'HDF5', mode: str = 'truncate', file_type: str = 'distribute', compression: Optional[str] = None) -> str

      DEPRECATED
      Save the Categorical object to HDF5 or Parquet. The result is a collection of HDF5/Parquet files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path and dataset. Each locale saves its chunk of the Strings array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in HDF5 files (must not already exist)
      :type dataset: str
      :param file_format: The format to save the file to.
      :type file_format: str {'HDF5 | 'Parquet'}
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Categorical dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")
      :param compression: {None | 'snappy' | 'gzip' | 'brotli' | 'zstd' | 'lz4'}
                          The compression type to use when writing.
                          This is only supported for Parquet files and will not be used with HDF5.
      :type compression: str (Optional)

      :rtype: String message indicating result of save operation

      :raises ValueError: Raised if the lengths of columns and values differ, or the mode is
          neither 'truncate' nor 'append'
      :raises TypeError: Raised if prefix_path, dataset, or mode is not a str

      .. rubric:: Notes

      Important implementation notes: (1) Strings state is saved as two datasets
      within an hdf5 group: one for the string characters and one for the
      segments corresponding to the start of each string, (2) the hdf5 group is named
      via the dataset parameter.

      .. seealso:: :obj:`-`, :obj:`-`



   .. py:attribute:: segments
      :value: None



   .. py:method:: set_categories(new_categories, NAvalue=None)

      Set categories to user-defined values.

      :param new_categories: The array of new categories to use. Must be unique.
      :type new_categories: Strings
      :param NAvalue: The value to use to represent missing/null data
      :type NAvalue: str scalar

      :returns: A new Categorical with the user-defined categories. Old values present
                in new categories will appear unchanged. Old values not present will
                be assigned the NA value.
      :rtype: Categorical



   .. py:attribute:: shape


   .. py:attribute:: size
      :type:  arkouda.numpy.dtypes.int_scalars


   .. py:method:: sort_values()


   .. py:method:: standardize_categories(arrays, NAvalue='N/A')
      :classmethod:


      Standardize an array of Categoricals so that they share the same categories.

      :param arrays: The Categoricals to standardize
      :type arrays: sequence of Categoricals
      :param NAvalue: The value to use to represent missing/null data
      :type NAvalue: str scalar

      :returns: A list of the original Categoricals remapped to the shared categories.
      :rtype: List of Categoricals



   .. py:method:: startswith(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element starts with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that start with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.contains`, :obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding
      method on Strings objects, because it searches the unique category
      labels instead of the full array.



   .. py:method:: to_hdf(prefix_path, dataset='categorical_array', mode='truncate', file_type='distribute')

      Save the Categorical to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
      :type file_type: str ("single" | "distribute")

      :rtype: None

      .. seealso:: :obj:`load`



   .. py:method:: to_list() -> List

      Convert the Categorical to a list, transferring data from
      the arkouda server to Python. This conversion discards category
      information and produces a list of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A list of strings corresponding to the values in
                this Categorical
      :rtype: list

      .. rubric:: Notes

      The number of bytes in the Categorical cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from
      the arkouda server to Python. This conversion discards category
      information and produces an ndarray of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A numpy ndarray of strings corresponding to the values in
                this array
      :rtype: np.ndarray

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_pandas() -> pandas.Categorical

      Return the equivalent Pandas Categorical.



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'categorical_array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      This functionality is currently not supported and will also raise a RuntimeError.
      Support is in development.
      Save the Categorical to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in HDF5 files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Categorical dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: Default None
                          Provide the compression type to use when writing the file.
                          Supported values: snappy, gzip, brotli, zstd, lz4
      :type compression: str (Optional)

      :rtype: String message indicating result of save operation

      :raises RuntimeError: On run due to compatability issues of Categorical with Parquet.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. seealso:: :obj:`to_hdf`



   .. py:method:: to_strings() -> List

      Convert the Categorical to Strings.

      :returns: A Strings object corresponding to the values in
                this Categorical.
      :rtype: arkouda.strings.Strings

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array(["a","b","c"])
      >>> a
      array(['a', 'b', 'c'])
      >>> c = ak.Categorical(a)
      >>> c.to_strings()
      array(['a', 'b', 'c'])

      >>> isinstance(c.to_strings(), ak.Strings)
      True



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a Categorical object to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the Categorical is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unique() -> Categorical


   .. py:method:: unregister() -> None

      Unregister this Categorical object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_categorical_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_categorical_by_name(user_defined_name: str) -> None
      :staticmethod:


      Function to unregister Categorical object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the Categorical object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path, dataset='categorical_array', repack=True)

      Overwrite the dataset with the name provided with this Categorical object. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the Categorical

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, the repack option allows for
        automatic creation of a file without the inaccessible data.



.. py:class:: Categorical(values, **kwargs)

   Represents an array of values belonging to named categories. Converting a
   Strings object to Categorical often saves memory and speeds up operations,
   especially if there are many repeated values, at the cost of some one-time
   work in initialization.

   :param values: Values to convert to categories
   :type values: Strings, Categorical, pd.Categorical
   :param NAvalue: The value to use to represent missing/null data
   :type NAvalue: str scalar

   .. attribute:: categories

      The set of category labels (determined automatically)

      :type: Strings

   .. attribute:: codes

      The category indices of the values or -1 for N/A

      :type: pdarray, int64

   .. attribute:: permutation

      The permutation that groups the values in the same order as categories

      :type: pdarray, int64

   .. attribute:: segments

      When values are grouped, the starting offset of each group

      :type: pdarray, int64

   .. attribute:: size

      The number of items in the array

      :type: Union[int,np.int64]

   .. attribute:: nlevels

      The number of distinct categories

      :type: Union[int,np.int64]

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: Union[int,np.int64]

   .. attribute:: shape

      The sizes of each dimension of the array

      :type: tuple


   .. py:attribute:: BinOps


   .. py:attribute:: RegisterablePieces


   .. py:attribute:: RequiredPieces


   .. py:method:: argsort()


   .. py:method:: attach(user_defined_name: str) -> Categorical
      :staticmethod:


      DEPRECATED
      Function to return a Categorical object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which Categorical object was registered under
      :type user_defined_name: str

      :returns: The Categorical object created by re-attaching to the corresponding server components
      :rtype: Categorical

      :raises TypeError: if user_defined_name is not a string

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_categorical_by_name`



   .. py:method:: concatenate(others: Sequence[Categorical], ordered: bool = True) -> Categorical

      Merge this Categorical with other Categorical objects in the array,
      concatenating the arrays and synchronizing the categories.

      :param others: The Categorical arrays to concatenate and merge with this one
      :type others: Sequence[Categorical]
      :param ordered: If True (default), the arrays will be appended in the
                      order given. If False, array data may be interleaved
                      in blocks, which can greatly improve performance but
                      results in non-deterministic ordering of elements.
      :type ordered: bool

      :returns: The merged Categorical object
      :rtype: Categorical

      :raises TypeError: Raised if any others array objects are not Categorical objects

      .. rubric:: Notes

      This operation can be expensive -- slower than concatenating Strings.



   .. py:method:: contains(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element contains the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that contain substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.startswith`, :obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:attribute:: dtype


   .. py:method:: endswith(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element ends with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that end with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.startswith`, :obj:`Categorical.contains`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether Categoricals are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the Categoricals are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> c = Categorical(ak.array(["a", "b", "c"]))
      >>> c_cpy = Categorical(ak.array(["a", "b", "c"]))
      >>> c.equals(c_cpy)
      True
      >>> c2 = Categorical(ak.array(["a", "x", "c"]))
      >>> c.equals(c2)
      False



   .. py:method:: from_codes(codes: arkouda.pdarrayclass.pdarray, categories: arkouda.strings.Strings, permutation=None, segments=None, **kwargs) -> Categorical
      :classmethod:


      Make a Categorical from codes and categories arrays. If codes and
      categories have already been pre-computed, this constructor saves
      time. If not, please use the normal constructor.

      :param codes: Category indices of each value
      :type codes: pdarray, int64
      :param categories: Unique category labels
      :type categories: Strings
      :param permutation: The permutation that groups the values in the same order
                          as categories
      :type permutation: pdarray, int64
      :param segments: When values are grouped, the starting offset of each group
      :type segments: pdarray, int64

      :returns: The Categorical object created from the input parameters
      :rtype: Categorical

      :raises TypeError: Raised if codes is not a pdarray of int64 objects or if
          categories is not a Strings object



   .. py:method:: from_return_msg(rep_msg) -> Categorical
      :classmethod:


      Create categorical from return message from server

      .. rubric:: Notes

      This is currently only used when reading a Categorical from HDF5 files.



   .. py:method:: group() -> arkouda.pdarrayclass.pdarray

      Return the permutation that groups the array, placing equivalent
      categories together. All instances of the same category are guaranteed
      to lie in one contiguous block of the permuted array, but the blocks
      are not necessarily ordered.

      :returns: The permutation that groups the array by value
      :rtype: pdarray

      .. seealso:: :obj:`GroupBy`, :obj:`unique`

      .. rubric:: Notes

      This method is faster than the corresponding Strings method. If the
      Categorical was created from a Strings object, then this function
      simply returns the cached permutation. Even if the Categorical was
      created using from_codes(), this function will be faster than
      Strings.group() because it sorts dense integer values, rather than
      128-bit hash values.



   .. py:method:: hash() -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

      Compute a 128-bit hash of each element of the Categorical.

      :returns: A tuple of two int64 pdarrays. The ith hash value is the concatenation
                of the ith values from each array.
      :rtype: Tuple[pdarray,pdarray]

      .. rubric:: Notes

      The implementation uses SipHash128, a fast and balanced hash function (used
      by Python for dictionaries and sets). For realistic numbers of strings (up
      to about 10**15), the probability of a collision between two 128-bit hash
      values is negligible.



   .. py:method:: in1d(test: Union[arkouda.strings.Strings, Categorical]) -> arkouda.pdarrayclass.pdarray

      Test whether each element of the Categorical object is
      also present in the test Strings or Categorical object.

      Returns a boolean array the same length as `self` that is True
      where an element of `self` is in `test` and False otherwise.

      :param test: The values against which to test each value of 'self`.
      :type test: Union[Strings,Categorical]

      :returns: The values `self[in1d]` are in the `test` Strings or Categorical object.
      :rtype: pdarray, bool

      :raises TypeError: Raised if test is not a Strings or Categorical object

      .. seealso:: :obj:`unique`, :obj:`intersect1d`, :obj:`union1d`

      .. rubric:: Notes

      `in1d` can be considered as an element-wise function version of the
      python keyword `in`, for 1-D sequences. ``in1d(a, b)`` is logically
      equivalent to ``ak.array([item in b for item in a])``, but is much
      faster and scales to arbitrarily large ``a``.

      .. rubric:: Examples

      >>> strings = ak.array([f'String {i}' for i in range(0,5)])
      >>> cat = ak.Categorical(strings)
      >>> ak.in1d(cat,strings)
      array([True True True True True])
      >>> strings = ak.array([f'String {i}' for i in range(5,9)])
      >>> catTwo = ak.Categorical(strings)
      >>> ak.in1d(cat,catTwo)
      array([False False False False False])



   .. py:property:: inferred_type
      :type: str


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_categorical_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isna()

      Find where values are missing or null (as defined by self.NAvalue)



   .. py:attribute:: logger


   .. py:property:: nbytes

      The size of the Categorical in bytes.

      :returns: The size of the Categorical in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: nlevels


   .. py:attribute:: objType
      :value: 'Categorical'



   .. py:method:: parse_hdf_categoricals(d: Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]]) -> Tuple[List[str], Dict[str, Categorical]]
      :staticmethod:


      This function should be used in conjunction with the load_all function which reads hdf5 files
      and reconstitutes Categorical objects.
      Categorical objects use a naming convention and HDF5 structure so they can be identified and
      constructed for the user.

      In general you should not call this method directly

      :param d:
      :type d: Dictionary of String to either Pdarray or Strings object

      :returns: * *2-Tuple of List of strings containing key names which should be removed and Dictionary of*
                * *base name to Categorical object*

      .. seealso:: :obj:`Categorical.save`, :obj:`load_all`



   .. py:attribute:: permutation
      :type:  Union[arkouda.pdarrayclass.pdarray, None]
      :value: None



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: register(user_defined_name: str) -> Categorical

      Register this Categorical object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Categorical is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Categorical which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Categoricals with the same name.
      :rtype: Categorical

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Categorical with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_categorical_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reset_categories() -> Categorical

      Recompute the category labels, discarding any unused labels. This
      method is often useful after slicing or indexing a Categorical array,
      when the resulting array only contains a subset of the original
      categories. In this case, eliminating unused categories can speed up
      other operations.

      :returns: A Categorical object generated from the current instance
      :rtype: Categorical



   .. py:method:: save(prefix_path: str, dataset: str = 'categorical_array', file_format: str = 'HDF5', mode: str = 'truncate', file_type: str = 'distribute', compression: Optional[str] = None) -> str

      DEPRECATED
      Save the Categorical object to HDF5 or Parquet. The result is a collection of HDF5/Parquet files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path and dataset. Each locale saves its chunk of the Strings array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in HDF5 files (must not already exist)
      :type dataset: str
      :param file_format: The format to save the file to.
      :type file_format: str {'HDF5 | 'Parquet'}
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Categorical dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")
      :param compression: {None | 'snappy' | 'gzip' | 'brotli' | 'zstd' | 'lz4'}
                          The compression type to use when writing.
                          This is only supported for Parquet files and will not be used with HDF5.
      :type compression: str (Optional)

      :rtype: String message indicating result of save operation

      :raises ValueError: Raised if the lengths of columns and values differ, or the mode is
          neither 'truncate' nor 'append'
      :raises TypeError: Raised if prefix_path, dataset, or mode is not a str

      .. rubric:: Notes

      Important implementation notes: (1) Strings state is saved as two datasets
      within an hdf5 group: one for the string characters and one for the
      segments corresponding to the start of each string, (2) the hdf5 group is named
      via the dataset parameter.

      .. seealso:: :obj:`-`, :obj:`-`



   .. py:attribute:: segments
      :value: None



   .. py:method:: set_categories(new_categories, NAvalue=None)

      Set categories to user-defined values.

      :param new_categories: The array of new categories to use. Must be unique.
      :type new_categories: Strings
      :param NAvalue: The value to use to represent missing/null data
      :type NAvalue: str scalar

      :returns: A new Categorical with the user-defined categories. Old values present
                in new categories will appear unchanged. Old values not present will
                be assigned the NA value.
      :rtype: Categorical



   .. py:attribute:: shape


   .. py:attribute:: size
      :type:  arkouda.numpy.dtypes.int_scalars


   .. py:method:: sort_values()


   .. py:method:: standardize_categories(arrays, NAvalue='N/A')
      :classmethod:


      Standardize an array of Categoricals so that they share the same categories.

      :param arrays: The Categoricals to standardize
      :type arrays: sequence of Categoricals
      :param NAvalue: The value to use to represent missing/null data
      :type NAvalue: str scalar

      :returns: A list of the original Categoricals remapped to the shared categories.
      :rtype: List of Categoricals



   .. py:method:: startswith(substr: Union[bytes, arkouda.numpy.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element starts with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that start with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.contains`, :obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding
      method on Strings objects, because it searches the unique category
      labels instead of the full array.



   .. py:method:: to_hdf(prefix_path, dataset='categorical_array', mode='truncate', file_type='distribute')

      Save the Categorical to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
      :type file_type: str ("single" | "distribute")

      :rtype: None

      .. seealso:: :obj:`load`



   .. py:method:: to_list() -> List

      Convert the Categorical to a list, transferring data from
      the arkouda server to Python. This conversion discards category
      information and produces a list of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A list of strings corresponding to the values in
                this Categorical
      :rtype: list

      .. rubric:: Notes

      The number of bytes in the Categorical cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from
      the arkouda server to Python. This conversion discards category
      information and produces an ndarray of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A numpy ndarray of strings corresponding to the values in
                this array
      :rtype: np.ndarray

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_pandas() -> pandas.Categorical

      Return the equivalent Pandas Categorical.



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'categorical_array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      This functionality is currently not supported and will also raise a RuntimeError.
      Support is in development.
      Save the Categorical to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in HDF5 files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Categorical dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: Default None
                          Provide the compression type to use when writing the file.
                          Supported values: snappy, gzip, brotli, zstd, lz4
      :type compression: str (Optional)

      :rtype: String message indicating result of save operation

      :raises RuntimeError: On run due to compatability issues of Categorical with Parquet.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. seealso:: :obj:`to_hdf`



   .. py:method:: to_strings() -> List

      Convert the Categorical to Strings.

      :returns: A Strings object corresponding to the values in
                this Categorical.
      :rtype: arkouda.strings.Strings

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array(["a","b","c"])
      >>> a
      array(['a', 'b', 'c'])
      >>> c = ak.Categorical(a)
      >>> c.to_strings()
      array(['a', 'b', 'c'])

      >>> isinstance(c.to_strings(), ak.Strings)
      True



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a Categorical object to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the Categorical is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unique() -> Categorical


   .. py:method:: unregister() -> None

      Unregister this Categorical object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_categorical_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_categorical_by_name(user_defined_name: str) -> None
      :staticmethod:


      Function to unregister Categorical object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the Categorical object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path, dataset='categorical_array', repack=True)

      Overwrite the dataset with the name provided with this Categorical object. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the Categorical

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, the repack option allows for
        automatic creation of a file without the inaccessible data.



.. py:class:: Complex128DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: Complex64DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: DType

   An enumeration.



   .. py:method:: BIGINT(*args, **kwargs)

      An enumeration.




   .. py:method:: BOOL(*args, **kwargs)

      An enumeration.




   .. py:method:: COMPLEX128(*args, **kwargs)

      An enumeration.




   .. py:method:: COMPLEX64(*args, **kwargs)

      An enumeration.




   .. py:method:: FLOAT(*args, **kwargs)

      An enumeration.




   .. py:method:: FLOAT32(*args, **kwargs)

      An enumeration.




   .. py:method:: FLOAT64(*args, **kwargs)

      An enumeration.




   .. py:method:: INT(*args, **kwargs)

      An enumeration.




   .. py:method:: INT16(*args, **kwargs)

      An enumeration.




   .. py:method:: INT32(*args, **kwargs)

      An enumeration.




   .. py:method:: INT64(*args, **kwargs)

      An enumeration.




   .. py:method:: INT8(*args, **kwargs)

      An enumeration.




   .. py:method:: STR(*args, **kwargs)

      An enumeration.




   .. py:method:: UINT(*args, **kwargs)

      An enumeration.




   .. py:method:: UINT16(*args, **kwargs)

      An enumeration.




   .. py:method:: UINT32(*args, **kwargs)

      An enumeration.




   .. py:method:: UINT64(*args, **kwargs)

      An enumeration.




   .. py:method:: UINT8(*args, **kwargs)

      An enumeration.




   .. py:method:: name(*args, **kwargs)

      The name of the Enum member.




   .. py:method:: value(*args, **kwargs)

      The value of the Enum member.




.. py:class:: DTypeObjects

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: DTypes

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: DataFrame(dict=None, /, **kwargs)

   Bases: :py:obj:`collections.UserDict`


   A DataFrame structure based on arkouda arrays.

   :param initialdata: Each list/dictionary entry corresponds to one column of the data and
                       should be a homogenous type. Different columns may have different
                       types. If using a dictionary, keys should be strings.
   :type initialdata: List or dictionary of lists, tuples, or pdarrays
   :param index: Index for the resulting frame. Defaults to an integer range.
   :type index: Index, pdarray, or Strings
   :param columns: Column labels to use if the data does not include them. Elements must
                   be strings. Defaults to an stringified integer range.
   :type columns: List, tuple, pdarray, or Strings

   .. rubric:: Examples

   Create an empty DataFrame and add a column of data:

   >>> import arkouda as ak
   >>> ak.connect()
   >>> df = ak.DataFrame()
   >>> df['a'] = ak.array([1,2,3])
   >>> display(df)

   +----+-----+
   |    |   a |
   +====+=====+
   |  0 |   1 |
   +----+-----+
   |  1 |   2 |
   +----+-----+
   |  2 |   3 |
   +----+-----+

   Create a new DataFrame using a dictionary of data:

   >>> userName = ak.array(['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice'])
   >>> userID = ak.array([111, 222, 111, 333, 222, 111])
   >>> item = ak.array([0, 0, 1, 1, 2, 0])
   >>> day = ak.array([5, 5, 6, 5, 6, 6])
   >>> amount = ak.array([0.5, 0.6, 1.1, 1.2, 4.3, 0.6])
   >>> df = ak.DataFrame({'userName': userName, 'userID': userID,
   >>>            'item': item, 'day': day, 'amount': amount})
   >>> display(df)

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Alice      |      111 |      0 |     5 |      0.5 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  3 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  4 | Bob        |      222 |      2 |     6 |      4.3 |
   +----+------------+----------+--------+-------+----------+
   |  5 | Alice      |      111 |      0 |     6 |      0.6 |
   +----+------------+----------+--------+-------+----------+

   Indexing works slightly differently than with pandas:

   >>> df[0]

   +------------+----------+
   | keys       |   values |
   +============+==========+
   | userName   |    Alice |
   +------------+----------+
   |userID      |      111 |
   +------------+----------+
   | item       |      0   |
   +------------+----------+
   | day        |      5   |
   +------------+----------+
   | amount     |     0.5  |
   +------------+----------+

   >>> df['userID']
   array([111, 222, 111, 333, 222, 111])

   >>> df['userName']
   array(['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice'])

   >>> df[ak.array([1,3,5])]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Alice      |      111 |      0 |     6 |      0.6 |
   +----+------------+----------+--------+-------+----------+

   Compute the stride:

   >>> df[1:5:1]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  3 | Bob        |      222 |      2 |     6 |      4.3 |
   +----+------------+----------+--------+-------+----------+

   >>> df[ak.array([1,2,3])]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+

   >>> df[['userID', 'day']]

   +----+----------+-------+
   |    |   userID |   day |
   +====+==========+=======+
   |  0 |      111 |     5 |
   +----+----------+-------+
   |  1 |      222 |     5 |
   +----+----------+-------+
   |  2 |      111 |     6 |
   +----+----------+-------+
   |  3 |      333 |     5 |
   +----+----------+-------+
   |  4 |      222 |     6 |
   +----+----------+-------+
   |  5 |      111 |     6 |
   +----+----------+-------+


   .. py:method:: GroupBy(keys, use_series=False, as_index=True, dropna=True)

      Group the dataframe by a column or a list of columns.

      :param keys: An (ordered) list of column names or a single string to group by.
      :type keys: str or list of str
      :param use_series: If True, returns an arkouda.dataframe.DataFrameGroupBy object.
                         Otherwise an arkouda.groupbyclass.GroupBy object.
      :type use_series: bool, default=False
      :param as_index: If True, groupby columns will be set as index
                       otherwise, the groupby columns will be treated as DataFrame columns.
      :type as_index: bool, default=True
      :param dropna: If True, and the groupby keys contain NaN values,
                     the NaN values together with the corresponding row will be dropped.
                     Otherwise, the rows corresponding to NaN values will be kept.
      :type dropna: bool, default=True

      :returns: If use_series = True, returns an arkouda.dataframe.DataFrameGroupBy object.
                Otherwise returns an arkouda.groupbyclass.GroupBy object.
      :rtype: arkouda.dataframe.DataFrameGroupBy or arkouda.groupbyclass.GroupBy

      .. seealso:: :obj:`arkouda.GroupBy`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.0, 1.0, 2.0, np.nan], 'col2': [4, 5, 6, 7]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      1 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |    nan |      7 |
      +----+--------+--------+

      >>> df.GroupBy("col1")
      <arkouda.groupbyclass.GroupBy at 0x7f2cf23e10c0>
      >>> df.GroupBy("col1").size()
      (array([1.00000000000000000 2.00000000000000000]), array([2 1]))

      >>> df.GroupBy("col1",use_series=True)
      col1
      1.0    2
      2.0    1
      dtype: int64
      >>> df.GroupBy("col1",use_series=True, as_index = False).size()

      +----+--------+--------+
      |    |   col1 |   size |
      +====+========+========+
      |  0 |      1 |      2 |
      +----+--------+--------+
      |  1 |      2 |      1 |
      +----+--------+--------+



   .. py:method:: all(axis=0) -> Union[Series, bool]

      Return whether all elements are True, potentially over an axis.

      Returns True unless there at least one element along a Dataframe axis that is False.

      Currently, will ignore any columns that are not type bool.
      This is equivalent to the pandas option bool_only=True.

      :param axis: Indicate which axis or axes should be reduced.

                   0 / index : reduce the index, return a Series whose index is the original column labels.

                   1 / columns : reduce the columns, return a Series whose index is the original index.

                   None : reduce all axes, return a scalar.
      :type axis: {0 or index, 1 or columns, None}, default = 0

      :rtype: arkouda.series.Series or bool

      :raises ValueError: Raised if axis does not have a value in {0 or index, 1 or columns, None}.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[True,True,True,False],"B":[True,True,True,False],
      ...          "C":[True,False,True,False],"D":[True,True,True,True]})

      +----+---------+---------+---------+--------+
      |    |   A     |   B     |   C     |   D    |
      +====+=========+=========+=========+========+
      |  0 |   True  |   True  |   True  |   True |
      +----+---------+---------+---------+--------+
      |  1 |   True  |   True  |   False |   True |
      +----+---------+---------+---------+--------+
      |  2 |   True  |   True  |   True  |   True |
      +----+---------+---------+---------+--------+
      |  3 |   False |   False |   False |   True |
      +----+---------+---------+---------+--------+

      >>> df.all(axis=0)
      A    False
      B    False
      C    False
      D     True
      dtype: bool
      >>> df.all(axis=1)
      0     True
      1    False
      2     True
      3    False
      dtype: bool
      >>> df.all(axis=None)
      False



   .. py:method:: any(axis=0) -> Union[Series, bool]

      Return whether any element is True, potentially over an axis.

      Returns False unless there is at least one element along a Dataframe axis that is True.

      Currently, will ignore any columns that are not type bool.
      This is equivalent to the pandas option bool_only=True.

      :param axis: Indicate which axis or axes should be reduced.

                   0 / index : reduce the index, return a Series whose index is the original column labels.

                   1 / columns : reduce the columns, return a Series whose index is the original index.

                   None : reduce all axes, return a scalar.
      :type axis: {0 or index, 1 or columns, None}, default = 0

      :rtype: arkouda.series.Series or bool

      :raises ValueError: Raised if axis does not have a value in {0 or index, 1 or columns, None}.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[True,True,True,False],"B":[True,True,True,False],
      ...          "C":[True,False,True,False],"D":[False,False,False,False]})

      +----+---------+---------+---------+---------+
      |    |   A     |   B     |   C     |   D     |
      +====+=========+=========+=========+=========+
      |  0 |   True  |   True  |   True  |   False |
      +----+---------+---------+---------+---------+
      |  1 |   True  |   True  |   False |   False |
      +----+---------+---------+---------+---------+
      |  2 |   True  |   True  |   True  |   False |
      +----+---------+---------+---------+---------+
      |  3 |   False |   False |   False |   False |
      +----+---------+---------+---------+---------+

      >>> df.any(axis=0)
      A     True
      B     True
      C     True
      D    False
      dtype: bool
      >>> df.any(axis=1)
      0     True
      1     True
      2     True
      3    False
      dtype: bool
      >>> df.any(axis=None)
      True



   .. py:method:: append(other, ordered=True)

      Concatenate data from 'other' onto the end of this DataFrame, in place.

      Explicitly, use the arkouda concatenate function to append the data
      from each column in other to the end of self. This operation is done
      in place, in the sense that the underlying pdarrays are updated from
      the result of the arkouda concatenate function, rather than returning
      a new DataFrame object containing the result.

      :param other: The DataFrame object whose data will be appended to this DataFrame.
      :type other: DataFrame
      :param ordered: If False, allow rows to be interleaved for better performance (but
                      data within a row remains together). By default, append all rows
                      to the end, in input order.
      :type ordered: bool, default=True

      :returns: Appending occurs in-place, but result is returned for compatibility.
      :rtype: self

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df1 = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df2 = ak.DataFrame({'col1': [3], 'col2': [5]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      3 |      5 |
      +----+--------+--------+

      >>> df1.append(df2)
      >>> df1

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+
      |  2 |      3 |      5 |
      +----+--------+--------+



   .. py:method:: apply_permutation(perm)

      Apply a permutation to an entire DataFrame.  The operation is done in
      place and the original DataFrame will be modified.

      This may be useful if you want to unsort an DataFrame, or even to
      apply an arbitrary permutation such as the inverse of a sorting
      permutation.

      :param perm: A permutation array. Should be the same size as the data
                   arrays, and should consist of the integers [0,size-1] in
                   some order. Very minimal testing is done to ensure this
                   is a permutation.
      :type perm: pdarray

      :rtype: None

      .. seealso:: :obj:`sort`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> perm_arry = ak.array([0, 2, 1])
      >>> df.apply_permutation(perm_arry)
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      3 |      6 |
      +----+--------+--------+
      |  2 |      2 |      5 |
      +----+--------+--------+



   .. py:method:: argsort(key, ascending=True)

      Return the permutation that sorts the dataframe by `key`.

      :param key: The key to sort on.
      :type key: str
      :param ascending: If true, sort the key in ascending order.
                        Otherwise, sort the key in descending order.
      :type ascending: bool, default = True

      :returns: The permutation array that sorts the data on `key`.
      :rtype: arkouda.pdarrayclass.pdarray

      .. seealso:: :obj:`coargsort`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.1, 3.1, 2.1], 'col2': [6, 5, 4]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    2.1 |      4 |
      +----+--------+--------+

      >>> df.argsort('col1')
      array([0 2 1])
      >>> sorted_df1 = df[df.argsort('col1')]
      >>> display(sorted_df1)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    2.1 |      4 |
      +----+--------+--------+
      |  2 |    3.1 |      5 |
      +----+--------+--------+

      >>> df.argsort('col2')
      array([2 1 0])
      >>> sorted_df2 = df[df.argsort('col2')]
      >>> display(sorted_df2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    2.1 |      4 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    1.1 |      6 |
      +----+--------+--------+



   .. py:method:: assign(**kwargs) -> DataFrame

      Assign new columns to a DataFrame.

      Returns a new object with all original columns in addition to new ones.
      Existing columns that are re-assigned will be overwritten.

      :param \*\*kwargs: The column names are keywords. If the values are
                         callable, they are computed on the DataFrame and
                         assigned to the new columns. The callable must not
                         change input DataFrame (though pandas doesn't check it).
                         If the values are not callable, (e.g. a Series, scalar, or array),
                         they are simply assigned.
      :type \*\*kwargs: dict of {str: callable or Series}

      :returns: A new DataFrame with the new columns in addition to
                all the existing columns.
      :rtype: DataFrame

      .. rubric:: Notes

      Assigning multiple columns within the same ``assign`` is possible.
      Later items in '\*\*kwargs' may refer to newly created or modified
      columns in 'df'; items are computed and assigned into 'df' in order.

      .. rubric:: Examples

      >>> df = ak.DataFrame({'temp_c': [17.0, 25.0]},
      ...                   index=['Portland', 'Berkeley'])
      >>> df
                temp_c
      Portland    17.0
      Berkeley    25.0

      Where the value is a callable, evaluated on `df`:

      >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)
                temp_c  temp_f
      Portland    17.0    62.6
      Berkeley    25.0    77.0

      Alternatively, the same behavior can be achieved by directly
      referencing an existing Series or sequence:

      >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)
                temp_c  temp_f
      Portland    17.0    62.6
      Berkeley    25.0    77.0

      You can create multiple columns within the same assign where one
      of the columns depends on another one defined within the same assign:

      >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,
      ...           temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9)
                temp_c  temp_f  temp_k
      Portland    17.0    62.6  290.15
      Berkeley    25.0    77.0  298.15



   .. py:method:: attach(user_defined_name: str) -> DataFrame

      Function to return a DataFrame object attached to the registered name in the
      arkouda server which was registered using register().

      :param user_defined_name: user defined name which DataFrame object was registered under.
      :type user_defined_name: str

      :returns: The DataFrame object created by re-attaching to the corresponding server components.
      :rtype: arkouda.dataframe.DataFrame

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: coargsort(keys, ascending=True)

      Return the permutation that sorts the dataframe by `keys`.

      Note: Sorting using Strings may not yield correct sort order.

      :param keys: The keys to sort on.
      :type keys: list of str

      :returns: The permutation array that sorts the data on `keys`.
      :rtype: arkouda.pdarrayclass.pdarray

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [2, 2, 1], 'col2': [3, 4, 3], 'col3':[5, 6, 7]})
      >>> display(df)

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  1 |      2 |      4 |      6 |
      +----+--------+--------+--------+
      |  2 |      1 |      3 |      7 |
      +----+--------+--------+--------+

      >>> df.coargsort(['col1', 'col2'])
      array([2 0 1])
      >>>



   .. py:property:: columns

      An Index where the values are the column names of the dataframe.

      :returns: The values of the index are the column names of the dataframe.
      :rtype: arkouda.index.Index

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df.columns
      Index(array(['col1', 'col2']), dtype='<U0')


   .. py:method:: concat(items, ordered=True)

      Essentially an append, but different formatting.





   .. py:method:: corr() -> DataFrame

      Return new DataFrame with pairwise correlation of columns.

      :returns: Arkouda DataFrame containing correlation matrix of all columns.
      :rtype: arkouda.dataframe.DataFrame

      :raises RuntimeError: Raised if there's a server-side error thrown.

      .. seealso:: :obj:`pdarray.corr`

      .. rubric:: Notes

      Generates the correlation matrix using Pearson R for all columns.

      Attempts to convert to numeric values where possible for inclusion in the matrix.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [-1, -2]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |     -1 |
      +----+--------+--------+
      |  1 |      2 |     -2 |
      +----+--------+--------+

      >>> corr = df.corr()

      +------+--------+--------+
      |      |   col1 |   col2 |
      +======+========+========+
      | col1 |      1 |     -1 |
      +------+--------+--------+
      | col2 |     -1 |      1 |
      +------+--------+--------+



   .. py:method:: count(axis: Union[int, str] = 0, numeric_only=False) -> Series

      Count non-NA cells for each column or row.

      The values np.NaN are considered NA.

      :param axis: If 0 or index counts are generated for each column.
                   If 1 or columns counts are generated for each row.
      :type axis: {0 or 'index', 1 or 'columns'}, default 0
      :param numeric_only: Include only float, int or boolean data.
      :type numeric_only: bool = False

      :returns: For each column/row the number of non-NA/null entries.
      :rtype: arkouda.series.Series

      :raises ValueError: Raised if axis is not 0, 1, 'index', or 'columns'.

      .. seealso:: :obj:`GroupBy.count`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({'col_A': ak.array([7, np.nan]), 'col_B':ak.array([1, 9])})
      >>> display(df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       7 |       1 |
      +----+---------+---------+
      |  1 |     nan |       9 |
      +----+---------+---------+

      >>> df.count()
      col_A    1
      col_B    2
      dtype: int64

      >>> df = ak.DataFrame({'col_A': ak.array(["a","b","c"]), 'col_B':ak.array([1, np.nan, np.nan])})
      >>> display(df)

      +----+---------+---------+
      |    | col_A   |   col_B |
      +====+=========+=========+
      |  0 | a       |       1 |
      +----+---------+---------+
      |  1 | b       |     nan |
      +----+---------+---------+
      |  2 | c       |     nan |
      +----+---------+---------+

      >>> df.count()
      col_A    3
      col_B    1
      dtype: int64

      >>> df.count(numeric_only=True)
      col_B    1
      dtype: int64

      >>> df.count(axis=1)
      0    2
      1    1
      2    1
      dtype: int64



   .. py:method:: drop(keys: Union[str, int, List[Union[str, int]]], axis: Union[str, int] = 0, inplace: bool = False) -> Union[None, DataFrame]

      Drop column/s or row/s from the dataframe.

      :param keys: The labels to be dropped on the given axis.
      :type keys: str, int or list
      :param axis: The axis on which to drop from. 0/'index' - drop rows, 1/'columns' - drop columns.
      :type axis: int or str
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`
      :rtype: arkouda.dataframe.DataFrame or None

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      Drop column

      >>> df.drop('col1', axis = 1)

      +----+--------+
      |    |   col2 |
      +====+========+
      |  0 |      3 |
      +----+--------+
      |  1 |      4 |
      +----+--------+

      Drop row

      >>> df.drop(0, axis = 0)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      2 |      4 |
      +----+--------+--------+



   .. py:method:: drop_duplicates(subset=None, keep='first')

      Drops duplcated rows and returns resulting DataFrame.

      If a subset of the columns are provided then only one instance of each
      duplicated row will be returned (keep determines which row).

      :param subset: Iterable of column names to use to dedupe.
      :type subset: Iterable
      :param keep: Determines which duplicates (if any) to keep.
      :type keep: {'first', 'last'}, default='first'

      :returns: DataFrame with duplicates removed.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 2, 3], 'col2': [4, 5, 5, 6]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      2 |      5 |
      +----+--------+--------+
      |  3 |      3 |      6 |
      +----+--------+--------+

      >>> df.drop_duplicates()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+



   .. py:method:: dropna(axis: Union[int, str] = 0, how: Optional[str] = None, thresh: Optional[int] = None, ignore_index: bool = False) -> DataFrame

      Remove missing values.

      :param axis: Determine if rows or columns which contain missing values are removed.

                   0, or 'index': Drop rows which contain missing values.

                   1, or 'columns': Drop columns which contain missing value.

                   Only a single axis is allowed.
      :type axis: {0 or 'index', 1 or 'columns'}, default = 0
      :param how: Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.

                  'any': If any NA values are present, drop that row or column.

                  'all': If all values are NA, drop that row or column.
      :type how: {'any', 'all'}, default='any'
      :param thresh: Require that many non - NA values.Cannot be combined with how.
      :type thresh: int, optional
      :param ignore_index: If ``True``, the resulting axis will be labeled 0, 1, , n - 1.
      :type ignore_index: bool, default ``False``

      :returns: DataFrame with NA entries dropped from it.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame(
          {
              "A": [True, True, True, True],
              "B": [1, np.nan, 2, np.nan],
              "C": [1, 2, 3, np.nan],
              "D": [False, False, False, False],
              "E": [1, 2, 3, 4],
              "F": ["a", "b", "c", "d"],
              "G": [1, 2, 3, 4],
          }
         )

      >>> display(df)

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True | nan |   2 | False |   2 | b   |   2 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  2 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  3 | True | nan | nan | False |   4 | d   |   4 |
      +----+------+-----+-----+-------+-----+-----+-----+

      >>> df.dropna()

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+

      >>> df.dropna(axis=1)

      +----+------+-------+-----+-----+-----+
      |    | A    | D     |   E | F   |   G |
      +====+======+=======+=====+=====+=====+
      |  0 | True | False |   1 | a   |   1 |
      +----+------+-------+-----+-----+-----+
      |  1 | True | False |   2 | b   |   2 |
      +----+------+-------+-----+-----+-----+
      |  2 | True | False |   3 | c   |   3 |
      +----+------+-------+-----+-----+-----+
      |  3 | True | False |   4 | d   |   4 |
      +----+------+-------+-----+-----+-----+

      >>> df.dropna(axis=1, thresh=3)

      +----+------+-----+-------+-----+-----+-----+
      |    | A    |   C | D     |   E | F   |   G |
      +====+======+=====+=======+=====+=====+=====+
      |  0 | True |   1 | False |   1 | a   |   1 |
      +----+------+-----+-------+-----+-----+-----+
      |  1 | True |   2 | False |   2 | b   |   2 |
      +----+------+-----+-------+-----+-----+-----+
      |  2 | True |   3 | False |   3 | c   |   3 |
      +----+------+-----+-------+-----+-----+-----+
      |  3 | True | nan | False |   4 | d   |   4 |
      +----+------+-----+-------+-----+-----+-----+

      >>> df.dropna(axis=1, how="all")

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True | nan |   2 | False |   2 | b   |   2 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  2 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  3 | True | nan | nan | False |   4 | d   |   4 |
      +----+------+-----+-----+-------+-----+-----+-----+



   .. py:property:: dtypes
      :type: DataFrame


      The dtypes of the dataframe.

      :returns: **dtypes** -- The dtypes of the dataframe.
      :rtype: arkouda.row.Row

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': ["a", "b"]})
      >>> df

      +----+--------+--------+
      |    |   col1 | col2   |
      +====+========+========+
      |  0 |      1 | a      |
      +----+--------+--------+
      |  1 |      2 | b      |
      +----+--------+--------+

      >>> df.dtypes

      +----+--------+
      |keys| values |
      +====+========+
      |col1|  int64 |
      +----+--------+
      |col2|    str |
      +----+--------+


   .. py:property:: empty
      :type: DataFrame


      Whether the dataframe is empty.

      :returns: True if the dataframe is empty, otherwise False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({})
      >>> df
       0 rows x 0 columns
      >>> df.empty
      True


   .. py:method:: filter_by_range(keys, low=1, high=None)

      Find all rows where the value count of the items in a given set of
      columns (keys) is within the range [low, high].

      To filter by a specific value, set low == high.

      :param keys: The names of the columns to group by.
      :type keys: str or list of str
      :param low: The lowest value count.
      :type low: int, default=1
      :param high: The highest value count, default to unlimited.
      :type high: int, default=None

      :returns: An array of boolean values for qualified rows in this DataFrame.
      :rtype: arkouda.pdarrayclass.pdarray

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 2, 2, 3, 3], 'col2': [4, 5, 6, 7, 8, 9]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |      2 |      7 |
      +----+--------+--------+
      |  4 |      3 |      8 |
      +----+--------+--------+
      |  5 |      3 |      9 |
      +----+--------+--------+

      >>> df.filter_by_range("col1", low=1, high=2)
      array([True False False False True True])

      >>> filtered_df = df[df.filter_by_range("col1", low=1, high=2)]
      >>> display(filtered_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      3 |      8 |
      +----+--------+--------+
      |  2 |      3 |      9 |
      +----+--------+--------+



   .. py:method:: from_pandas(pd_df)

      Copy the data from a pandas DataFrame into a new arkouda.dataframe.DataFrame.

      :param pd_df: A pandas DataFrame to convert.
      :type pd_df: pandas.DataFrame

      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import pandas as pd
      >>> pd_df = pd.DataFrame({"A":[1,2],"B":[3,4]})
      >>> type(pd_df)
      pandas.core.frame.DataFrame
      >>> display(pd_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+

      >>> ak_df = DataFrame.from_pandas(pd_df)
      >>> type(ak_df)
      arkouda.dataframe.DataFrame
      >>> display(ak_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: from_return_msg(rep_msg)

      Creates a DataFrame object from an arkouda server response message.

      :param rep_msg: Server response message used to create a DataFrame.
      :type rep_msg: string

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: groupby(keys, use_series=True, as_index=True, dropna=True)

      Group the dataframe by a column or a list of columns.  Alias for GroupBy.

      :param keys: An (ordered) list of column names or a single string to group by.
      :type keys: str or list of str
      :param use_series: If True, returns an arkouda.dataframe.DataFrameGroupBy object.
                         Otherwise an arkouda.groupbyclass.GroupBy object.
      :type use_series: bool, default=True
      :param as_index: If True, groupby columns will be set as index
                       otherwise, the groupby columns will be treated as DataFrame columns.
      :type as_index: bool, default=True
      :param dropna: If True, and the groupby keys contain NaN values,
                     the NaN values together with the corresponding row will be dropped.
                     Otherwise, the rows corresponding to NaN values will be kept.
      :type dropna: bool, default=True

      :returns: If use_series = True, returns an arkouda.dataframe.DataFrameGroupBy object.
                Otherwise returns an arkouda.groupbyclass.GroupBy object.
      :rtype: arkouda.dataframe.DataFrameGroupBy or arkouda.groupbyclass.GroupBy

      .. seealso:: :obj:`arkouda.GroupBy`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.0, 1.0, 2.0, np.nan], 'col2': [4, 5, 6, 7]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      1 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |    nan |      7 |
      +----+--------+--------+

      >>> df.GroupBy("col1")
      <arkouda.groupbyclass.GroupBy at 0x7f2cf23e10c0>
      >>> df.GroupBy("col1").size()
      (array([1.00000000000000000 2.00000000000000000]), array([2 1]))

      >>> df.GroupBy("col1",use_series=True)
      col1
      1.0    2
      2.0    1
      dtype: int64
      >>> df.GroupBy("col1",use_series=True, as_index = False).size()

      +----+--------+--------+
      |    |   col1 |   size |
      +====+========+========+
      |  0 |      1 |      2 |
      +----+--------+--------+
      |  1 |      2 |      1 |
      +----+--------+--------+



   .. py:method:: head(n=5)

      Return the first `n` rows.

      This function returns the first `n` rows of the the dataframe. It is
      useful for quickly verifying data, for example, after sorting or
      appending rows.

      :param n: Number of rows to select.
      :type n: int, default = 5

      :returns: The first `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`tail`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(10), 'col2': -1 * ak.arange(10)})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+
      |  5 |      5 |     -5 |
      +----+--------+--------+
      |  6 |      6 |     -6 |
      +----+--------+--------+
      |  7 |      7 |     -7 |
      +----+--------+--------+
      |  8 |      8 |     -8 |
      +----+--------+--------+
      |  9 |      9 |     -9 |
      +----+--------+--------+

      >>> df.head()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+

      >>> df.head(n=2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+



   .. py:property:: index

      The index of the dataframe.

      :returns: The index of the dataframe.
      :rtype: arkouda.index.Index or arkouda.index.MultiIndex

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df.index
      Index(array([0 1]), dtype='int64')


   .. py:property:: info

      Returns a summary string of this dataframe.

      :returns: A summary string of this dataframe.
      :rtype: str

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': ["a", "b"]})
      >>> df

      +----+--------+--------+
      |    |   col1 | col2   |
      +====+========+========+
      |  0 |      1 | a      |
      +----+--------+--------+
      |  1 |      2 | b      |
      +----+--------+--------+

      >>> df.info
      "DataFrame(['col1', 'col2'], 2 rows, 20 B)"


   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry.

      :returns: Indicates if the object is contained in the registry.
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components.

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_dataframe_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: isin(values: Union[pdarray, Dict, Series, DataFrame]) -> DataFrame

      Determine whether each element in the DataFrame is contained in values.

      :param values: The values to check for in DataFrame. Series can only have a single index.
      :type values: pdarray, dict, Series, or DataFrame

      :returns: Arkouda DataFrame of booleans showing whether each element in the DataFrame is
                contained in values.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`ak.Series.isin`

      .. rubric:: Notes

      - Pandas supports values being an iterable type. In arkouda, we replace this with pdarray.
      - Pandas supports ~ operations. Currently, ak.DataFrame does not support this.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col_A': ak.array([7, 3]), 'col_B':ak.array([1, 9])})
      >>> display(df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       7 |       1 |
      +----+---------+---------+
      |  1 |       3 |       9 |
      +----+---------+---------+

      When `values` is a pdarray, check every value in the DataFrame to determine if
      it exists in values.

      >>> df.isin(ak.array([0, 1]))

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       1 |
      +----+---------+---------+
      |  1 |       0 |       0 |
      +----+---------+---------+

      When `values` is a dict, the values in the dict are passed to check the column
      indicated by the key.

      >>> df.isin({'col_A': ak.array([0, 3])})

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       0 |
      +----+---------+---------+
      |  1 |       1 |       0 |
      +----+---------+---------+

      When `values` is a Series, each column is checked if values is present positionally.
      This means that for `True` to be returned, the indexes must be the same.

      >>> i = ak.Index(ak.arange(2))
      >>> s = ak.Series(data=[3, 9], index=i)
      >>> df.isin(s)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       0 |
      +----+---------+---------+
      |  1 |       0 |       1 |
      +----+---------+---------+

      When `values` is a DataFrame, the index and column must match.
      Note that 9 is not found because the column name does not match.

      >>> other_df = ak.DataFrame({'col_A':ak.array([7, 3]), 'col_C':ak.array([0, 9])})
      >>> df.isin(other_df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       1 |       0 |
      +----+---------+---------+
      |  1 |       1 |       0 |
      +----+---------+---------+



   .. py:method:: isna() -> DataFrame

      Detect missing values.

      Return a boolean same-sized object indicating if the values are NA.
      numpy.NaN values get mapped to True values.
      Everything else gets mapped to False values.

      :returns: Mask of bool values for each element in DataFrame
                that indicates whether an element is an NA value.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({"A": [np.nan, 2, 2, 3], "B": [3, np.nan, 5, 6],
      ...          "C": [1, np.nan, 2, np.nan], "D":["a","b","c","d"]})
      >>> display(df)

      +----+-----+-----+-----+-----+
      |    |   A |   B |   C | D   |
      +====+=====+=====+=====+=====+
      |  0 | nan |   3 |   1 | a   |
      +----+-----+-----+-----+-----+
      |  1 |   2 | nan | nan | b   |
      +----+-----+-----+-----+-----+
      |  2 |   2 |   5 |   2 | c   |
      +----+-----+-----+-----+-----+
      |  3 |   3 |   6 | nan | d   |
      +----+-----+-----+-----+-----+

      >>> df.isna()
             A      B      C      D
      0   True  False  False  False
      1  False   True   True  False
      2  False  False  False  False
      3  False  False   True  False (4 rows x 4 columns)



   .. py:method:: load(prefix_path, file_format='INFER')

      Load dataframe from file.
      file_format needed for consistency with other load functions.

      :param prefix_path: The prefix path for the data.
      :type prefix_path: str
      :param file_format:
      :type file_format: string, default = "INFER"

      :returns: A dataframe loaded from the prefix_path.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      To store data in <my_dir>/my_data_LOCALE0000,
      use "<my_dir>/my_data" as the prefix.

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf5_output','my_data')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)
      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> df.save(my_path, file_type="distribute")
      >>> df.load(my_path)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+



   .. py:method:: memory_usage(index=True, unit='B') -> Series

      Return the memory usage of each column in bytes.

      The memory usage can optionally include the contribution of
      the index.

      :param index: Specifies whether to include the memory usage of the DataFrame's
                    index in returned Series. If ``index=True``, the memory usage of
                    the index is the first item in the output.
      :type index: bool, default True
      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: A Series whose index is the original column names and whose values
                is the memory usage of each column in bytes.
      :rtype: Series

      .. seealso:: :obj:`arkouda.pdarrayclass.nbytes`, :obj:`arkouda.index.Index.memory_usage`, :obj:`arkouda.index.MultiIndex.memory_usage`, :obj:`arkouda.series.Series.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> dtypes = [ak.int64, ak.float64,  ak.bool]
      >>> data = dict([(str(t), ak.ones(5000, dtype=ak.int64).astype(t)) for t in dtypes])
      >>> df = ak.DataFrame(data)
      >>> display(df.head())

      +----+---------+-----------+--------+
      |    |   int64 |   float64 | bool   |
      +====+=========+===========+========+
      |  0 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  1 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  2 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  3 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  4 |       1 |         1 | True   |
      +----+---------+-----------+--------+

      >>> df.memory_usage()

      +---------+-------+
      |         |     0 |
      +=========+=======+
      | Index   | 40000 |
      +---------+-------+
      | int64   | 40000 |
      +---------+-------+
      | float64 | 40000 |
      +---------+-------+
      | bool    |  5000 |
      +---------+-------+

      >>> df.memory_usage(index=False)

      +---------+-------+
      |         |     0 |
      +=========+=======+
      | int64   | 40000 |
      +---------+-------+
      | float64 | 40000 |
      +---------+-------+
      | bool    |  5000 |
      +---------+-------+

      >>> df.memory_usage(unit="KB")

      +---------+----------+
      |         |        0 |
      +=========+==========+
      | Index   | 39.0625  |
      +---------+----------+
      | int64   | 39.0625  |
      +---------+----------+
      | float64 | 39.0625  |
      +---------+----------+
      | bool    |  4.88281 |
      +---------+----------+

      To get the approximate total memory usage:

      >>>  df.memory_usage(index=True).sum()



   .. py:method:: memory_usage_info(unit='GB')

      A formatted string representation of the size of this DataFrame.

      :param unit: Unit to return. One of {'KB', 'MB', 'GB'}.
      :type unit: str, default = "GB"

      :returns: A string representation of the number of bytes used by this DataFrame in [unit]s.
      :rtype: str

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(1000), 'col2': ak.arange(1000)})
      >>> df.memory_usage_info()
      '0.00 GB'

      >>> df.memory_usage_info(unit="KB")
      '15 KB'



   .. py:method:: merge(right: DataFrame, on: Optional[Union[str, List[str]]] = None, how: str = 'inner', left_suffix: str = '_x', right_suffix: str = '_y', convert_ints: bool = True, sort: bool = True) -> DataFrame

      Merge Arkouda DataFrames with a database-style join.
      The resulting dataframe contains rows from both DataFrames as specified by
      the merge condition (based on the "how" and "on" parameters).

      Based on pandas merge functionality.
      https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html

      :param right: The Right DataFrame to be joined.
      :type right: DataFrame
      :param on: The name or list of names of the DataFrame column(s) to join on.
                 If on is None, this defaults to the intersection of the columns in both DataFrames.
      :type on: Optional[Union[str, List[str]]] = None
      :param how: The merge condition.
                  Must be "inner", "left", or "right".
      :type how: {"inner", "left", "right}, default = "inner"
      :param left_suffix: A string indicating the suffix to add to columns from the left dataframe for overlapping
                          column names in both left and right. Defaults to "_x". Only used when how is "inner".
      :type left_suffix: str, default = "_x"
      :param right_suffix: A string indicating the suffix to add to columns from the right dataframe for overlapping
                           column names in both left and right. Defaults to "_y". Only used when how is "inner".
      :type right_suffix: str, default = "_y"
      :param convert_ints: If True, convert columns with missing int values (due to the join) to float64.
                           This is to match pandas.
                           If False, do not convert the column dtypes.
                           This has no effect when how = "inner".
      :type convert_ints: bool = True
      :param sort: If True, DataFrame is returned sorted by "on".
                   Otherwise, the DataFrame is not sorted.
      :type sort: bool = True

      :returns: Joined Arkouda DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. note:: Multiple column joins are only supported for integer columns.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> left_df = ak.DataFrame({'col1': ak.arange(5), 'col2': -1 * ak.arange(5)})
      >>> display(left_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+

      >>> right_df = ak.DataFrame({'col1': 2 * ak.arange(5), 'col2': 2 * ak.arange(5)})
      >>> display(right_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      2 |      2 |
      +----+--------+--------+
      |  2 |      4 |      4 |
      +----+--------+--------+
      |  3 |      6 |      6 |
      +----+--------+--------+
      |  4 |      8 |      8 |
      +----+--------+--------+

      >>> left_df.merge(right_df, on = "col1")

      +----+--------+----------+----------+
      |    |   col1 |   col2_x |   col2_y |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      2 |       -2 |        2 |
      +----+--------+----------+----------+
      |  2 |      4 |       -4 |        4 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "left")

      +----+--------+----------+----------+
      |    |   col1 |   col2_y |   col2_x |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      1 |      nan |       -1 |
      +----+--------+----------+----------+
      |  2 |      2 |        2 |       -2 |
      +----+--------+----------+----------+
      |  3 |      3 |      nan |       -3 |
      +----+--------+----------+----------+
      |  4 |      4 |        4 |       -4 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "right")

      +----+--------+----------+----------+
      |    |   col1 |   col2_x |   col2_y |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      2 |       -2 |        2 |
      +----+--------+----------+----------+
      |  2 |      4 |       -4 |        4 |
      +----+--------+----------+----------+
      |  3 |      6 |      nan |        6 |
      +----+--------+----------+----------+
      |  4 |      8 |      nan |        8 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "outer")

      +----+--------+----------+----------+
      |    |   col1 |   col2_y |   col2_x |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      1 |      nan |       -1 |
      +----+--------+----------+----------+
      |  2 |      2 |        2 |       -2 |
      +----+--------+----------+----------+
      |  3 |      3 |      nan |       -3 |
      +----+--------+----------+----------+
      |  4 |      4 |        4 |       -4 |
      +----+--------+----------+----------+
      |  5 |      6 |        6 |      nan |
      +----+--------+----------+----------+
      |  6 |      8 |        8 |      nan |
      +----+--------+----------+----------+



   .. py:method:: notna() -> DataFrame

      Detect existing (non-missing) values.

      Return a boolean same-sized object indicating if the values are not NA.
      numpy.NaN values get mapped to False values.

      :returns: Mask of bool values for each element in DataFrame
                that indicates whether an element is not an NA value.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({"A": [np.nan, 2, 2, 3], "B": [3, np.nan, 5, 6],
      ...          "C": [1, np.nan, 2, np.nan], "D":["a","b","c","d"]})
      >>> display(df)

      +----+-----+-----+-----+-----+
      |    |   A |   B |   C | D   |
      +====+=====+=====+=====+=====+
      |  0 | nan |   3 |   1 | a   |
      +----+-----+-----+-----+-----+
      |  1 |   2 | nan | nan | b   |
      +----+-----+-----+-----+-----+
      |  2 |   2 |   5 |   2 | c   |
      +----+-----+-----+-----+-----+
      |  3 |   3 |   6 | nan | d   |
      +----+-----+-----+-----+-----+

      >>> df.notna()
             A      B      C     D
      0  False   True   True  True
      1   True  False  False  True
      2   True   True   True  True
      3   True   True  False  True (4 rows x 4 columns)



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: read_csv(filename: str, col_delim: str = ',')

      Read the columns of a CSV file into an Arkouda DataFrame.
      If the file contains the appropriately formatted header, typed data will be returned.
      Otherwise, all data will be returned as a Strings objects.

      :param filename: Filename to read data from.
      :type filename: str
      :param col_delim: The delimiter for columns within the data.
      :type col_delim: str, default=","

      :returns: Arkouda DataFrame containing the columns from the CSV file.
      :rtype: arkouda.dataframe.DataFrame

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.

      .. seealso:: :obj:`to_csv`

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations.
      - The column delimiter is expected to be the same for column names and data.
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline ("\\n") at this time.
      - Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'csv_output','my_data')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_csv(my_path)
      >>> df2 = DataFrame.read_csv(my_path + "_LOCALE0000")
      >>> display(df2)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: register(user_defined_name: str) -> DataFrame

      Register this DataFrame object and underlying components with the Arkouda server.

      :param user_defined_name: User defined name the DataFrame is to be registered under.
                                This will be the root name for underlying components.
      :type user_defined_name: str

      :returns: The same DataFrame which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different DataFrames with the same name.
      :rtype: arkouda.dataframe.DataFrame

      :raises TypeError: Raised if user_defined_name is not a str.
      :raises RegistrationError: If the server was unable to register the DataFrame with the user_defined_name.

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_dataframe_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      Any changes made to a DataFrame object after registering with the server may not be reflected
      in attached copies.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: rename(mapper: Optional[Union[Callable, Dict]] = None, index: Optional[Union[Callable, Dict]] = None, column: Optional[Union[Callable, Dict]] = None, axis: Union[str, int] = 0, inplace: bool = False) -> Optional[DataFrame]

      Rename indexes or columns according to a mapping.

      :param mapper: Function or dictionary mapping existing values to new values.
                     Nonexistent names will not raise an error.
                     Uses the value of axis to determine if renaming column or index
      :type mapper: callable or dict-like, Optional
      :param column: Function or dictionary mapping existing column names to
                     new column names. Nonexistent names will not raise an
                     error.
                     When this is set, axis is ignored.
      :type column: callable or dict-like, Optional
      :param index: Function or dictionary mapping existing index names to
                    new index names. Nonexistent names will not raise an
                    error.
                    When this is set, axis is ignored.
      :type index: callable or dict-like, Optional
      :param axis: Indicates which axis to perform the rename.
                   0/"index" - Indexes
                   1/"column" - Columns
      :type axis: int or str, default=0
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`.
      :rtype: arkouda.dataframe.DataFrame or None

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A": ak.array([1, 2, 3]), "B": ak.array([4, 5, 6])})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename columns using a mapping:

      >>> df.rename(column={'A':'a', 'B':'c'})

      +----+-----+-----+
      |    |   a |   c |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename indexes using a mapping:

      >>> df.rename(index={0:99, 2:11})

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename using an axis style parameter:

      >>> df.rename(str.lower, axis='column')

      +----+-----+-----+
      |    |   a |   b |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+



   .. py:method:: reset_index(size: Optional[int] = None, inplace: bool = False) -> Union[None, DataFrame]

      Set the index to an integer range.

      Useful if this dataframe is the result of a slice operation from
      another dataframe, or if you have permuted the rows and no longer need
      to keep that ordering on the rows.

      :param size: If size is passed, do not attempt to determine size based on
                   existing column sizes. Assume caller handles consistency correctly.
      :type size: int, optional
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`.
      :rtype: arkouda.dataframe.DataFrame or None

      .. note::

         Pandas adds a column 'index' to indicate the original index. Arkouda does not currently
         support this behavior.

      .. rubric:: Example

      >>> df = ak.DataFrame({"A": ak.array([1, 2, 3]), "B": ak.array([4, 5, 6])})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      >>> perm_df = df[ak.array([0,2,1])]
      >>> display(perm_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   3 |   6 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+

      >>> perm_df.reset_index()

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   3 |   6 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+



   .. py:method:: sample(n=5)

      Return a random sample of `n` rows.

      :param n: Number of rows to return.
      :type n: int, default=5

      :returns: The sampled `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Example

      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+

      Random output of size 3:

      >>> df.sample(n=3)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   4 |  -4 |
      +----+-----+-----+



   .. py:method:: save(path, index=False, columns=None, file_format='HDF5', file_type='distribute', compression: Optional[str] = None)

      DEPRECATED
      Save DataFrame to disk, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: list, default=None
      :param file_format: 'HDF5' or 'Parquet'. Defaults to 'HDF5'
      :type file_format: str, default='HDF5'
      :param file_type: "single" or "distribute"
                        If single, will right a single file to locale 0.
      :type file_type: str, default=distribute
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Compression type. Only used for Parquet
      :type compression: str (Optional)

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf5_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> df.save(my_path + '/my_data', file_type="single")
      >>> df.load(my_path + '/my_data')

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+



   .. py:property:: shape

      The shape of the dataframe.

      :returns: Tuple of array dimensions.
      :rtype: tuple of int

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> df.shape
      (3, 2)


   .. py:property:: size

      Returns the number of bytes on the arkouda server.

      :returns: The number of bytes on the arkouda server.
      :rtype: int

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> df.size
      6


   .. py:method:: sort_index(ascending=True)

      Sort the DataFrame by indexed columns.

      Note: Fails on sort order of arkouda.strings.Strings columns when multiple columns being sorted.

      :param ascending: Sort values in ascending (default) or descending order.
      :type ascending: bool, default = True

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1.1, 3.1, 2.1], 'col2': [6, 5, 4]},
      ...          index = Index(ak.array([2,0,1]), name="idx"))

      >>> display(df)

      +----+--------+--------+
      | idx|   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    2.1 |      4 |
      +----+--------+--------+

      >>> df.sort_index()

      +----+--------+--------+
      | idx|   col1 |   col2 |
      +====+========+========+
      |  0 |    3.1 |      5 |
      +----+--------+--------+
      |  1 |    2.1 |      4 |
      +----+--------+--------+
      |  2 |    1.1 |      6 |
      +----+--------+--------+



   .. py:method:: sort_values(by=None, ascending=True)

      Sort the DataFrame by one or more columns.

      If no column is specified, all columns are used.

      Note: Fails on order of arkouda.strings.Strings columns when multiple columns being sorted.

      :param by: The name(s) of the column(s) to sort by.
      :type by: str or list/tuple of str, default = None
      :param ascending: Sort values in ascending (default) or descending order.
      :type ascending: bool, default = True

      .. seealso:: :obj:`apply_permutation`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [2, 2, 1], 'col2': [3, 4, 3], 'col3':[5, 6, 7]})
      >>> display(df)

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  1 |      2 |      4 |      6 |
      +----+--------+--------+--------+
      |  2 |      1 |      3 |      7 |
      +----+--------+--------+--------+

      >>> df.sort_values()

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      1 |      3 |      7 |
      +----+--------+--------+--------+
      |  1 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  2 |      2 |      4 |      6 |
      +----+--------+--------+--------+

      >>> df.sort_values("col3")

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      1 |      3 |      7 |
      +----+--------+--------+--------+
      |  1 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  2 |      2 |      4 |      6 |
      +----+--------+--------+--------+



   .. py:method:: tail(n=5)

      Return the last `n` rows.

      This function returns the last `n` rows for the dataframe. It is
      useful for quickly testing if your object has the right type of data in
      it.

      :param n: Number of rows to select.
      :type n: int, default=5

      :returns: The last `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`arkouda.dataframe.head`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(10), 'col2': -1 * ak.arange(10)})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+
      |  5 |      5 |     -5 |
      +----+--------+--------+
      |  6 |      6 |     -6 |
      +----+--------+--------+
      |  7 |      7 |     -7 |
      +----+--------+--------+
      |  8 |      8 |     -8 |
      +----+--------+--------+
      |  9 |      9 |     -9 |
      +----+--------+--------+

      >>> df.tail()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      5 |     -5 |
      +----+--------+--------+
      |  1 |      6 |     -6 |
      +----+--------+--------+
      |  2 |      7 |     -7 |
      +----+--------+--------+
      |  3 |      8 |     -8 |
      +----+--------+--------+
      |  4 |      9 |     -9 |
      +----+--------+--------+

      >>> df.tail(n=2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      8 |     -8 |
      +----+--------+--------+
      |  1 |      9 |     -9 |
      +----+--------+--------+



   .. py:method:: to_csv(path: str, index: bool = False, columns: Optional[List[str]] = None, col_delim: str = ',', overwrite: bool = False)

      Writes DataFrame to CSV file(s). File will contain a column for each column in the DataFrame.
      All CSV Files written by Arkouda include a header denoting data types of the columns.
      Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      :param path: The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                   when they are written to disk.
      :type path: str
      :param index: If True, the index of the DataFrame will be written to the file
                    as a column.
      :type index: bool, default=False
      :param columns: Column names to assign when writing data.
      :type columns: list of str (Optional)
      :param col_delim: Value to be used to separate columns within the file.
                        Please be sure that the value used DOES NOT appear in your dataset.
      :type col_delim: str, default=","
      :param overwrite: If True, any existing files matching your provided prefix_path will
                        be overwritten. If False, an error will be returned if existing files are found.
      :type overwrite: bool, default=False

      :rtype: None

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations.
      - The column delimiter is expected to be the same for column names and data.
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline ("\\n") at this time.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'csv_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_csv(my_path + "/my_data")
      >>> df2 = DataFrame.read_csv(my_path + "/my_data" + "_LOCALE0000")
      >>> display(df2)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: to_hdf(path, index=False, columns=None, file_type='distribute')

      Save DataFrame to disk as hdf5, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: List, default = None
      :param file_type: Whether to save to a single file or distribute across Locales.
      :type file_type: str (single | distribute), default=distribute

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_parquet`, :obj:`load`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: to_markdown(mode='wt', index=True, tablefmt='grid', storage_options=None, **kwargs)

      Print DataFrame in Markdown-friendly format.

      :param mode: Mode in which file is opened, "wt" by default.
      :type mode: str, optional
      :param index: Add index (row) labels.
      :type index: bool, optional, default True
      :param tablefmt: Table format to call from tablulate:
                       https://pypi.org/project/tabulate/
      :type tablefmt: str = "grid"
      :param storage_options: Extra options that make sense for a particular storage connection,
                              e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec,
                              e.g., starting s3://, gcs://.
                              An error will be raised if providing this argument with a non-fsspec URL.
                              See the fsspec and backend storage implementation docs for the set
                              of allowed keys and values.
      :type storage_options: dict, optional
      :param \*\*kwargs: These parameters will be passed to tabulate.

      .. note::

         This function should only be called on small DataFrames as it calls pandas.DataFrame.to_markdown:
         https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.to_markdown.html

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"animal_1": ["elk", "pig"], "animal_2": ["dog", "quetzal"]})
      >>> print(df.to_markdown())
      +----+------------+------------+
      |    | animal_1   | animal_2   |
      +====+============+============+
      |  0 | elk        | dog        |
      +----+------------+------------+
      |  1 | pig        | quetzal    |
      +----+------------+------------+

      Suppress the index:

      >>> print(df.to_markdown(index = False))
      +------------+------------+
      | animal_1   | animal_2   |
      +============+============+
      | elk        | dog        |
      +------------+------------+
      | pig        | quetzal    |
      +------------+------------+





   .. py:method:: to_pandas(datalimit=1073741824, retain_index=False)

      Send this DataFrame to a pandas DataFrame.

      :param datalimit: The maximum number size, in megabytes to transfer. The requested
                        DataFrame will be converted to a pandas DataFrame only if the
                        estimated size of the DataFrame does not exceed this value.
      :type datalimit: int, default=arkouda.client.maxTransferBytes
      :param retain_index: Normally, to_pandas() creates a new range index object. If you want
                           to keep the index column, set this to True.
      :type retain_index: bool, default=False

      :returns: The result of converting this DataFrame to a pandas DataFrame.
      :rtype: pandas.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> ak_df = ak.DataFrame({"A": ak.arange(2), "B": -1 * ak.arange(2)})
      >>> type(ak_df)
      arkouda.dataframe.DataFrame
      >>> display(ak_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+

      >>> import pandas as pd
      >>> pd_df = ak_df.to_pandas()
      >>> type(pd_df)
      pandas.core.frame.DataFrame
      >>> display(pd_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+



   .. py:method:: to_parquet(path, index=False, columns=None, compression: Optional[str] = None, convert_categoricals: bool = False)

      Save DataFrame to disk as parquet, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: list
      :param compression: Provide the compression type to use when writing the file.
                          Supported values: snappy, gzip, brotli, zstd, lz4
      :type compression: str (Optional), default=None
      :param convert_categoricals: Parquet requires all columns to be the same size and Categoricals
                                   don't satisfy that requirement.
                                   If set, write the equivalent Strings in place of any Categorical columns.
      :type convert_categoricals: bool, default=False

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_hdf`, :obj:`load`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'parquet_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_parquet(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   B |   A |
      +====+=====+=====+
      |  0 |   3 |   1 |
      +----+-----+-----+
      |  1 |   4 |   2 |
      +----+-----+-----+



   .. py:method:: transfer(hostname, port)

      Sends a DataFrame to a different Arkouda server.

      :param hostname: The hostname where the Arkouda server intended to
                       receive the DataFrame is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :returns: A message indicating a complete transfer.
      :rtype: str

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister()

      Unregister this DataFrame object in the arkouda server which was previously
      registered using register() and/or attached to using attach().

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister.

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_dataframe_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: unregister_dataframe_by_name(user_defined_name: str) -> str

      Function to unregister DataFrame object by name which was registered
      with the arkouda server via register().

      :param user_defined_name: Name under which the DataFrame object was registered.
      :type user_defined_name: str

      :raises TypeError: If user_defined_name is not a string.
      :raises RegistrationError: If there is an issue attempting to unregister any underlying components.

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister_dataframe_by_name("my_table_name")
      >>> df.is_registered()
      False



   .. py:method:: update_hdf(prefix_path: str, index=False, columns=None, repack: bool = True)

      Overwrite the dataset with the name provided with this dataframe. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share.
      :type prefix_path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: List, default=None
      :param repack: HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool, default=True

      :returns: Success message if successful.
      :rtype: str

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      If the dataset provided does not exist, it will be added.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+

      >>> df2 = ak.DataFrame({"A":[5,6],"B":[7,8]})
      >>> df2.update_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   5 |   7 |
      +----+-----+-----+
      |  1 |   6 |   8 |
      +----+-----+-----+



   .. py:method:: update_nrows()

      Computes the number of rows on the arkouda server and updates the size parameter.




.. py:class:: DataFrame(dict=None, /, **kwargs)

   Bases: :py:obj:`collections.UserDict`


   A DataFrame structure based on arkouda arrays.

   :param initialdata: Each list/dictionary entry corresponds to one column of the data and
                       should be a homogenous type. Different columns may have different
                       types. If using a dictionary, keys should be strings.
   :type initialdata: List or dictionary of lists, tuples, or pdarrays
   :param index: Index for the resulting frame. Defaults to an integer range.
   :type index: Index, pdarray, or Strings
   :param columns: Column labels to use if the data does not include them. Elements must
                   be strings. Defaults to an stringified integer range.
   :type columns: List, tuple, pdarray, or Strings

   .. rubric:: Examples

   Create an empty DataFrame and add a column of data:

   >>> import arkouda as ak
   >>> ak.connect()
   >>> df = ak.DataFrame()
   >>> df['a'] = ak.array([1,2,3])
   >>> display(df)

   +----+-----+
   |    |   a |
   +====+=====+
   |  0 |   1 |
   +----+-----+
   |  1 |   2 |
   +----+-----+
   |  2 |   3 |
   +----+-----+

   Create a new DataFrame using a dictionary of data:

   >>> userName = ak.array(['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice'])
   >>> userID = ak.array([111, 222, 111, 333, 222, 111])
   >>> item = ak.array([0, 0, 1, 1, 2, 0])
   >>> day = ak.array([5, 5, 6, 5, 6, 6])
   >>> amount = ak.array([0.5, 0.6, 1.1, 1.2, 4.3, 0.6])
   >>> df = ak.DataFrame({'userName': userName, 'userID': userID,
   >>>            'item': item, 'day': day, 'amount': amount})
   >>> display(df)

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Alice      |      111 |      0 |     5 |      0.5 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  3 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  4 | Bob        |      222 |      2 |     6 |      4.3 |
   +----+------------+----------+--------+-------+----------+
   |  5 | Alice      |      111 |      0 |     6 |      0.6 |
   +----+------------+----------+--------+-------+----------+

   Indexing works slightly differently than with pandas:

   >>> df[0]

   +------------+----------+
   | keys       |   values |
   +============+==========+
   | userName   |    Alice |
   +------------+----------+
   |userID      |      111 |
   +------------+----------+
   | item       |      0   |
   +------------+----------+
   | day        |      5   |
   +------------+----------+
   | amount     |     0.5  |
   +------------+----------+

   >>> df['userID']
   array([111, 222, 111, 333, 222, 111])

   >>> df['userName']
   array(['Alice', 'Bob', 'Alice', 'Carol', 'Bob', 'Alice'])

   >>> df[ak.array([1,3,5])]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Alice      |      111 |      0 |     6 |      0.6 |
   +----+------------+----------+--------+-------+----------+

   Compute the stride:

   >>> df[1:5:1]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+
   |  3 | Bob        |      222 |      2 |     6 |      4.3 |
   +----+------------+----------+--------+-------+----------+

   >>> df[ak.array([1,2,3])]

   +----+------------+----------+--------+-------+----------+
   |    | userName   |   userID |   item |   day |   amount |
   +====+============+==========+========+=======+==========+
   |  0 | Bob        |      222 |      0 |     5 |      0.6 |
   +----+------------+----------+--------+-------+----------+
   |  1 | Alice      |      111 |      1 |     6 |      1.1 |
   +----+------------+----------+--------+-------+----------+
   |  2 | Carol      |      333 |      1 |     5 |      1.2 |
   +----+------------+----------+--------+-------+----------+

   >>> df[['userID', 'day']]

   +----+----------+-------+
   |    |   userID |   day |
   +====+==========+=======+
   |  0 |      111 |     5 |
   +----+----------+-------+
   |  1 |      222 |     5 |
   +----+----------+-------+
   |  2 |      111 |     6 |
   +----+----------+-------+
   |  3 |      333 |     5 |
   +----+----------+-------+
   |  4 |      222 |     6 |
   +----+----------+-------+
   |  5 |      111 |     6 |
   +----+----------+-------+


   .. py:method:: GroupBy(keys, use_series=False, as_index=True, dropna=True)

      Group the dataframe by a column or a list of columns.

      :param keys: An (ordered) list of column names or a single string to group by.
      :type keys: str or list of str
      :param use_series: If True, returns an arkouda.dataframe.DataFrameGroupBy object.
                         Otherwise an arkouda.groupbyclass.GroupBy object.
      :type use_series: bool, default=False
      :param as_index: If True, groupby columns will be set as index
                       otherwise, the groupby columns will be treated as DataFrame columns.
      :type as_index: bool, default=True
      :param dropna: If True, and the groupby keys contain NaN values,
                     the NaN values together with the corresponding row will be dropped.
                     Otherwise, the rows corresponding to NaN values will be kept.
      :type dropna: bool, default=True

      :returns: If use_series = True, returns an arkouda.dataframe.DataFrameGroupBy object.
                Otherwise returns an arkouda.groupbyclass.GroupBy object.
      :rtype: arkouda.dataframe.DataFrameGroupBy or arkouda.groupbyclass.GroupBy

      .. seealso:: :obj:`arkouda.GroupBy`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.0, 1.0, 2.0, np.nan], 'col2': [4, 5, 6, 7]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      1 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |    nan |      7 |
      +----+--------+--------+

      >>> df.GroupBy("col1")
      <arkouda.groupbyclass.GroupBy at 0x7f2cf23e10c0>
      >>> df.GroupBy("col1").size()
      (array([1.00000000000000000 2.00000000000000000]), array([2 1]))

      >>> df.GroupBy("col1",use_series=True)
      col1
      1.0    2
      2.0    1
      dtype: int64
      >>> df.GroupBy("col1",use_series=True, as_index = False).size()

      +----+--------+--------+
      |    |   col1 |   size |
      +====+========+========+
      |  0 |      1 |      2 |
      +----+--------+--------+
      |  1 |      2 |      1 |
      +----+--------+--------+



   .. py:method:: all(axis=0) -> Union[Series, bool]

      Return whether all elements are True, potentially over an axis.

      Returns True unless there at least one element along a Dataframe axis that is False.

      Currently, will ignore any columns that are not type bool.
      This is equivalent to the pandas option bool_only=True.

      :param axis: Indicate which axis or axes should be reduced.

                   0 / index : reduce the index, return a Series whose index is the original column labels.

                   1 / columns : reduce the columns, return a Series whose index is the original index.

                   None : reduce all axes, return a scalar.
      :type axis: {0 or index, 1 or columns, None}, default = 0

      :rtype: arkouda.series.Series or bool

      :raises ValueError: Raised if axis does not have a value in {0 or index, 1 or columns, None}.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[True,True,True,False],"B":[True,True,True,False],
      ...          "C":[True,False,True,False],"D":[True,True,True,True]})

      +----+---------+---------+---------+--------+
      |    |   A     |   B     |   C     |   D    |
      +====+=========+=========+=========+========+
      |  0 |   True  |   True  |   True  |   True |
      +----+---------+---------+---------+--------+
      |  1 |   True  |   True  |   False |   True |
      +----+---------+---------+---------+--------+
      |  2 |   True  |   True  |   True  |   True |
      +----+---------+---------+---------+--------+
      |  3 |   False |   False |   False |   True |
      +----+---------+---------+---------+--------+

      >>> df.all(axis=0)
      A    False
      B    False
      C    False
      D     True
      dtype: bool
      >>> df.all(axis=1)
      0     True
      1    False
      2     True
      3    False
      dtype: bool
      >>> df.all(axis=None)
      False



   .. py:method:: any(axis=0) -> Union[Series, bool]

      Return whether any element is True, potentially over an axis.

      Returns False unless there is at least one element along a Dataframe axis that is True.

      Currently, will ignore any columns that are not type bool.
      This is equivalent to the pandas option bool_only=True.

      :param axis: Indicate which axis or axes should be reduced.

                   0 / index : reduce the index, return a Series whose index is the original column labels.

                   1 / columns : reduce the columns, return a Series whose index is the original index.

                   None : reduce all axes, return a scalar.
      :type axis: {0 or index, 1 or columns, None}, default = 0

      :rtype: arkouda.series.Series or bool

      :raises ValueError: Raised if axis does not have a value in {0 or index, 1 or columns, None}.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[True,True,True,False],"B":[True,True,True,False],
      ...          "C":[True,False,True,False],"D":[False,False,False,False]})

      +----+---------+---------+---------+---------+
      |    |   A     |   B     |   C     |   D     |
      +====+=========+=========+=========+=========+
      |  0 |   True  |   True  |   True  |   False |
      +----+---------+---------+---------+---------+
      |  1 |   True  |   True  |   False |   False |
      +----+---------+---------+---------+---------+
      |  2 |   True  |   True  |   True  |   False |
      +----+---------+---------+---------+---------+
      |  3 |   False |   False |   False |   False |
      +----+---------+---------+---------+---------+

      >>> df.any(axis=0)
      A     True
      B     True
      C     True
      D    False
      dtype: bool
      >>> df.any(axis=1)
      0     True
      1     True
      2     True
      3    False
      dtype: bool
      >>> df.any(axis=None)
      True



   .. py:method:: append(other, ordered=True)

      Concatenate data from 'other' onto the end of this DataFrame, in place.

      Explicitly, use the arkouda concatenate function to append the data
      from each column in other to the end of self. This operation is done
      in place, in the sense that the underlying pdarrays are updated from
      the result of the arkouda concatenate function, rather than returning
      a new DataFrame object containing the result.

      :param other: The DataFrame object whose data will be appended to this DataFrame.
      :type other: DataFrame
      :param ordered: If False, allow rows to be interleaved for better performance (but
                      data within a row remains together). By default, append all rows
                      to the end, in input order.
      :type ordered: bool, default=True

      :returns: Appending occurs in-place, but result is returned for compatibility.
      :rtype: self

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df1 = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df2 = ak.DataFrame({'col1': [3], 'col2': [5]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      3 |      5 |
      +----+--------+--------+

      >>> df1.append(df2)
      >>> df1

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+
      |  2 |      3 |      5 |
      +----+--------+--------+



   .. py:method:: apply_permutation(perm)

      Apply a permutation to an entire DataFrame.  The operation is done in
      place and the original DataFrame will be modified.

      This may be useful if you want to unsort an DataFrame, or even to
      apply an arbitrary permutation such as the inverse of a sorting
      permutation.

      :param perm: A permutation array. Should be the same size as the data
                   arrays, and should consist of the integers [0,size-1] in
                   some order. Very minimal testing is done to ensure this
                   is a permutation.
      :type perm: pdarray

      :rtype: None

      .. seealso:: :obj:`sort`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> perm_arry = ak.array([0, 2, 1])
      >>> df.apply_permutation(perm_arry)
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      3 |      6 |
      +----+--------+--------+
      |  2 |      2 |      5 |
      +----+--------+--------+



   .. py:method:: argsort(key, ascending=True)

      Return the permutation that sorts the dataframe by `key`.

      :param key: The key to sort on.
      :type key: str
      :param ascending: If true, sort the key in ascending order.
                        Otherwise, sort the key in descending order.
      :type ascending: bool, default = True

      :returns: The permutation array that sorts the data on `key`.
      :rtype: arkouda.pdarrayclass.pdarray

      .. seealso:: :obj:`coargsort`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.1, 3.1, 2.1], 'col2': [6, 5, 4]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    2.1 |      4 |
      +----+--------+--------+

      >>> df.argsort('col1')
      array([0 2 1])
      >>> sorted_df1 = df[df.argsort('col1')]
      >>> display(sorted_df1)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    2.1 |      4 |
      +----+--------+--------+
      |  2 |    3.1 |      5 |
      +----+--------+--------+

      >>> df.argsort('col2')
      array([2 1 0])
      >>> sorted_df2 = df[df.argsort('col2')]
      >>> display(sorted_df2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |    2.1 |      4 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    1.1 |      6 |
      +----+--------+--------+



   .. py:method:: assign(**kwargs) -> DataFrame

      Assign new columns to a DataFrame.

      Returns a new object with all original columns in addition to new ones.
      Existing columns that are re-assigned will be overwritten.

      :param \*\*kwargs: The column names are keywords. If the values are
                         callable, they are computed on the DataFrame and
                         assigned to the new columns. The callable must not
                         change input DataFrame (though pandas doesn't check it).
                         If the values are not callable, (e.g. a Series, scalar, or array),
                         they are simply assigned.
      :type \*\*kwargs: dict of {str: callable or Series}

      :returns: A new DataFrame with the new columns in addition to
                all the existing columns.
      :rtype: DataFrame

      .. rubric:: Notes

      Assigning multiple columns within the same ``assign`` is possible.
      Later items in '\*\*kwargs' may refer to newly created or modified
      columns in 'df'; items are computed and assigned into 'df' in order.

      .. rubric:: Examples

      >>> df = ak.DataFrame({'temp_c': [17.0, 25.0]},
      ...                   index=['Portland', 'Berkeley'])
      >>> df
                temp_c
      Portland    17.0
      Berkeley    25.0

      Where the value is a callable, evaluated on `df`:

      >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)
                temp_c  temp_f
      Portland    17.0    62.6
      Berkeley    25.0    77.0

      Alternatively, the same behavior can be achieved by directly
      referencing an existing Series or sequence:

      >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)
                temp_c  temp_f
      Portland    17.0    62.6
      Berkeley    25.0    77.0

      You can create multiple columns within the same assign where one
      of the columns depends on another one defined within the same assign:

      >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,
      ...           temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9)
                temp_c  temp_f  temp_k
      Portland    17.0    62.6  290.15
      Berkeley    25.0    77.0  298.15



   .. py:method:: attach(user_defined_name: str) -> DataFrame

      Function to return a DataFrame object attached to the registered name in the
      arkouda server which was registered using register().

      :param user_defined_name: user defined name which DataFrame object was registered under.
      :type user_defined_name: str

      :returns: The DataFrame object created by re-attaching to the corresponding server components.
      :rtype: arkouda.dataframe.DataFrame

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: coargsort(keys, ascending=True)

      Return the permutation that sorts the dataframe by `keys`.

      Note: Sorting using Strings may not yield correct sort order.

      :param keys: The keys to sort on.
      :type keys: list of str

      :returns: The permutation array that sorts the data on `keys`.
      :rtype: arkouda.pdarrayclass.pdarray

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [2, 2, 1], 'col2': [3, 4, 3], 'col3':[5, 6, 7]})
      >>> display(df)

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  1 |      2 |      4 |      6 |
      +----+--------+--------+--------+
      |  2 |      1 |      3 |      7 |
      +----+--------+--------+--------+

      >>> df.coargsort(['col1', 'col2'])
      array([2 0 1])
      >>>



   .. py:property:: columns

      An Index where the values are the column names of the dataframe.

      :returns: The values of the index are the column names of the dataframe.
      :rtype: arkouda.index.Index

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df.columns
      Index(array(['col1', 'col2']), dtype='<U0')


   .. py:method:: concat(items, ordered=True)

      Essentially an append, but different formatting.





   .. py:method:: corr() -> DataFrame

      Return new DataFrame with pairwise correlation of columns.

      :returns: Arkouda DataFrame containing correlation matrix of all columns.
      :rtype: arkouda.dataframe.DataFrame

      :raises RuntimeError: Raised if there's a server-side error thrown.

      .. seealso:: :obj:`pdarray.corr`

      .. rubric:: Notes

      Generates the correlation matrix using Pearson R for all columns.

      Attempts to convert to numeric values where possible for inclusion in the matrix.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [-1, -2]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |     -1 |
      +----+--------+--------+
      |  1 |      2 |     -2 |
      +----+--------+--------+

      >>> corr = df.corr()

      +------+--------+--------+
      |      |   col1 |   col2 |
      +======+========+========+
      | col1 |      1 |     -1 |
      +------+--------+--------+
      | col2 |     -1 |      1 |
      +------+--------+--------+



   .. py:method:: count(axis: Union[int, str] = 0, numeric_only=False) -> Series

      Count non-NA cells for each column or row.

      The values np.NaN are considered NA.

      :param axis: If 0 or index counts are generated for each column.
                   If 1 or columns counts are generated for each row.
      :type axis: {0 or 'index', 1 or 'columns'}, default 0
      :param numeric_only: Include only float, int or boolean data.
      :type numeric_only: bool = False

      :returns: For each column/row the number of non-NA/null entries.
      :rtype: arkouda.series.Series

      :raises ValueError: Raised if axis is not 0, 1, 'index', or 'columns'.

      .. seealso:: :obj:`GroupBy.count`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({'col_A': ak.array([7, np.nan]), 'col_B':ak.array([1, 9])})
      >>> display(df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       7 |       1 |
      +----+---------+---------+
      |  1 |     nan |       9 |
      +----+---------+---------+

      >>> df.count()
      col_A    1
      col_B    2
      dtype: int64

      >>> df = ak.DataFrame({'col_A': ak.array(["a","b","c"]), 'col_B':ak.array([1, np.nan, np.nan])})
      >>> display(df)

      +----+---------+---------+
      |    | col_A   |   col_B |
      +====+=========+=========+
      |  0 | a       |       1 |
      +----+---------+---------+
      |  1 | b       |     nan |
      +----+---------+---------+
      |  2 | c       |     nan |
      +----+---------+---------+

      >>> df.count()
      col_A    3
      col_B    1
      dtype: int64

      >>> df.count(numeric_only=True)
      col_B    1
      dtype: int64

      >>> df.count(axis=1)
      0    2
      1    1
      2    1
      dtype: int64



   .. py:method:: drop(keys: Union[str, int, List[Union[str, int]]], axis: Union[str, int] = 0, inplace: bool = False) -> Union[None, DataFrame]

      Drop column/s or row/s from the dataframe.

      :param keys: The labels to be dropped on the given axis.
      :type keys: str, int or list
      :param axis: The axis on which to drop from. 0/'index' - drop rows, 1/'columns' - drop columns.
      :type axis: int or str
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`
      :rtype: arkouda.dataframe.DataFrame or None

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      Drop column

      >>> df.drop('col1', axis = 1)

      +----+--------+
      |    |   col2 |
      +====+========+
      |  0 |      3 |
      +----+--------+
      |  1 |      4 |
      +----+--------+

      Drop row

      >>> df.drop(0, axis = 0)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      2 |      4 |
      +----+--------+--------+



   .. py:method:: drop_duplicates(subset=None, keep='first')

      Drops duplcated rows and returns resulting DataFrame.

      If a subset of the columns are provided then only one instance of each
      duplicated row will be returned (keep determines which row).

      :param subset: Iterable of column names to use to dedupe.
      :type subset: Iterable
      :param keep: Determines which duplicates (if any) to keep.
      :type keep: {'first', 'last'}, default='first'

      :returns: DataFrame with duplicates removed.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 2, 3], 'col2': [4, 5, 5, 6]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      2 |      5 |
      +----+--------+--------+
      |  3 |      3 |      6 |
      +----+--------+--------+

      >>> df.drop_duplicates()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+



   .. py:method:: dropna(axis: Union[int, str] = 0, how: Optional[str] = None, thresh: Optional[int] = None, ignore_index: bool = False) -> DataFrame

      Remove missing values.

      :param axis: Determine if rows or columns which contain missing values are removed.

                   0, or 'index': Drop rows which contain missing values.

                   1, or 'columns': Drop columns which contain missing value.

                   Only a single axis is allowed.
      :type axis: {0 or 'index', 1 or 'columns'}, default = 0
      :param how: Determine if row or column is removed from DataFrame, when we have at least one NA or all NA.

                  'any': If any NA values are present, drop that row or column.

                  'all': If all values are NA, drop that row or column.
      :type how: {'any', 'all'}, default='any'
      :param thresh: Require that many non - NA values.Cannot be combined with how.
      :type thresh: int, optional
      :param ignore_index: If ``True``, the resulting axis will be labeled 0, 1, , n - 1.
      :type ignore_index: bool, default ``False``

      :returns: DataFrame with NA entries dropped from it.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame(
          {
              "A": [True, True, True, True],
              "B": [1, np.nan, 2, np.nan],
              "C": [1, 2, 3, np.nan],
              "D": [False, False, False, False],
              "E": [1, 2, 3, 4],
              "F": ["a", "b", "c", "d"],
              "G": [1, 2, 3, 4],
          }
         )

      >>> display(df)

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True | nan |   2 | False |   2 | b   |   2 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  2 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  3 | True | nan | nan | False |   4 | d   |   4 |
      +----+------+-----+-----+-------+-----+-----+-----+

      >>> df.dropna()

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+

      >>> df.dropna(axis=1)

      +----+------+-------+-----+-----+-----+
      |    | A    | D     |   E | F   |   G |
      +====+======+=======+=====+=====+=====+
      |  0 | True | False |   1 | a   |   1 |
      +----+------+-------+-----+-----+-----+
      |  1 | True | False |   2 | b   |   2 |
      +----+------+-------+-----+-----+-----+
      |  2 | True | False |   3 | c   |   3 |
      +----+------+-------+-----+-----+-----+
      |  3 | True | False |   4 | d   |   4 |
      +----+------+-------+-----+-----+-----+

      >>> df.dropna(axis=1, thresh=3)

      +----+------+-----+-------+-----+-----+-----+
      |    | A    |   C | D     |   E | F   |   G |
      +====+======+=====+=======+=====+=====+=====+
      |  0 | True |   1 | False |   1 | a   |   1 |
      +----+------+-----+-------+-----+-----+-----+
      |  1 | True |   2 | False |   2 | b   |   2 |
      +----+------+-----+-------+-----+-----+-----+
      |  2 | True |   3 | False |   3 | c   |   3 |
      +----+------+-----+-------+-----+-----+-----+
      |  3 | True | nan | False |   4 | d   |   4 |
      +----+------+-----+-------+-----+-----+-----+

      >>> df.dropna(axis=1, how="all")

      +----+------+-----+-----+-------+-----+-----+-----+
      |    | A    |   B |   C | D     |   E | F   |   G |
      +====+======+=====+=====+=======+=====+=====+=====+
      |  0 | True |   1 |   1 | False |   1 | a   |   1 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  1 | True | nan |   2 | False |   2 | b   |   2 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  2 | True |   2 |   3 | False |   3 | c   |   3 |
      +----+------+-----+-----+-------+-----+-----+-----+
      |  3 | True | nan | nan | False |   4 | d   |   4 |
      +----+------+-----+-----+-------+-----+-----+-----+



   .. py:property:: dtypes
      :type: DataFrame


      The dtypes of the dataframe.

      :returns: **dtypes** -- The dtypes of the dataframe.
      :rtype: arkouda.row.Row

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': ["a", "b"]})
      >>> df

      +----+--------+--------+
      |    |   col1 | col2   |
      +====+========+========+
      |  0 |      1 | a      |
      +----+--------+--------+
      |  1 |      2 | b      |
      +----+--------+--------+

      >>> df.dtypes

      +----+--------+
      |keys| values |
      +====+========+
      |col1|  int64 |
      +----+--------+
      |col2|    str |
      +----+--------+


   .. py:property:: empty
      :type: DataFrame


      Whether the dataframe is empty.

      :returns: True if the dataframe is empty, otherwise False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({})
      >>> df
       0 rows x 0 columns
      >>> df.empty
      True


   .. py:method:: filter_by_range(keys, low=1, high=None)

      Find all rows where the value count of the items in a given set of
      columns (keys) is within the range [low, high].

      To filter by a specific value, set low == high.

      :param keys: The names of the columns to group by.
      :type keys: str or list of str
      :param low: The lowest value count.
      :type low: int, default=1
      :param high: The highest value count, default to unlimited.
      :type high: int, default=None

      :returns: An array of boolean values for qualified rows in this DataFrame.
      :rtype: arkouda.pdarrayclass.pdarray

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 2, 2, 3, 3], 'col2': [4, 5, 6, 7, 8, 9]})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |      2 |      7 |
      +----+--------+--------+
      |  4 |      3 |      8 |
      +----+--------+--------+
      |  5 |      3 |      9 |
      +----+--------+--------+

      >>> df.filter_by_range("col1", low=1, high=2)
      array([True False False False True True])

      >>> filtered_df = df[df.filter_by_range("col1", low=1, high=2)]
      >>> display(filtered_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      3 |      8 |
      +----+--------+--------+
      |  2 |      3 |      9 |
      +----+--------+--------+



   .. py:method:: from_pandas(pd_df)

      Copy the data from a pandas DataFrame into a new arkouda.dataframe.DataFrame.

      :param pd_df: A pandas DataFrame to convert.
      :type pd_df: pandas.DataFrame

      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import pandas as pd
      >>> pd_df = pd.DataFrame({"A":[1,2],"B":[3,4]})
      >>> type(pd_df)
      pandas.core.frame.DataFrame
      >>> display(pd_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+

      >>> ak_df = DataFrame.from_pandas(pd_df)
      >>> type(ak_df)
      arkouda.dataframe.DataFrame
      >>> display(ak_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: from_return_msg(rep_msg)

      Creates a DataFrame object from an arkouda server response message.

      :param rep_msg: Server response message used to create a DataFrame.
      :type rep_msg: string

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: groupby(keys, use_series=True, as_index=True, dropna=True)

      Group the dataframe by a column or a list of columns.  Alias for GroupBy.

      :param keys: An (ordered) list of column names or a single string to group by.
      :type keys: str or list of str
      :param use_series: If True, returns an arkouda.dataframe.DataFrameGroupBy object.
                         Otherwise an arkouda.groupbyclass.GroupBy object.
      :type use_series: bool, default=True
      :param as_index: If True, groupby columns will be set as index
                       otherwise, the groupby columns will be treated as DataFrame columns.
      :type as_index: bool, default=True
      :param dropna: If True, and the groupby keys contain NaN values,
                     the NaN values together with the corresponding row will be dropped.
                     Otherwise, the rows corresponding to NaN values will be kept.
      :type dropna: bool, default=True

      :returns: If use_series = True, returns an arkouda.dataframe.DataFrameGroupBy object.
                Otherwise returns an arkouda.groupbyclass.GroupBy object.
      :rtype: arkouda.dataframe.DataFrameGroupBy or arkouda.groupbyclass.GroupBy

      .. seealso:: :obj:`arkouda.GroupBy`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1.0, 1.0, 2.0, np.nan], 'col2': [4, 5, 6, 7]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      1 |      5 |
      +----+--------+--------+
      |  2 |      2 |      6 |
      +----+--------+--------+
      |  3 |    nan |      7 |
      +----+--------+--------+

      >>> df.GroupBy("col1")
      <arkouda.groupbyclass.GroupBy at 0x7f2cf23e10c0>
      >>> df.GroupBy("col1").size()
      (array([1.00000000000000000 2.00000000000000000]), array([2 1]))

      >>> df.GroupBy("col1",use_series=True)
      col1
      1.0    2
      2.0    1
      dtype: int64
      >>> df.GroupBy("col1",use_series=True, as_index = False).size()

      +----+--------+--------+
      |    |   col1 |   size |
      +====+========+========+
      |  0 |      1 |      2 |
      +----+--------+--------+
      |  1 |      2 |      1 |
      +----+--------+--------+



   .. py:method:: head(n=5)

      Return the first `n` rows.

      This function returns the first `n` rows of the the dataframe. It is
      useful for quickly verifying data, for example, after sorting or
      appending rows.

      :param n: Number of rows to select.
      :type n: int, default = 5

      :returns: The first `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`tail`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(10), 'col2': -1 * ak.arange(10)})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+
      |  5 |      5 |     -5 |
      +----+--------+--------+
      |  6 |      6 |     -6 |
      +----+--------+--------+
      |  7 |      7 |     -7 |
      +----+--------+--------+
      |  8 |      8 |     -8 |
      +----+--------+--------+
      |  9 |      9 |     -9 |
      +----+--------+--------+

      >>> df.head()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+

      >>> df.head(n=2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+



   .. py:property:: index

      The index of the dataframe.

      :returns: The index of the dataframe.
      :rtype: arkouda.index.Index or arkouda.index.MultiIndex

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      3 |
      +----+--------+--------+
      |  1 |      2 |      4 |
      +----+--------+--------+

      >>> df.index
      Index(array([0 1]), dtype='int64')


   .. py:property:: info

      Returns a summary string of this dataframe.

      :returns: A summary string of this dataframe.
      :rtype: str

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2], 'col2': ["a", "b"]})
      >>> df

      +----+--------+--------+
      |    |   col1 | col2   |
      +====+========+========+
      |  0 |      1 | a      |
      +----+--------+--------+
      |  1 |      2 | b      |
      +----+--------+--------+

      >>> df.info
      "DataFrame(['col1', 'col2'], 2 rows, 20 B)"


   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry.

      :returns: Indicates if the object is contained in the registry.
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components.

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_dataframe_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: isin(values: Union[pdarray, Dict, Series, DataFrame]) -> DataFrame

      Determine whether each element in the DataFrame is contained in values.

      :param values: The values to check for in DataFrame. Series can only have a single index.
      :type values: pdarray, dict, Series, or DataFrame

      :returns: Arkouda DataFrame of booleans showing whether each element in the DataFrame is
                contained in values.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`ak.Series.isin`

      .. rubric:: Notes

      - Pandas supports values being an iterable type. In arkouda, we replace this with pdarray.
      - Pandas supports ~ operations. Currently, ak.DataFrame does not support this.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col_A': ak.array([7, 3]), 'col_B':ak.array([1, 9])})
      >>> display(df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       7 |       1 |
      +----+---------+---------+
      |  1 |       3 |       9 |
      +----+---------+---------+

      When `values` is a pdarray, check every value in the DataFrame to determine if
      it exists in values.

      >>> df.isin(ak.array([0, 1]))

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       1 |
      +----+---------+---------+
      |  1 |       0 |       0 |
      +----+---------+---------+

      When `values` is a dict, the values in the dict are passed to check the column
      indicated by the key.

      >>> df.isin({'col_A': ak.array([0, 3])})

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       0 |
      +----+---------+---------+
      |  1 |       1 |       0 |
      +----+---------+---------+

      When `values` is a Series, each column is checked if values is present positionally.
      This means that for `True` to be returned, the indexes must be the same.

      >>> i = ak.Index(ak.arange(2))
      >>> s = ak.Series(data=[3, 9], index=i)
      >>> df.isin(s)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       0 |       0 |
      +----+---------+---------+
      |  1 |       0 |       1 |
      +----+---------+---------+

      When `values` is a DataFrame, the index and column must match.
      Note that 9 is not found because the column name does not match.

      >>> other_df = ak.DataFrame({'col_A':ak.array([7, 3]), 'col_C':ak.array([0, 9])})
      >>> df.isin(other_df)

      +----+---------+---------+
      |    |   col_A |   col_B |
      +====+=========+=========+
      |  0 |       1 |       0 |
      +----+---------+---------+
      |  1 |       1 |       0 |
      +----+---------+---------+



   .. py:method:: isna() -> DataFrame

      Detect missing values.

      Return a boolean same-sized object indicating if the values are NA.
      numpy.NaN values get mapped to True values.
      Everything else gets mapped to False values.

      :returns: Mask of bool values for each element in DataFrame
                that indicates whether an element is an NA value.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({"A": [np.nan, 2, 2, 3], "B": [3, np.nan, 5, 6],
      ...          "C": [1, np.nan, 2, np.nan], "D":["a","b","c","d"]})
      >>> display(df)

      +----+-----+-----+-----+-----+
      |    |   A |   B |   C | D   |
      +====+=====+=====+=====+=====+
      |  0 | nan |   3 |   1 | a   |
      +----+-----+-----+-----+-----+
      |  1 |   2 | nan | nan | b   |
      +----+-----+-----+-----+-----+
      |  2 |   2 |   5 |   2 | c   |
      +----+-----+-----+-----+-----+
      |  3 |   3 |   6 | nan | d   |
      +----+-----+-----+-----+-----+

      >>> df.isna()
             A      B      C      D
      0   True  False  False  False
      1  False   True   True  False
      2  False  False  False  False
      3  False  False   True  False (4 rows x 4 columns)



   .. py:method:: load(prefix_path, file_format='INFER')

      Load dataframe from file.
      file_format needed for consistency with other load functions.

      :param prefix_path: The prefix path for the data.
      :type prefix_path: str
      :param file_format:
      :type file_format: string, default = "INFER"

      :returns: A dataframe loaded from the prefix_path.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      To store data in <my_dir>/my_data_LOCALE0000,
      use "<my_dir>/my_data" as the prefix.

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf5_output','my_data')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)
      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> df.save(my_path, file_type="distribute")
      >>> df.load(my_path)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+



   .. py:method:: memory_usage(index=True, unit='B') -> Series

      Return the memory usage of each column in bytes.

      The memory usage can optionally include the contribution of
      the index.

      :param index: Specifies whether to include the memory usage of the DataFrame's
                    index in returned Series. If ``index=True``, the memory usage of
                    the index is the first item in the output.
      :type index: bool, default True
      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: A Series whose index is the original column names and whose values
                is the memory usage of each column in bytes.
      :rtype: Series

      .. seealso:: :obj:`arkouda.pdarrayclass.nbytes`, :obj:`arkouda.index.Index.memory_usage`, :obj:`arkouda.index.MultiIndex.memory_usage`, :obj:`arkouda.series.Series.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> dtypes = [ak.int64, ak.float64,  ak.bool]
      >>> data = dict([(str(t), ak.ones(5000, dtype=ak.int64).astype(t)) for t in dtypes])
      >>> df = ak.DataFrame(data)
      >>> display(df.head())

      +----+---------+-----------+--------+
      |    |   int64 |   float64 | bool   |
      +====+=========+===========+========+
      |  0 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  1 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  2 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  3 |       1 |         1 | True   |
      +----+---------+-----------+--------+
      |  4 |       1 |         1 | True   |
      +----+---------+-----------+--------+

      >>> df.memory_usage()

      +---------+-------+
      |         |     0 |
      +=========+=======+
      | Index   | 40000 |
      +---------+-------+
      | int64   | 40000 |
      +---------+-------+
      | float64 | 40000 |
      +---------+-------+
      | bool    |  5000 |
      +---------+-------+

      >>> df.memory_usage(index=False)

      +---------+-------+
      |         |     0 |
      +=========+=======+
      | int64   | 40000 |
      +---------+-------+
      | float64 | 40000 |
      +---------+-------+
      | bool    |  5000 |
      +---------+-------+

      >>> df.memory_usage(unit="KB")

      +---------+----------+
      |         |        0 |
      +=========+==========+
      | Index   | 39.0625  |
      +---------+----------+
      | int64   | 39.0625  |
      +---------+----------+
      | float64 | 39.0625  |
      +---------+----------+
      | bool    |  4.88281 |
      +---------+----------+

      To get the approximate total memory usage:

      >>>  df.memory_usage(index=True).sum()



   .. py:method:: memory_usage_info(unit='GB')

      A formatted string representation of the size of this DataFrame.

      :param unit: Unit to return. One of {'KB', 'MB', 'GB'}.
      :type unit: str, default = "GB"

      :returns: A string representation of the number of bytes used by this DataFrame in [unit]s.
      :rtype: str

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(1000), 'col2': ak.arange(1000)})
      >>> df.memory_usage_info()
      '0.00 GB'

      >>> df.memory_usage_info(unit="KB")
      '15 KB'



   .. py:method:: merge(right: DataFrame, on: Optional[Union[str, List[str]]] = None, how: str = 'inner', left_suffix: str = '_x', right_suffix: str = '_y', convert_ints: bool = True, sort: bool = True) -> DataFrame

      Merge Arkouda DataFrames with a database-style join.
      The resulting dataframe contains rows from both DataFrames as specified by
      the merge condition (based on the "how" and "on" parameters).

      Based on pandas merge functionality.
      https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html

      :param right: The Right DataFrame to be joined.
      :type right: DataFrame
      :param on: The name or list of names of the DataFrame column(s) to join on.
                 If on is None, this defaults to the intersection of the columns in both DataFrames.
      :type on: Optional[Union[str, List[str]]] = None
      :param how: The merge condition.
                  Must be "inner", "left", or "right".
      :type how: {"inner", "left", "right}, default = "inner"
      :param left_suffix: A string indicating the suffix to add to columns from the left dataframe for overlapping
                          column names in both left and right. Defaults to "_x". Only used when how is "inner".
      :type left_suffix: str, default = "_x"
      :param right_suffix: A string indicating the suffix to add to columns from the right dataframe for overlapping
                           column names in both left and right. Defaults to "_y". Only used when how is "inner".
      :type right_suffix: str, default = "_y"
      :param convert_ints: If True, convert columns with missing int values (due to the join) to float64.
                           This is to match pandas.
                           If False, do not convert the column dtypes.
                           This has no effect when how = "inner".
      :type convert_ints: bool = True
      :param sort: If True, DataFrame is returned sorted by "on".
                   Otherwise, the DataFrame is not sorted.
      :type sort: bool = True

      :returns: Joined Arkouda DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. note:: Multiple column joins are only supported for integer columns.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> left_df = ak.DataFrame({'col1': ak.arange(5), 'col2': -1 * ak.arange(5)})
      >>> display(left_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+

      >>> right_df = ak.DataFrame({'col1': 2 * ak.arange(5), 'col2': 2 * ak.arange(5)})
      >>> display(right_df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      2 |      2 |
      +----+--------+--------+
      |  2 |      4 |      4 |
      +----+--------+--------+
      |  3 |      6 |      6 |
      +----+--------+--------+
      |  4 |      8 |      8 |
      +----+--------+--------+

      >>> left_df.merge(right_df, on = "col1")

      +----+--------+----------+----------+
      |    |   col1 |   col2_x |   col2_y |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      2 |       -2 |        2 |
      +----+--------+----------+----------+
      |  2 |      4 |       -4 |        4 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "left")

      +----+--------+----------+----------+
      |    |   col1 |   col2_y |   col2_x |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      1 |      nan |       -1 |
      +----+--------+----------+----------+
      |  2 |      2 |        2 |       -2 |
      +----+--------+----------+----------+
      |  3 |      3 |      nan |       -3 |
      +----+--------+----------+----------+
      |  4 |      4 |        4 |       -4 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "right")

      +----+--------+----------+----------+
      |    |   col1 |   col2_x |   col2_y |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      2 |       -2 |        2 |
      +----+--------+----------+----------+
      |  2 |      4 |       -4 |        4 |
      +----+--------+----------+----------+
      |  3 |      6 |      nan |        6 |
      +----+--------+----------+----------+
      |  4 |      8 |      nan |        8 |
      +----+--------+----------+----------+

      >>> left_df.merge(right_df, on = "col1", how = "outer")

      +----+--------+----------+----------+
      |    |   col1 |   col2_y |   col2_x |
      +====+========+==========+==========+
      |  0 |      0 |        0 |        0 |
      +----+--------+----------+----------+
      |  1 |      1 |      nan |       -1 |
      +----+--------+----------+----------+
      |  2 |      2 |        2 |       -2 |
      +----+--------+----------+----------+
      |  3 |      3 |      nan |       -3 |
      +----+--------+----------+----------+
      |  4 |      4 |        4 |       -4 |
      +----+--------+----------+----------+
      |  5 |      6 |        6 |      nan |
      +----+--------+----------+----------+
      |  6 |      8 |        8 |      nan |
      +----+--------+----------+----------+



   .. py:method:: notna() -> DataFrame

      Detect existing (non-missing) values.

      Return a boolean same-sized object indicating if the values are not NA.
      numpy.NaN values get mapped to False values.

      :returns: Mask of bool values for each element in DataFrame
                that indicates whether an element is not an NA value.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import numpy as np
      >>> df = ak.DataFrame({"A": [np.nan, 2, 2, 3], "B": [3, np.nan, 5, 6],
      ...          "C": [1, np.nan, 2, np.nan], "D":["a","b","c","d"]})
      >>> display(df)

      +----+-----+-----+-----+-----+
      |    |   A |   B |   C | D   |
      +====+=====+=====+=====+=====+
      |  0 | nan |   3 |   1 | a   |
      +----+-----+-----+-----+-----+
      |  1 |   2 | nan | nan | b   |
      +----+-----+-----+-----+-----+
      |  2 |   2 |   5 |   2 | c   |
      +----+-----+-----+-----+-----+
      |  3 |   3 |   6 | nan | d   |
      +----+-----+-----+-----+-----+

      >>> df.notna()
             A      B      C     D
      0  False   True   True  True
      1   True  False  False  True
      2   True   True   True  True
      3   True   True  False  True (4 rows x 4 columns)



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: read_csv(filename: str, col_delim: str = ',')

      Read the columns of a CSV file into an Arkouda DataFrame.
      If the file contains the appropriately formatted header, typed data will be returned.
      Otherwise, all data will be returned as a Strings objects.

      :param filename: Filename to read data from.
      :type filename: str
      :param col_delim: The delimiter for columns within the data.
      :type col_delim: str, default=","

      :returns: Arkouda DataFrame containing the columns from the CSV file.
      :rtype: arkouda.dataframe.DataFrame

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.

      .. seealso:: :obj:`to_csv`

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations.
      - The column delimiter is expected to be the same for column names and data.
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline ("\\n") at this time.
      - Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'csv_output','my_data')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_csv(my_path)
      >>> df2 = DataFrame.read_csv(my_path + "_LOCALE0000")
      >>> display(df2)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: register(user_defined_name: str) -> DataFrame

      Register this DataFrame object and underlying components with the Arkouda server.

      :param user_defined_name: User defined name the DataFrame is to be registered under.
                                This will be the root name for underlying components.
      :type user_defined_name: str

      :returns: The same DataFrame which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different DataFrames with the same name.
      :rtype: arkouda.dataframe.DataFrame

      :raises TypeError: Raised if user_defined_name is not a str.
      :raises RegistrationError: If the server was unable to register the DataFrame with the user_defined_name.

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_dataframe_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      Any changes made to a DataFrame object after registering with the server may not be reflected
      in attached copies.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: rename(mapper: Optional[Union[Callable, Dict]] = None, index: Optional[Union[Callable, Dict]] = None, column: Optional[Union[Callable, Dict]] = None, axis: Union[str, int] = 0, inplace: bool = False) -> Optional[DataFrame]

      Rename indexes or columns according to a mapping.

      :param mapper: Function or dictionary mapping existing values to new values.
                     Nonexistent names will not raise an error.
                     Uses the value of axis to determine if renaming column or index
      :type mapper: callable or dict-like, Optional
      :param column: Function or dictionary mapping existing column names to
                     new column names. Nonexistent names will not raise an
                     error.
                     When this is set, axis is ignored.
      :type column: callable or dict-like, Optional
      :param index: Function or dictionary mapping existing index names to
                    new index names. Nonexistent names will not raise an
                    error.
                    When this is set, axis is ignored.
      :type index: callable or dict-like, Optional
      :param axis: Indicates which axis to perform the rename.
                   0/"index" - Indexes
                   1/"column" - Columns
      :type axis: int or str, default=0
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`.
      :rtype: arkouda.dataframe.DataFrame or None

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A": ak.array([1, 2, 3]), "B": ak.array([4, 5, 6])})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename columns using a mapping:

      >>> df.rename(column={'A':'a', 'B':'c'})

      +----+-----+-----+
      |    |   a |   c |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename indexes using a mapping:

      >>> df.rename(index={0:99, 2:11})

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      Rename using an axis style parameter:

      >>> df.rename(str.lower, axis='column')

      +----+-----+-----+
      |    |   a |   b |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+



   .. py:method:: reset_index(size: Optional[int] = None, inplace: bool = False) -> Union[None, DataFrame]

      Set the index to an integer range.

      Useful if this dataframe is the result of a slice operation from
      another dataframe, or if you have permuted the rows and no longer need
      to keep that ordering on the rows.

      :param size: If size is passed, do not attempt to determine size based on
                   existing column sizes. Assume caller handles consistency correctly.
      :type size: int, optional
      :param inplace: When True, perform the operation on the calling object.
                      When False, return a new object.
      :type inplace: bool, default=False

      :returns: DateFrame when `inplace=False`;
                None when `inplace=True`.
      :rtype: arkouda.dataframe.DataFrame or None

      .. note::

         Pandas adds a column 'index' to indicate the original index. Arkouda does not currently
         support this behavior.

      .. rubric:: Example

      >>> df = ak.DataFrame({"A": ak.array([1, 2, 3]), "B": ak.array([4, 5, 6])})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   2 |   5 |
      +----+-----+-----+
      |  2 |   3 |   6 |
      +----+-----+-----+

      >>> perm_df = df[ak.array([0,2,1])]
      >>> display(perm_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   3 |   6 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+

      >>> perm_df.reset_index()

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   4 |
      +----+-----+-----+
      |  1 |   3 |   6 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+



   .. py:method:: sample(n=5)

      Return a random sample of `n` rows.

      :param n: Number of rows to return.
      :type n: int, default=5

      :returns: The sampled `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Example

      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+

      Random output of size 3:

      >>> df.sample(n=3)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   4 |  -4 |
      +----+-----+-----+



   .. py:method:: save(path, index=False, columns=None, file_format='HDF5', file_type='distribute', compression: Optional[str] = None)

      DEPRECATED
      Save DataFrame to disk, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: list, default=None
      :param file_format: 'HDF5' or 'Parquet'. Defaults to 'HDF5'
      :type file_format: str, default='HDF5'
      :param file_type: "single" or "distribute"
                        If single, will right a single file to locale 0.
      :type file_type: str, default=distribute
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Compression type. Only used for Parquet
      :type compression: str (Optional)

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf5_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A": ak.arange(5), "B": -1 * ak.arange(5)})
      >>> df.save(my_path + '/my_data', file_type="single")
      >>> df.load(my_path + '/my_data')

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+
      |  2 |   2 |  -2 |
      +----+-----+-----+
      |  3 |   3 |  -3 |
      +----+-----+-----+
      |  4 |   4 |  -4 |
      +----+-----+-----+



   .. py:property:: shape

      The shape of the dataframe.

      :returns: Tuple of array dimensions.
      :rtype: tuple of int

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> df.shape
      (3, 2)


   .. py:property:: size

      Returns the number of bytes on the arkouda server.

      :returns: The number of bytes on the arkouda server.
      :rtype: int

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      1 |      4 |
      +----+--------+--------+
      |  1 |      2 |      5 |
      +----+--------+--------+
      |  2 |      3 |      6 |
      +----+--------+--------+

      >>> df.size
      6


   .. py:method:: sort_index(ascending=True)

      Sort the DataFrame by indexed columns.

      Note: Fails on sort order of arkouda.strings.Strings columns when multiple columns being sorted.

      :param ascending: Sort values in ascending (default) or descending order.
      :type ascending: bool, default = True

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1.1, 3.1, 2.1], 'col2': [6, 5, 4]},
      ...          index = Index(ak.array([2,0,1]), name="idx"))

      >>> display(df)

      +----+--------+--------+
      | idx|   col1 |   col2 |
      +====+========+========+
      |  0 |    1.1 |      6 |
      +----+--------+--------+
      |  1 |    3.1 |      5 |
      +----+--------+--------+
      |  2 |    2.1 |      4 |
      +----+--------+--------+

      >>> df.sort_index()

      +----+--------+--------+
      | idx|   col1 |   col2 |
      +====+========+========+
      |  0 |    3.1 |      5 |
      +----+--------+--------+
      |  1 |    2.1 |      4 |
      +----+--------+--------+
      |  2 |    1.1 |      6 |
      +----+--------+--------+



   .. py:method:: sort_values(by=None, ascending=True)

      Sort the DataFrame by one or more columns.

      If no column is specified, all columns are used.

      Note: Fails on order of arkouda.strings.Strings columns when multiple columns being sorted.

      :param by: The name(s) of the column(s) to sort by.
      :type by: str or list/tuple of str, default = None
      :param ascending: Sort values in ascending (default) or descending order.
      :type ascending: bool, default = True

      .. seealso:: :obj:`apply_permutation`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': [2, 2, 1], 'col2': [3, 4, 3], 'col3':[5, 6, 7]})
      >>> display(df)

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  1 |      2 |      4 |      6 |
      +----+--------+--------+--------+
      |  2 |      1 |      3 |      7 |
      +----+--------+--------+--------+

      >>> df.sort_values()

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      1 |      3 |      7 |
      +----+--------+--------+--------+
      |  1 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  2 |      2 |      4 |      6 |
      +----+--------+--------+--------+

      >>> df.sort_values("col3")

      +----+--------+--------+--------+
      |    |   col1 |   col2 |   col3 |
      +====+========+========+========+
      |  0 |      1 |      3 |      7 |
      +----+--------+--------+--------+
      |  1 |      2 |      3 |      5 |
      +----+--------+--------+--------+
      |  2 |      2 |      4 |      6 |
      +----+--------+--------+--------+



   .. py:method:: tail(n=5)

      Return the last `n` rows.

      This function returns the last `n` rows for the dataframe. It is
      useful for quickly testing if your object has the right type of data in
      it.

      :param n: Number of rows to select.
      :type n: int, default=5

      :returns: The last `n` rows of the DataFrame.
      :rtype: arkouda.dataframe.DataFrame

      .. seealso:: :obj:`arkouda.dataframe.head`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({'col1': ak.arange(10), 'col2': -1 * ak.arange(10)})
      >>> display(df)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      0 |      0 |
      +----+--------+--------+
      |  1 |      1 |     -1 |
      +----+--------+--------+
      |  2 |      2 |     -2 |
      +----+--------+--------+
      |  3 |      3 |     -3 |
      +----+--------+--------+
      |  4 |      4 |     -4 |
      +----+--------+--------+
      |  5 |      5 |     -5 |
      +----+--------+--------+
      |  6 |      6 |     -6 |
      +----+--------+--------+
      |  7 |      7 |     -7 |
      +----+--------+--------+
      |  8 |      8 |     -8 |
      +----+--------+--------+
      |  9 |      9 |     -9 |
      +----+--------+--------+

      >>> df.tail()

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      5 |     -5 |
      +----+--------+--------+
      |  1 |      6 |     -6 |
      +----+--------+--------+
      |  2 |      7 |     -7 |
      +----+--------+--------+
      |  3 |      8 |     -8 |
      +----+--------+--------+
      |  4 |      9 |     -9 |
      +----+--------+--------+

      >>> df.tail(n=2)

      +----+--------+--------+
      |    |   col1 |   col2 |
      +====+========+========+
      |  0 |      8 |     -8 |
      +----+--------+--------+
      |  1 |      9 |     -9 |
      +----+--------+--------+



   .. py:method:: to_csv(path: str, index: bool = False, columns: Optional[List[str]] = None, col_delim: str = ',', overwrite: bool = False)

      Writes DataFrame to CSV file(s). File will contain a column for each column in the DataFrame.
      All CSV Files written by Arkouda include a header denoting data types of the columns.
      Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      :param path: The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                   when they are written to disk.
      :type path: str
      :param index: If True, the index of the DataFrame will be written to the file
                    as a column.
      :type index: bool, default=False
      :param columns: Column names to assign when writing data.
      :type columns: list of str (Optional)
      :param col_delim: Value to be used to separate columns within the file.
                        Please be sure that the value used DOES NOT appear in your dataset.
      :type col_delim: str, default=","
      :param overwrite: If True, any existing files matching your provided prefix_path will
                        be overwritten. If False, an error will be returned if existing files are found.
      :type overwrite: bool, default=False

      :rtype: None

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist.
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server.

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations.
      - The column delimiter is expected to be the same for column names and data.
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline ("\\n") at this time.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'csv_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_csv(my_path + "/my_data")
      >>> df2 = DataFrame.read_csv(my_path + "/my_data" + "_LOCALE0000")
      >>> display(df2)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: to_hdf(path, index=False, columns=None, file_type='distribute')

      Save DataFrame to disk as hdf5, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: List, default = None
      :param file_type: Whether to save to a single file or distribute across Locales.
      :type file_type: str (single | distribute), default=distribute

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_parquet`, :obj:`load`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+



   .. py:method:: to_markdown(mode='wt', index=True, tablefmt='grid', storage_options=None, **kwargs)

      Print DataFrame in Markdown-friendly format.

      :param mode: Mode in which file is opened, "wt" by default.
      :type mode: str, optional
      :param index: Add index (row) labels.
      :type index: bool, optional, default True
      :param tablefmt: Table format to call from tablulate:
                       https://pypi.org/project/tabulate/
      :type tablefmt: str = "grid"
      :param storage_options: Extra options that make sense for a particular storage connection,
                              e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec,
                              e.g., starting s3://, gcs://.
                              An error will be raised if providing this argument with a non-fsspec URL.
                              See the fsspec and backend storage implementation docs for the set
                              of allowed keys and values.
      :type storage_options: dict, optional
      :param \*\*kwargs: These parameters will be passed to tabulate.

      .. note::

         This function should only be called on small DataFrames as it calls pandas.DataFrame.to_markdown:
         https://pandas.pydata.org/pandas-docs/version/1.2.4/reference/api/pandas.DataFrame.to_markdown.html

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"animal_1": ["elk", "pig"], "animal_2": ["dog", "quetzal"]})
      >>> print(df.to_markdown())
      +----+------------+------------+
      |    | animal_1   | animal_2   |
      +====+============+============+
      |  0 | elk        | dog        |
      +----+------------+------------+
      |  1 | pig        | quetzal    |
      +----+------------+------------+

      Suppress the index:

      >>> print(df.to_markdown(index = False))
      +------------+------------+
      | animal_1   | animal_2   |
      +============+============+
      | elk        | dog        |
      +------------+------------+
      | pig        | quetzal    |
      +------------+------------+





   .. py:method:: to_pandas(datalimit=1073741824, retain_index=False)

      Send this DataFrame to a pandas DataFrame.

      :param datalimit: The maximum number size, in megabytes to transfer. The requested
                        DataFrame will be converted to a pandas DataFrame only if the
                        estimated size of the DataFrame does not exceed this value.
      :type datalimit: int, default=arkouda.client.maxTransferBytes
      :param retain_index: Normally, to_pandas() creates a new range index object. If you want
                           to keep the index column, set this to True.
      :type retain_index: bool, default=False

      :returns: The result of converting this DataFrame to a pandas DataFrame.
      :rtype: pandas.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> ak_df = ak.DataFrame({"A": ak.arange(2), "B": -1 * ak.arange(2)})
      >>> type(ak_df)
      arkouda.dataframe.DataFrame
      >>> display(ak_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+

      >>> import pandas as pd
      >>> pd_df = ak_df.to_pandas()
      >>> type(pd_df)
      pandas.core.frame.DataFrame
      >>> display(pd_df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |  -1 |
      +----+-----+-----+



   .. py:method:: to_parquet(path, index=False, columns=None, compression: Optional[str] = None, convert_categoricals: bool = False)

      Save DataFrame to disk as parquet, preserving column names.

      :param path: File path to save data.
      :type path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: list
      :param compression: Provide the compression type to use when writing the file.
                          Supported values: snappy, gzip, brotli, zstd, lz4
      :type compression: str (Optional), default=None
      :param convert_categoricals: Parquet requires all columns to be the same size and Categoricals
                                   don't satisfy that requirement.
                                   If set, write the equivalent Strings in place of any Categorical columns.
      :type convert_categoricals: bool, default=False

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      This method saves one file per locale of the arkouda server. All
      files are prefixed by the path argument and suffixed by their
      locale number.

      .. seealso:: :obj:`to_hdf`, :obj:`load`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'parquet_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_parquet(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   B |   A |
      +====+=====+=====+
      |  0 |   3 |   1 |
      +----+-----+-----+
      |  1 |   4 |   2 |
      +----+-----+-----+



   .. py:method:: transfer(hostname, port)

      Sends a DataFrame to a different Arkouda server.

      :param hostname: The hostname where the Arkouda server intended to
                       receive the DataFrame is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :returns: A message indicating a complete transfer.
      :rtype: str

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister()

      Unregister this DataFrame object in the arkouda server which was previously
      registered using register() and/or attached to using attach().

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister.

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_dataframe_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister()
      >>> df.is_registered()
      False



   .. py:method:: unregister_dataframe_by_name(user_defined_name: str) -> str

      Function to unregister DataFrame object by name which was registered
      with the arkouda server via register().

      :param user_defined_name: Name under which the DataFrame object was registered.
      :type user_defined_name: str

      :raises TypeError: If user_defined_name is not a string.
      :raises RegistrationError: If there is an issue attempting to unregister any underlying components.

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Example

      >>> df = ak.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
      >>> df.register("my_table_name")
      >>> df.attach("my_table_name")
      >>> df.is_registered()
      True
      >>> df.unregister_dataframe_by_name("my_table_name")
      >>> df.is_registered()
      False



   .. py:method:: update_hdf(prefix_path: str, index=False, columns=None, repack: bool = True)

      Overwrite the dataset with the name provided with this dataframe. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share.
      :type prefix_path: str
      :param index: If True, save the index column. By default, do not save the index.
      :type index: bool, default=False
      :param columns: List of columns to include in the file. If None, writes out all columns.
      :type columns: List, default=None
      :param repack: HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool, default=True

      :returns: Success message if successful.
      :rtype: str

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      If the dataset provided does not exist, it will be added.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> import os.path
      >>> from pathlib import Path
      >>> my_path = os.path.join(os.getcwd(), 'hdf_output')
      >>> Path(my_path).mkdir(parents=True, exist_ok=True)

      >>> df = ak.DataFrame({"A":[1,2],"B":[3,4]})
      >>> df.to_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+

      >>> df2 = ak.DataFrame({"A":[5,6],"B":[7,8]})
      >>> df2.update_hdf(my_path + "/my_data")
      >>> df.load(my_path + "/my_data")

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   5 |   7 |
      +----+-----+-----+
      |  1 |   6 |   8 |
      +----+-----+-----+



   .. py:method:: update_nrows()

      Computes the number of rows on the arkouda server and updates the size parameter.




.. py:class:: DataFrameGroupBy

   A DataFrame that has been grouped by a subset of columns.

   :param gb_key_names: The column name(s) associated with the aggregated columns.
   :type gb_key_names: str or list(str), default=None
   :param as_index: If True, interpret aggregated column as index
                    (only implemented for single dimensional aggregates).
                    Otherwise, treat aggregated column as a dataframe column.
   :type as_index: bool, default=True

   .. attribute:: gb

      GroupBy object, where the aggregation keys are values of column(s) of a dataframe,
      usually in preparation for aggregating with respect to the other columns.

      :type: arkouda.groupbyclass.GroupBy

   .. attribute:: df

      The dataframe containing the original data.

      :type: arkouda.dataframe.DataFrame

   .. attribute:: gb_key_names

      The column name(s) associated with the aggregated columns.

      :type: str or list(str)

   .. attribute:: as_index

      If True the grouped values of the aggregation keys will be treated as an index.

      :type: bool, default=True


   .. py:method:: all(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: any(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: argmax(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: argmin(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: broadcast(x, permute=True)

      Fill each groups segment with a constant value.

      :param x: The values to put in each groups segment.
      :type x: Series or pdarray
      :param permute: If True (default), permute broadcast values back to the
                      ordering of the original array on which GroupBy was called.
                      If False, the broadcast values are grouped by value.
      :type permute: bool, default=True

      :returns: A Series with the Index of the original frame and the values of the broadcast.
      :rtype: arkouda.series.Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> from arkouda.dataframe import DataFrameGroupBy
      >>> df = ak.DataFrame({"A":[1,2,2,3],"B":[3,4,5,6]})

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+
      |  3 |   3 |   6 |
      +----+-----+-----+

      >>> gb = df.groupby("A")
      >>> x = ak.array([10,11,12])
      >>> s = DataFrameGroupBy.broadcast(gb, x)
      >>> df["C"] = s.values
      >>> display(df)

      +----+-----+-----+-----+
      |    |   A |   B |   C |
      +====+=====+=====+=====+
      |  0 |   1 |   3 |  10 |
      +----+-----+-----+-----+
      |  1 |   2 |   4 |  11 |
      +----+-----+-----+-----+
      |  2 |   2 |   5 |  11 |
      +----+-----+-----+-----+
      |  3 |   3 |   6 |  12 |
      +----+-----+-----+-----+



   .. py:method:: count(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: diff(colname)

      Create a difference aggregate for the given column.

      For each group, the difference between successive values is calculated.
      Aggregate operations (mean,min,max,std,var) can be done on the results.

      :param colname: Name of the column to compute the difference on.
      :type colname: str

      :returns: Object containing the differences, which can be aggregated.
      :rtype: DiffAggregate

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[1,2,2,2,3,3],"B":[3,9,11,27,86,100]})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   9 |
      +----+-----+-----+
      |  2 |   2 |  11 |
      +----+-----+-----+
      |  3 |   2 |  27 |
      +----+-----+-----+
      |  4 |   3 |  86 |
      +----+-----+-----+
      |  5 |   3 | 100 |
      +----+-----+-----+

      >>> gb = df.groupby("A")
      >>> gb.diff("B").values
      array([nan nan 2.00000000000000000 16.00000000000000000 nan 14.00000000000000000])



   .. py:method:: first(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: head(n: int = 5, sort_index: bool = True) -> DataFrame

      Return the first n rows from each group.

      :param n: Maximum number of rows to return for each group.
                If the number of rows in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param sort_index: If true, return the DataFrame with indices sorted.
      :type sort_index: bool, default = True

      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda import *
      >>> df = ak.DataFrame({"a":ak.arange(10) %3 , "b":ak.arange(10)})

      +----+-----+-----+
      |    |   a |   b |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |   1 |
      +----+-----+-----+
      |  2 |   2 |   2 |
      +----+-----+-----+
      |  3 |   0 |   3 |
      +----+-----+-----+
      |  4 |   1 |   4 |
      +----+-----+-----+
      |  5 |   2 |   5 |
      +----+-----+-----+
      |  6 |   0 |   6 |
      +----+-----+-----+
      |  7 |   1 |   7 |
      +----+-----+-----+
      |  8 |   2 |   8 |
      +----+-----+-----+
      |  9 |   0 |   9 |
      +----+-----+-----+

      >>> df.groupby("a").head(2)

      +----+-----+-----+
      |    |   a |   b |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   0 |   3 |
      +----+-----+-----+
      |  2 |   1 |   1 |
      +----+-----+-----+
      |  3 |   1 |   4 |
      +----+-----+-----+
      |  4 |   2 |   2 |
      +----+-----+-----+
      |  5 |   2 |   5 |
      +----+-----+-----+



   .. py:method:: max(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: mean(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: median(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: min(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: mode(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: nunique(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: prod(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: sample(n=None, frac=None, replace=False, weights=None, random_state=None)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the same row more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the underlying DataFrame
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional

      :returns: A new DataFrame containing items randomly sampled from each group
                sorted according to the grouped columns.
      :rtype: DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[3,1,2,1,2,3],"B":[3,4,5,6,7,8]})
      >>> display(df)
      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   3 |   3 |
      +----+-----+-----+
      |  1 |   1 |   4 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+
      |  3 |   1 |   6 |
      +----+-----+-----+
      |  4 |   2 |   7 |
      +----+-----+-----+
      |  5 |   3 |   8 |
      +----+-----+-----+

      >>> df.groupby("A").sample(random_state=6)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  3 |   1 |   6 |
      +----+-----+-----+
      |  4 |   2 |   7 |
      +----+-----+-----+
      |  5 |   3 |   8 |
      +----+-----+-----+

      >>> df.groupby("A").sample(frac=0.5, random_state=3, weights=ak.array([1,1,1,0,0,0]))

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  1 |   1 |   4 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+
      |  0 |   3 |   3 |
      +----+-----+-----+

      >>> df.groupby("A").sample(n=3, replace=True, random_state=ak.random.default_rng(7))
      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  1 |   1 |   4 |
      +----+-----+-----+
      |  3 |   1 |   6 |
      +----+-----+-----+
      |  1 |   1 |   4 |
      +----+-----+-----+
      |  4 |   2 |   7 |
      +----+-----+-----+
      |  4 |   2 |   7 |
      +----+-----+-----+
      |  4 |   2 |   7 |
      +----+-----+-----+
      |  0 |   3 |   3 |
      +----+-----+-----+
      |  5 |   3 |   8 |
      +----+-----+-----+
      |  5 |   3 |   8 |
      +----+-----+-----+



   .. py:method:: size(as_series=None, sort_index=True)

      Compute the size of each value as the total number of rows, including NaN values.

      :param as_series: Indicates whether to return arkouda.dataframe.DataFrame (if as_series = False) or
                        arkouda.series.Series (if as_series = True)
      :type as_series: bool, default=None
      :param sort_index: If True, results will be returned with index values sorted in ascending order.
      :type sort_index: bool, default=True

      :rtype: arkouda.dataframe.DataFrame or arkouda.series.Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> df = ak.DataFrame({"A":[1,2,2,3],"B":[3,4,5,6]})
      >>> display(df)

      +----+-----+-----+
      |    |   A |   B |
      +====+=====+=====+
      |  0 |   1 |   3 |
      +----+-----+-----+
      |  1 |   2 |   4 |
      +----+-----+-----+
      |  2 |   2 |   5 |
      +----+-----+-----+
      |  3 |   3 |   6 |
      +----+-----+-----+

      >>> df.groupby("A").size(as_series = False)

      +----+---------+
      |    |   size  |
      +====+=========+
      |  0 |       1 |
      +----+---------+
      |  1 |       2 |
      +----+---------+
      |  2 |       1 |
      +----+---------+



   .. py:method:: std(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: sum(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: tail(n: int = 5, sort_index: bool = True) -> DataFrame

      Return the last n rows from each group.

      :param n: Maximum number of rows to return for each group.
                If the number of rows in a group is less than n,
                all the rows from that group will be returned.
      :type n: int, optional, default = 5
      :param sort_index: If true, return the DataFrame with indices sorted.
      :type sort_index: bool, default = True

      :rtype: arkouda.dataframe.DataFrame

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> from arkouda import *
      >>> df = ak.DataFrame({"a":ak.arange(10) %3 , "b":ak.arange(10)})

      +----+-----+-----+
      |    |   a |   b |
      +====+=====+=====+
      |  0 |   0 |   0 |
      +----+-----+-----+
      |  1 |   1 |   1 |
      +----+-----+-----+
      |  2 |   2 |   2 |
      +----+-----+-----+
      |  3 |   0 |   3 |
      +----+-----+-----+
      |  4 |   1 |   4 |
      +----+-----+-----+
      |  5 |   2 |   5 |
      +----+-----+-----+
      |  6 |   0 |   6 |
      +----+-----+-----+
      |  7 |   1 |   7 |
      +----+-----+-----+
      |  8 |   2 |   8 |
      +----+-----+-----+
      |  9 |   0 |   9 |
      +----+-----+-----+

      >>> df.groupby("a").tail(2)

      +----+-----+-----+
      |    |   a |   b |
      +====+=====+=====+
      |  0 |   0 |   6 |
      +----+-----+-----+
      |  1 |   0 |   9 |
      +----+-----+-----+
      |  2 |   1 |   4 |
      +----+-----+-----+
      |  3 |   1 |   7 |
      +----+-----+-----+
      |  4 |   2 |   5 |
      +----+-----+-----+
      |  5 |   2 |   8 |
      +----+-----+-----+



   .. py:method:: unique(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: var(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



   .. py:method:: xor(colnames=None)

      Aggregate the operation, with the grouped column(s) values as keys.

      :param colnames: Column name or list of column names to compute the aggregation over.
      :type colnames: (list of) str, default=None

      :rtype: arkouda.dataframe.DataFrame



.. py:class:: DataSource

   DataSource(destpath='.')

   A generic data source file (file, http, ftp, ...).

   DataSources can be local files or remote files/URLs.  The files may
   also be compressed or uncompressed. DataSource hides some of the
   low-level details of downloading the file, allowing you to simply pass
   in a valid file path (or URL) and obtain a file object.

   :param destpath: Path to the directory where the source file gets downloaded to for
                    use.  If `destpath` is None, a temporary directory will be created.
                    The default path is the current directory.
   :type destpath: str or None, optional

   .. rubric:: Notes

   URLs require a scheme string (``http://``) to be used, without it they
   will fail::

       >>> repos = np.DataSource()
       >>> repos.exists('www.google.com/index.html')
       False
       >>> repos.exists('http://www.google.com/index.html')
       True

   Temporary directories are deleted when the DataSource is deleted.

   .. rubric:: Examples

   ::

       >>> ds = np.DataSource('/home/guido')
       >>> urlname = 'http://www.google.com/'
       >>> gfile = ds.open('http://www.google.com/')
       >>> ds.abspath(urlname)
       '/home/guido/www.google.com/index.html'

       >>> ds = np.DataSource(None)  # use with temporary file
       >>> ds.open('/home/guido/foobar.txt')
       <open file '/home/guido.foobar.txt', mode 'r' at 0x91d4430>
       >>> ds.abspath('/home/guido/foobar.txt')
       '/tmp/.../home/guido/foobar.txt'


   .. py:method:: abspath(path)

      Return absolute path of file in the DataSource directory.

      If `path` is an URL, then `abspath` will return either the location
      the file exists locally or the location it would exist when opened
      using the `open` method.

      :param path: Can be a local file or a remote URL.
      :type path: str

      :returns: **out** -- Complete path, including the `DataSource` destination directory.
      :rtype: str

      .. rubric:: Notes

      The functionality is based on `os.path.abspath`.



   .. py:method:: exists(path)

      Test if path exists.

      Test if `path` exists as (and in this order):

      - a local file.
      - a remote URL that has been downloaded and stored locally in the
        `DataSource` directory.
      - a remote URL that has not been downloaded, but is valid and
        accessible.

      :param path: Can be a local file or a remote URL.
      :type path: str

      :returns: **out** -- True if `path` exists.
      :rtype: bool

      .. rubric:: Notes

      When `path` is an URL, `exists` will return True if it's either
      stored locally in the `DataSource` directory, or is a valid remote
      URL.  `DataSource` does not discriminate between the two, the file
      is accessible if it exists in either location.



   .. py:method:: open(path, mode='r', encoding=None, newline=None)

      Open and return file-like object.

      If `path` is an URL, it will be downloaded, stored in the
      `DataSource` directory and opened from there.

      :param path: Local file path or URL to open.
      :type path: str
      :param mode: Mode to open `path`.  Mode 'r' for reading, 'w' for writing,
                   'a' to append. Available modes depend on the type of object
                   specified by `path`. Default is 'r'.
      :type mode: {'r', 'w', 'a'}, optional
      :param encoding: Open text file with given encoding. The default encoding will be
                       what `io.open` uses.
      :type encoding: {None, str}, optional
      :param newline: Newline to use when reading text file.
      :type newline: {None, str}, optional

      :returns: **out** -- File object.
      :rtype: file object



.. py:class:: DateTime64DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: Datetime(pda, unit: str = _BASE_UNIT)

   Bases: :py:obj:`_AbstractBaseTime`


   Represents a date and/or time.

   Datetime is the Arkouda analog to pandas DatetimeIndex and
   other timeseries data types.

   :param pda:
   :type pda: int64 pdarray, pd.DatetimeIndex, pd.Series, or np.datetime64 array
   :param unit: For int64 pdarray, denotes the unit of the input. Ignored for pandas
                and numpy arrays, which carry their own unit. Not case-sensitive;
                prefixes of full names (like 'sec') are accepted.

                Possible values:

                * 'weeks' or 'w'
                * 'days' or 'd'
                * 'hours' or 'h'
                * 'minutes', 'm', or 't'
                * 'seconds' or 's'
                * 'milliseconds', 'ms', or 'l'
                * 'microseconds', 'us', or 'u'
                * 'nanoseconds', 'ns', or 'n'

                Unlike in pandas, units cannot be combined or mixed with integers
   :type unit: str, default 'ns'

   .. rubric:: Notes

   The ``.values`` attribute is always in nanoseconds with int64 dtype.


   .. py:property:: date


   .. py:property:: day


   .. py:property:: day_of_week


   .. py:property:: day_of_year


   .. py:property:: dayofweek


   .. py:property:: dayofyear


   .. py:property:: hour


   .. py:property:: is_leap_year


   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isocalendar()


   .. py:property:: microsecond


   .. py:property:: millisecond


   .. py:property:: minute


   .. py:property:: month


   .. py:property:: nanosecond


   .. py:method:: register(user_defined_name)

      Register this Datetime object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Datetime is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Datetime which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Datetimes with the same name.
      :rtype: Datetime

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Datetimes with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: second


   .. py:attribute:: special_objType
      :value: 'Datetime'



   .. py:method:: sum()

      Return the sum of all elements in the array.



   .. py:attribute:: supported_opeq


   .. py:attribute:: supported_with_datetime


   .. py:attribute:: supported_with_pdarray


   .. py:attribute:: supported_with_r_datetime


   .. py:attribute:: supported_with_r_pdarray


   .. py:attribute:: supported_with_r_timedelta


   .. py:attribute:: supported_with_timedelta


   .. py:method:: to_pandas()

      Convert array to a pandas DatetimeIndex. Note: if the array size
      exceeds client.maxTransferBytes, a RuntimeError is raised.

      .. seealso:: :obj:`to_ndarray`



   .. py:method:: unregister()

      Unregister this Datetime object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: week


   .. py:property:: weekday


   .. py:property:: weekofyear


   .. py:property:: year


.. py:class:: Datetime(pda, unit: str = _BASE_UNIT)

   Bases: :py:obj:`_AbstractBaseTime`


   Represents a date and/or time.

   Datetime is the Arkouda analog to pandas DatetimeIndex and
   other timeseries data types.

   :param pda:
   :type pda: int64 pdarray, pd.DatetimeIndex, pd.Series, or np.datetime64 array
   :param unit: For int64 pdarray, denotes the unit of the input. Ignored for pandas
                and numpy arrays, which carry their own unit. Not case-sensitive;
                prefixes of full names (like 'sec') are accepted.

                Possible values:

                * 'weeks' or 'w'
                * 'days' or 'd'
                * 'hours' or 'h'
                * 'minutes', 'm', or 't'
                * 'seconds' or 's'
                * 'milliseconds', 'ms', or 'l'
                * 'microseconds', 'us', or 'u'
                * 'nanoseconds', 'ns', or 'n'

                Unlike in pandas, units cannot be combined or mixed with integers
   :type unit: str, default 'ns'

   .. rubric:: Notes

   The ``.values`` attribute is always in nanoseconds with int64 dtype.


   .. py:property:: date


   .. py:property:: day


   .. py:property:: day_of_week


   .. py:property:: day_of_year


   .. py:property:: dayofweek


   .. py:property:: dayofyear


   .. py:property:: hour


   .. py:property:: is_leap_year


   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isocalendar()


   .. py:property:: microsecond


   .. py:property:: millisecond


   .. py:property:: minute


   .. py:property:: month


   .. py:property:: nanosecond


   .. py:method:: register(user_defined_name)

      Register this Datetime object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Datetime is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Datetime which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Datetimes with the same name.
      :rtype: Datetime

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Datetimes with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: second


   .. py:attribute:: special_objType
      :value: 'Datetime'



   .. py:method:: sum()

      Return the sum of all elements in the array.



   .. py:attribute:: supported_opeq


   .. py:attribute:: supported_with_datetime


   .. py:attribute:: supported_with_pdarray


   .. py:attribute:: supported_with_r_datetime


   .. py:attribute:: supported_with_r_pdarray


   .. py:attribute:: supported_with_r_timedelta


   .. py:attribute:: supported_with_timedelta


   .. py:method:: to_pandas()

      Convert array to a pandas DatetimeIndex. Note: if the array size
      exceeds client.maxTransferBytes, a RuntimeError is raised.

      .. seealso:: :obj:`to_ndarray`



   .. py:method:: unregister()

      Unregister this Datetime object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: week


   .. py:property:: weekday


   .. py:property:: weekofyear


   .. py:property:: year


.. py:class:: Datetime(pda, unit: str = _BASE_UNIT)

   Bases: :py:obj:`_AbstractBaseTime`


   Represents a date and/or time.

   Datetime is the Arkouda analog to pandas DatetimeIndex and
   other timeseries data types.

   :param pda:
   :type pda: int64 pdarray, pd.DatetimeIndex, pd.Series, or np.datetime64 array
   :param unit: For int64 pdarray, denotes the unit of the input. Ignored for pandas
                and numpy arrays, which carry their own unit. Not case-sensitive;
                prefixes of full names (like 'sec') are accepted.

                Possible values:

                * 'weeks' or 'w'
                * 'days' or 'd'
                * 'hours' or 'h'
                * 'minutes', 'm', or 't'
                * 'seconds' or 's'
                * 'milliseconds', 'ms', or 'l'
                * 'microseconds', 'us', or 'u'
                * 'nanoseconds', 'ns', or 'n'

                Unlike in pandas, units cannot be combined or mixed with integers
   :type unit: str, default 'ns'

   .. rubric:: Notes

   The ``.values`` attribute is always in nanoseconds with int64 dtype.


   .. py:property:: date


   .. py:property:: day


   .. py:property:: day_of_week


   .. py:property:: day_of_year


   .. py:property:: dayofweek


   .. py:property:: dayofyear


   .. py:property:: hour


   .. py:property:: is_leap_year


   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isocalendar()


   .. py:property:: microsecond


   .. py:property:: millisecond


   .. py:property:: minute


   .. py:property:: month


   .. py:property:: nanosecond


   .. py:method:: register(user_defined_name)

      Register this Datetime object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Datetime is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Datetime which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Datetimes with the same name.
      :rtype: Datetime

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Datetimes with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: second


   .. py:attribute:: special_objType
      :value: 'Datetime'



   .. py:method:: sum()

      Return the sum of all elements in the array.



   .. py:attribute:: supported_opeq


   .. py:attribute:: supported_with_datetime


   .. py:attribute:: supported_with_pdarray


   .. py:attribute:: supported_with_r_datetime


   .. py:attribute:: supported_with_r_pdarray


   .. py:attribute:: supported_with_r_timedelta


   .. py:attribute:: supported_with_timedelta


   .. py:method:: to_pandas()

      Convert array to a pandas DatetimeIndex. Note: if the array size
      exceeds client.maxTransferBytes, a RuntimeError is raised.

      .. seealso:: :obj:`to_ndarray`



   .. py:method:: unregister()

      Unregister this Datetime object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: week


   .. py:property:: weekday


   .. py:property:: weekofyear


   .. py:property:: year


.. py:class:: DatetimeAccessor(series)

   Bases: :py:obj:`Properties`


   .. py:attribute:: series


.. py:class:: DiffAggregate

   A column in a GroupBy that has been differenced.
   Aggregation operations can be done on the result.

   .. attribute:: gb

      GroupBy object, where the aggregation keys are values of column(s) of a dataframe.

      :type: arkouda.groupbyclass.GroupBy

   .. attribute:: values

      A column to compute the difference on.

      :type: arkouda.series.Series.


   .. py:method:: all()


   .. py:method:: any()


   .. py:method:: argmax()


   .. py:method:: argmin()


   .. py:method:: count()


   .. py:method:: first()


   .. py:method:: max()


   .. py:method:: mean()


   .. py:method:: median()


   .. py:method:: min()


   .. py:method:: mode()


   .. py:method:: nunique()


   .. py:method:: prod()


   .. py:method:: std()


   .. py:method:: sum()


   .. py:method:: unique()


   .. py:method:: var()


   .. py:method:: xor()


.. py:class:: ErrorMode

   An enumeration.



   .. py:method:: ignore(*args, **kwargs)

      An enumeration.




   .. py:method:: name(*args, **kwargs)

      The name of the Enum member.




   .. py:method:: return_validity(*args, **kwargs)

      An enumeration.




   .. py:method:: strict(*args, **kwargs)

      An enumeration.




   .. py:method:: value(*args, **kwargs)

      The value of the Enum member.




.. py:class:: False_(value)

   Bases: :py:obj:`numpy.generic`


   Boolean type (True or False), stored as a byte.

       .. warning::

          The :class:`bool_` type is not a subclass of the :class:`int_` type
          (the :class:`bool_` is not even a number type). This is different
          than Python's default implementation of :class:`bool` as a
          sub-class of :class:`int`.

       :Character code: ``'?'``



.. py:class:: Fields(values, names, MSB_left=True, pad='-', separator='', show_int=True)

   Bases: :py:obj:`BitVector`


   An integer-backed representation of a set of named binary fields, e.g. flags.

   :param values: The array of field values. If (u)int64, the values are used as-is for the
                  binary representation of fields. If Strings, the values are converted
                  to binary according to the mapping defined by the names and MSB_left
                  arguments.
   :type values: pdarray or Strings
   :param names: The names of the fields, in order. A string will be treated as a list
                 of single-character field names. Multi-character field names are allowed,
                 but must be passed as a list or tuple and user must specify a separator.
   :type names: str or sequence of str
   :param MSB_left: Controls how field names are mapped to binary values. If True (default),
                    the left-most field name corresponds to the most significant bit in the
                    binary representation. If False, the left-most field name corresponds to
                    the least significant bit.
   :type MSB_left: bool
   :param pad: Character to display when field is not present. Use empty string if no
               padding is desired.
   :type pad: str
   :param separator: Substring that separates fields. Used to parse input values (if ak.Strings)
                     and to display output.
   :type separator: str
   :param show_int: If True (default), display the integer value of the binary fields in output.
   :type show_int: bool

   :returns: **fields** -- The array of field values
   :rtype: Fields

   .. rubric:: Notes

   This class is a thin wrapper around pdarray that mostly affects
   how values are displayed to the user. Operators and methods will
   typically treat this class like an int64 pdarray.


   .. py:attribute:: MSB_left
      :value: True



   .. py:method:: format(x)

      Format a single binary value as a string of named fields.



   .. py:attribute:: name
      :value: None



   .. py:attribute:: names


   .. py:attribute:: namewidth


   .. py:method:: opeq(other, op)


   .. py:attribute:: pad


   .. py:attribute:: padchar
      :value: '-'



   .. py:attribute:: separator
      :value: ''



   .. py:attribute:: show_int
      :value: True



.. py:class:: Float16DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: Float32DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: Float64DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: GROUPBY_REDUCTION_TYPES

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: GroupBy

   Group an array or list of arrays by value, usually in preparation
   for aggregating the within-group values of another array.

   :param keys: The array to group by value, or if list, the column arrays to group by row
   :type keys: (list of) pdarray, Strings, or Categorical
   :param assume_sorted: If True, assume keys is already sorted (Default: False)
   :type assume_sorted: bool

   .. attribute:: nkeys

      The number of key arrays (columns)

      :type: int

   .. attribute:: size

      The length of the input array(s), i.e. number of rows

      :type: int

   .. attribute:: permutation

      The permutation that sorts the keys array(s) by value (row)

      :type: pdarray

   .. attribute:: unique_keys

      The unique values of the keys array(s), in grouped order

      :type: (list of) pdarray, Strings, or Categorical

   .. attribute:: ngroups

      The length of the unique_keys array(s), i.e. number of groups

      :type: int

   .. attribute:: segments

      The start index of each group in the grouped array(s)

      :type: pdarray

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. attribute:: dropna

      If True, and the groupby keys contain NaN values,
      the NaN values together with the corresponding row will be dropped.
      Otherwise, the rows corresponding to NaN values will be kept.

      :type: bool (default=True)

   :raises TypeError: Raised if keys is a pdarray with a dtype other than int64

   .. rubric:: Notes

   Integral pdarrays, Strings, and Categoricals are natively supported, but
   float64 and bool arrays are not.

   For a user-defined class to be groupable, it must inherit from pdarray
   and define or overload the grouping API:
     1) a ._get_grouping_keys() method that returns a list of pdarrays
        that can be (co)argsorted.
     2) (Optional) a .group() method that returns the permutation that
        groups the array
   If the input is a single array with a .group() method defined, method 2
   will be used; otherwise, method 1 will be used.


   .. py:method:: AND(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise AND of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise AND reduction on
      each group.

      :param values: The values to group and reduce with AND
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise AND of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: OR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise OR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise OR reduction on
      each group.

      :param values: The values to group and reduce with OR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise OR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: Reductions(*args, **kwargs)

      frozenset() -> empty frozenset object
      frozenset(iterable) -> frozenset object

      Build an immutable unordered collection of unique elements.




   .. py:method:: XOR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise XOR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise XOR reduction on
      each group.

      :param values: The values to group and reduce with XOR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise XOR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: aggregate(values: groupable, operator: str, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, groupable]

      Using the permutation stored in the GroupBy instance, group another
      array of values and apply a reduction to each group's values.

      :param values: The values to group and reduce
      :type values: pdarray
      :param operator: The name of the reduction operator to use
      :type operator: str
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **aggregates** (*groupable*) -- One aggregate value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if the requested operator is not supported for the
          values dtype

      .. rubric:: Examples

      >>> keys = ak.arange(0, 10)
      >>> vals = ak.linspace(-1, 1, 10)
      >>> g = ak.GroupBy(keys)
      >>> g.aggregate(vals, 'sum')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777768,
      -0.55555555555555536, -0.33333333333333348, -0.11111111111111116,
      0.11111111111111116, 0.33333333333333348, 0.55555555555555536, 0.77777777777777768,
      1]))
      >>> g.aggregate(vals, 'min')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777779,
      -0.55555555555555558, -0.33333333333333337, -0.11111111111111116, 0.11111111111111116,
      0.33333333333333326, 0.55555555555555536, 0.77777777777777768, 1]))



   .. py:method:: all(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform an "and" reduction on
      each group.

      :param values: The values to group and reduce with "and"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: any(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and perform an "or" reduction on each group.

      :param values: The values to group and reduce with "or"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array



   .. py:method:: argmax(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      maximum of each group's values.

      :param values: The values to group and find argmax
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argmaxima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The returned indices refer to the original values array as passed in,
      not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmax(b)
      (array([2, 3, 4]), array([9, 3, 2]))



   .. py:method:: argmin(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      minimum of each group's values.

      :param values: The values to group and find argmin
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argminima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values
          size or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if argmin is not supported for the values dtype

      .. rubric:: Notes

      The returned indices refer to the original values array as
      passed in, not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmin(b)
      (array([2, 3, 4]), array([5, 4, 2]))



   .. py:method:: attach(user_defined_name: str) -> GroupBy

      Function to return a GroupBy object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which GroupBy object was registered under
      :type user_defined_name: str

      :returns: The GroupBy object created by re-attaching to the corresponding server components
      :rtype: GroupBy

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_groupby_by_name`



   .. py:method:: broadcast(values: Union[pdarray, Strings], permute: bool = True) -> Union[pdarray, Strings]

      Fill each group's segment with a constant value.

      :param values: The values to put in each group's segment
      :type values: pdarray, Strings
      :param permute: If True (default), permute broadcast values back to the ordering
                      of the original array on which GroupBy was called. If False, the
                      broadcast values are grouped by value.
      :type permute: bool

      :returns: The broadcasted values
      :rtype: pdarray, Strings

      :raises TypeError: Raised if value is not a pdarray object
      :raises ValueError: Raised if the values array does not have one
          value per segment

      .. rubric:: Notes

      This function is a sparse analog of ``np.broadcast``. If a
      GroupBy object represents a sparse matrix (tensor), then
      this function takes a (dense) column vector and replicates
      each value to the non-zero elements in the corresponding row.

      .. rubric:: Examples

      >>> a = ak.array([0, 1, 0, 1, 0])
      >>> values = ak.array([3, 5])
      >>> g = ak.GroupBy(a)
      # By default, result is in original order
      >>> g.broadcast(values)
      array([3, 5, 3, 5, 3])
      # With permute=False, result is in grouped order
      >>> g.broadcast(values, permute=False)
      array([3, 3, 3, 5, 5]
      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 1, 4, 4, 4, 1, 3, 3, 2, 2])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> g.broadcast(counts > 2)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts == 3)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts < 4)
      array([True True True True True True True True True True])



   .. py:method:: build_from_components(user_defined_name: Optional[str] = None, **kwargs) -> GroupBy

      function to build a new GroupBy object from component keys and permutation.

      :param user_defined_name: and assign it the given name
      :type user_defined_name: str (Optional) Passing a name will init the new GroupBy
      :param kwargs: Expected keys are "orig_keys", "permutation", "unique_keys", and "segments"
      :type kwargs: dict Dictionary of components required for rebuilding the GroupBy.

      :returns: The GroupBy object created by using the given components
      :rtype: GroupBy



   .. py:method:: count(values: pdarray) -> Tuple[groupable, pdarray]

      Count the number of elements in each group.  NaN values will be excluded from the total.

      :param values: The values to be count by group (excluding NaN values).
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears (excluding NaN values).

      .. rubric:: Examples

      >>> a = ak.array([1, 0, -1, 1, 0, -1])
      >>> a
      array([1 0 -1 1 0 -1])
      >>> b = ak.array([1, np.nan, -1, np.nan, np.nan, -1], dtype = "float64")
      >>> b
      array([1.00000000000000000 nan -1.00000000000000000 nan nan -1.00000000000000000])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.count(b)
      >>> keys
      array([-1 0 1])
      >>> counts
      array([2 0 1])



   .. py:method:: first(values: groupable_element_type) -> Tuple[groupable, groupable_element_type]

      First value in each group.

      :param values: The values from which to take the first of each group
      :type values: pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first value of each group



   .. py:method:: from_return_msg(rep_msg)


   .. py:method:: head(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the first n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.head(v, 2, return_indices=True)
      >>> _, values = g.head(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([0 3 1 4 2 5])
      >>> values
      array([0 3 1 4 2 5])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.head(v2, 2, return_indices=True)
      >>> _, values2 = g.head(v2, 2, return_indices=False)
      >>> idx2
      array([0 3 1 4 2 5])
      >>> values2
      array([0 -6 -2 -8 -4 -10])



   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_groupby_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: max(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the maximum of each
      group's values.

      :param values: The values to group and find maxima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_maxima** (*pdarray*) -- One maximum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if max is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if max is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.max(b)
      (array([2, 3, 4]), array([4, 4, 3]))



   .. py:method:: mean(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the mean of each group's
      values.

      :param values: The values to group and average
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_means** (*pdarray, float64*) -- One mean value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.mean(b)
      (array([2, 3, 4]), array([2.6666666666666665, 2.7999999999999998, 3]))



   .. py:method:: median(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the median of each group's
      values.

      :param values: The values to group and find median
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_medians** (*pdarray, float64*) -- One median value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,9)
      >>> a
      array([4 1 4 3 2 2 2 3 3])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([4 1 4 3 2 2 2 3 3])
      >>> b = ak.linspace(-5,5,9)
      >>> b
      array([-5 -3.75 -2.5 -1.25 0 1.25 2.5 3.75 5])
      >>> g.median(b)
      (array([1 2 3 4]), array([-3.75 1.25 3.75 -3.75]))



   .. py:method:: min(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the minimum of each group's
      values.

      :param values: The values to group and find minima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_minima** (*pdarray*) -- One minimum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if min is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if min is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.min(b)
      (array([2, 3, 4]), array([1, 1, 3]))



   .. py:method:: mode(values: groupable) -> Tuple[groupable, groupable]

      Most common value in each group. If a group is multi-modal, return the
      modal value that occurs first.

      :param values: The values from which to take the mode of each group
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) pdarray-like*) -- The most common value of each group



   .. py:method:: most_common(values)

      (Deprecated) See `GroupBy.mode()`.




   .. py:method:: nunique(values: groupable) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and return the number of unique values in each group.

      :param values: The values to group and find unique values
      :type values: pdarray, int64

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **group_nunique** (*groupable*) -- Number of unique values per unique key in the GroupBy instance

      :raises TypeError: Raised if the dtype(s) of values array(s) does/do not support
          the nunique method
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if nunique is not supported for the values dtype

      .. rubric:: Examples

      >>> data = ak.array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> data
      array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> labels = ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> labels
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g = ak.GroupBy(labels)
      >>> g.keys
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g.nunique(data)
      array([1,2,3,4]), array([2, 2, 3, 1])
      #    Group (1,1,1) has values [3,4,3] -> there are 2 unique values 3&4
      #    Group (2,2,2) has values [1,1,4] -> 2 unique values 1&4
      #    Group (3,3,3) has values [3,4,1] -> 3 unique values
      #    Group (4) has values [4] -> 1 unique value



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: prod(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the product of each group's
      values.

      :param values: The values to group and multiply
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_products** (*pdarray, float64*) -- One product per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if prod is not supported for the values dtype

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.prod(b)
      (array([2, 3, 4]), array([12, 108.00000000000003, 8.9999999999999982]))



   .. py:method:: register(user_defined_name: str) -> GroupBy

      Register this GroupBy object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the GroupBy is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same GroupBy which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different GroupBys with the same name.
      :rtype: GroupBy

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the GroupBy with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: sample(values: groupable, n=None, frac=None, replace=False, weights=None, random_state=None, return_indices=False, permute_samples=False)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param values: The values from which to sample, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the value more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the groupby keys
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional
      :param return_indices: if True, return the indices of the sampled values.
                             Otherwise, return the sample values.
      :type return_indices: bool, default False
      :param permute_samples: if True, return permute the samples according to group
                              Otherwise, keep samples in original order.
      :type permute_samples: bool, default False

      :returns: if return_indices is True, return the indices of the sampled values.
                Otherwise, return the sample values.
      :rtype: pdarray



   .. py:method:: size() -> Tuple[groupable, pdarray]

      Count the number of elements in each group, i.e. the number of times
      each key appears.  This counts the total number of rows (including NaN values).

      :param none:

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears

      .. seealso:: :obj:`count`

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 2, 3, 1, 2, 4, 3, 4, 3, 4])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> keys
      array([1, 2, 3, 4])
      >>> counts
      array([1, 2, 4, 3])



   .. py:method:: std(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the standard deviation of
      each group's values.

      :param values: The values to group and find standard deviation
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_stds** (*pdarray, float64*) -- One std value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The standard deviation is the square root of the average of the squared
      deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

      The average squared deviation is normally calculated as
      ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
      the divisor ``N - ddof`` is used instead. In standard statistical
      practice, ``ddof=1`` provides an unbiased estimator of the variance
      of the infinite population. ``ddof=0`` provides a maximum likelihood
      estimate of the variance for normally distributed variables. The
      standard deviation computed in this function is the square root of
      the estimated variance, so even with ``ddof=1``, it will not be an
      unbiased estimate of the standard deviation per se.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.std(b)
      (array([2 3 4]), array([1.5275252316519465 1.0954451150103321 0]))



   .. py:method:: sum(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and sum each group's values.

      :param values: The values to group and sum
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_sums** (*pdarray*) -- One sum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The grouped sum of a boolean ``pdarray`` returns integers.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.sum(b)
      (array([2, 3, 4]), array([8, 14, 6]))



   .. py:method:: tail(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the last n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The last n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.tail(v, 2, return_indices=True)
      >>> _, values = g.tail(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([6 9 4 7 5 8])
      >>> values
      array([6 9 4 7 5 8])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.tail(v2, 2, return_indices=True)
      >>> _, values2 = g.tail(v2, 2, return_indices=False)
      >>> idx2
      array([6 9 4 7 5 8])
      >>> values2
      array([-12 -18 -8 -14 -10 -16])



   .. py:method:: to_hdf(prefix_path, dataset='groupby', mode='truncate', file_type='distribute')

      Save the GroupBy to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :returns: * *None*
                * *GroupBy is not currently supported by Parquet*



   .. py:method:: unique(values: groupable)

      Return the set of unique values in each group, as a SegArray.

      :param values: The values to unique
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) SegArray*) -- The unique values of each group

      :raises TypeError: Raised if values is or contains Strings or Categorical



   .. py:method:: unregister()

      Unregister this GroupBy object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_groupby_by_name(user_defined_name: str) -> None

      Function to unregister GroupBy object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the GroupBy object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'groupby', repack: bool = True)


   .. py:method:: var(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the variance of
      each group's values.

      :param values: The values to group and find variance
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_vars** (*pdarray, float64*) -- One var value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The variance is the average of the squared deviations from the mean,
      i.e.,  ``var = mean((x - x.mean())**2)``.

      The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
      If, however, `ddof` is specified, the divisor ``N - ddof`` is used
      instead.  In standard statistical practice, ``ddof=1`` provides an
      unbiased estimator of the variance of a hypothetical infinite population.
      ``ddof=0`` provides a maximum likelihood estimate of the variance for
      normally distributed variables.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.var(b)
      (array([2 3 4]), array([2.333333333333333 1.2 0]))



.. py:class:: GroupBy

   Group an array or list of arrays by value, usually in preparation
   for aggregating the within-group values of another array.

   :param keys: The array to group by value, or if list, the column arrays to group by row
   :type keys: (list of) pdarray, Strings, or Categorical
   :param assume_sorted: If True, assume keys is already sorted (Default: False)
   :type assume_sorted: bool

   .. attribute:: nkeys

      The number of key arrays (columns)

      :type: int

   .. attribute:: size

      The length of the input array(s), i.e. number of rows

      :type: int

   .. attribute:: permutation

      The permutation that sorts the keys array(s) by value (row)

      :type: pdarray

   .. attribute:: unique_keys

      The unique values of the keys array(s), in grouped order

      :type: (list of) pdarray, Strings, or Categorical

   .. attribute:: ngroups

      The length of the unique_keys array(s), i.e. number of groups

      :type: int

   .. attribute:: segments

      The start index of each group in the grouped array(s)

      :type: pdarray

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. attribute:: dropna

      If True, and the groupby keys contain NaN values,
      the NaN values together with the corresponding row will be dropped.
      Otherwise, the rows corresponding to NaN values will be kept.

      :type: bool (default=True)

   :raises TypeError: Raised if keys is a pdarray with a dtype other than int64

   .. rubric:: Notes

   Integral pdarrays, Strings, and Categoricals are natively supported, but
   float64 and bool arrays are not.

   For a user-defined class to be groupable, it must inherit from pdarray
   and define or overload the grouping API:
     1) a ._get_grouping_keys() method that returns a list of pdarrays
        that can be (co)argsorted.
     2) (Optional) a .group() method that returns the permutation that
        groups the array
   If the input is a single array with a .group() method defined, method 2
   will be used; otherwise, method 1 will be used.


   .. py:method:: AND(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise AND of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise AND reduction on
      each group.

      :param values: The values to group and reduce with AND
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise AND of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: OR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise OR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise OR reduction on
      each group.

      :param values: The values to group and reduce with OR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise OR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: Reductions(*args, **kwargs)

      frozenset() -> empty frozenset object
      frozenset(iterable) -> frozenset object

      Build an immutable unordered collection of unique elements.




   .. py:method:: XOR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise XOR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise XOR reduction on
      each group.

      :param values: The values to group and reduce with XOR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise XOR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: aggregate(values: groupable, operator: str, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, groupable]

      Using the permutation stored in the GroupBy instance, group another
      array of values and apply a reduction to each group's values.

      :param values: The values to group and reduce
      :type values: pdarray
      :param operator: The name of the reduction operator to use
      :type operator: str
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **aggregates** (*groupable*) -- One aggregate value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if the requested operator is not supported for the
          values dtype

      .. rubric:: Examples

      >>> keys = ak.arange(0, 10)
      >>> vals = ak.linspace(-1, 1, 10)
      >>> g = ak.GroupBy(keys)
      >>> g.aggregate(vals, 'sum')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777768,
      -0.55555555555555536, -0.33333333333333348, -0.11111111111111116,
      0.11111111111111116, 0.33333333333333348, 0.55555555555555536, 0.77777777777777768,
      1]))
      >>> g.aggregate(vals, 'min')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777779,
      -0.55555555555555558, -0.33333333333333337, -0.11111111111111116, 0.11111111111111116,
      0.33333333333333326, 0.55555555555555536, 0.77777777777777768, 1]))



   .. py:method:: all(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform an "and" reduction on
      each group.

      :param values: The values to group and reduce with "and"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: any(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and perform an "or" reduction on each group.

      :param values: The values to group and reduce with "or"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array



   .. py:method:: argmax(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      maximum of each group's values.

      :param values: The values to group and find argmax
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argmaxima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The returned indices refer to the original values array as passed in,
      not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmax(b)
      (array([2, 3, 4]), array([9, 3, 2]))



   .. py:method:: argmin(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      minimum of each group's values.

      :param values: The values to group and find argmin
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argminima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values
          size or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if argmin is not supported for the values dtype

      .. rubric:: Notes

      The returned indices refer to the original values array as
      passed in, not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmin(b)
      (array([2, 3, 4]), array([5, 4, 2]))



   .. py:method:: attach(user_defined_name: str) -> GroupBy

      Function to return a GroupBy object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which GroupBy object was registered under
      :type user_defined_name: str

      :returns: The GroupBy object created by re-attaching to the corresponding server components
      :rtype: GroupBy

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_groupby_by_name`



   .. py:method:: broadcast(values: Union[pdarray, Strings], permute: bool = True) -> Union[pdarray, Strings]

      Fill each group's segment with a constant value.

      :param values: The values to put in each group's segment
      :type values: pdarray, Strings
      :param permute: If True (default), permute broadcast values back to the ordering
                      of the original array on which GroupBy was called. If False, the
                      broadcast values are grouped by value.
      :type permute: bool

      :returns: The broadcasted values
      :rtype: pdarray, Strings

      :raises TypeError: Raised if value is not a pdarray object
      :raises ValueError: Raised if the values array does not have one
          value per segment

      .. rubric:: Notes

      This function is a sparse analog of ``np.broadcast``. If a
      GroupBy object represents a sparse matrix (tensor), then
      this function takes a (dense) column vector and replicates
      each value to the non-zero elements in the corresponding row.

      .. rubric:: Examples

      >>> a = ak.array([0, 1, 0, 1, 0])
      >>> values = ak.array([3, 5])
      >>> g = ak.GroupBy(a)
      # By default, result is in original order
      >>> g.broadcast(values)
      array([3, 5, 3, 5, 3])
      # With permute=False, result is in grouped order
      >>> g.broadcast(values, permute=False)
      array([3, 3, 3, 5, 5]
      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 1, 4, 4, 4, 1, 3, 3, 2, 2])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> g.broadcast(counts > 2)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts == 3)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts < 4)
      array([True True True True True True True True True True])



   .. py:method:: build_from_components(user_defined_name: Optional[str] = None, **kwargs) -> GroupBy

      function to build a new GroupBy object from component keys and permutation.

      :param user_defined_name: and assign it the given name
      :type user_defined_name: str (Optional) Passing a name will init the new GroupBy
      :param kwargs: Expected keys are "orig_keys", "permutation", "unique_keys", and "segments"
      :type kwargs: dict Dictionary of components required for rebuilding the GroupBy.

      :returns: The GroupBy object created by using the given components
      :rtype: GroupBy



   .. py:method:: count(values: pdarray) -> Tuple[groupable, pdarray]

      Count the number of elements in each group.  NaN values will be excluded from the total.

      :param values: The values to be count by group (excluding NaN values).
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears (excluding NaN values).

      .. rubric:: Examples

      >>> a = ak.array([1, 0, -1, 1, 0, -1])
      >>> a
      array([1 0 -1 1 0 -1])
      >>> b = ak.array([1, np.nan, -1, np.nan, np.nan, -1], dtype = "float64")
      >>> b
      array([1.00000000000000000 nan -1.00000000000000000 nan nan -1.00000000000000000])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.count(b)
      >>> keys
      array([-1 0 1])
      >>> counts
      array([2 0 1])



   .. py:method:: first(values: groupable_element_type) -> Tuple[groupable, groupable_element_type]

      First value in each group.

      :param values: The values from which to take the first of each group
      :type values: pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first value of each group



   .. py:method:: from_return_msg(rep_msg)


   .. py:method:: head(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the first n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.head(v, 2, return_indices=True)
      >>> _, values = g.head(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([0 3 1 4 2 5])
      >>> values
      array([0 3 1 4 2 5])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.head(v2, 2, return_indices=True)
      >>> _, values2 = g.head(v2, 2, return_indices=False)
      >>> idx2
      array([0 3 1 4 2 5])
      >>> values2
      array([0 -6 -2 -8 -4 -10])



   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_groupby_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: max(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the maximum of each
      group's values.

      :param values: The values to group and find maxima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_maxima** (*pdarray*) -- One maximum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if max is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if max is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.max(b)
      (array([2, 3, 4]), array([4, 4, 3]))



   .. py:method:: mean(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the mean of each group's
      values.

      :param values: The values to group and average
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_means** (*pdarray, float64*) -- One mean value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.mean(b)
      (array([2, 3, 4]), array([2.6666666666666665, 2.7999999999999998, 3]))



   .. py:method:: median(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the median of each group's
      values.

      :param values: The values to group and find median
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_medians** (*pdarray, float64*) -- One median value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,9)
      >>> a
      array([4 1 4 3 2 2 2 3 3])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([4 1 4 3 2 2 2 3 3])
      >>> b = ak.linspace(-5,5,9)
      >>> b
      array([-5 -3.75 -2.5 -1.25 0 1.25 2.5 3.75 5])
      >>> g.median(b)
      (array([1 2 3 4]), array([-3.75 1.25 3.75 -3.75]))



   .. py:method:: min(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the minimum of each group's
      values.

      :param values: The values to group and find minima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_minima** (*pdarray*) -- One minimum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if min is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if min is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.min(b)
      (array([2, 3, 4]), array([1, 1, 3]))



   .. py:method:: mode(values: groupable) -> Tuple[groupable, groupable]

      Most common value in each group. If a group is multi-modal, return the
      modal value that occurs first.

      :param values: The values from which to take the mode of each group
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) pdarray-like*) -- The most common value of each group



   .. py:method:: most_common(values)

      (Deprecated) See `GroupBy.mode()`.




   .. py:method:: nunique(values: groupable) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and return the number of unique values in each group.

      :param values: The values to group and find unique values
      :type values: pdarray, int64

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **group_nunique** (*groupable*) -- Number of unique values per unique key in the GroupBy instance

      :raises TypeError: Raised if the dtype(s) of values array(s) does/do not support
          the nunique method
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if nunique is not supported for the values dtype

      .. rubric:: Examples

      >>> data = ak.array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> data
      array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> labels = ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> labels
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g = ak.GroupBy(labels)
      >>> g.keys
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g.nunique(data)
      array([1,2,3,4]), array([2, 2, 3, 1])
      #    Group (1,1,1) has values [3,4,3] -> there are 2 unique values 3&4
      #    Group (2,2,2) has values [1,1,4] -> 2 unique values 1&4
      #    Group (3,3,3) has values [3,4,1] -> 3 unique values
      #    Group (4) has values [4] -> 1 unique value



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: prod(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the product of each group's
      values.

      :param values: The values to group and multiply
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_products** (*pdarray, float64*) -- One product per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if prod is not supported for the values dtype

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.prod(b)
      (array([2, 3, 4]), array([12, 108.00000000000003, 8.9999999999999982]))



   .. py:method:: register(user_defined_name: str) -> GroupBy

      Register this GroupBy object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the GroupBy is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same GroupBy which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different GroupBys with the same name.
      :rtype: GroupBy

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the GroupBy with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: sample(values: groupable, n=None, frac=None, replace=False, weights=None, random_state=None, return_indices=False, permute_samples=False)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param values: The values from which to sample, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the value more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the groupby keys
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional
      :param return_indices: if True, return the indices of the sampled values.
                             Otherwise, return the sample values.
      :type return_indices: bool, default False
      :param permute_samples: if True, return permute the samples according to group
                              Otherwise, keep samples in original order.
      :type permute_samples: bool, default False

      :returns: if return_indices is True, return the indices of the sampled values.
                Otherwise, return the sample values.
      :rtype: pdarray



   .. py:method:: size() -> Tuple[groupable, pdarray]

      Count the number of elements in each group, i.e. the number of times
      each key appears.  This counts the total number of rows (including NaN values).

      :param none:

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears

      .. seealso:: :obj:`count`

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 2, 3, 1, 2, 4, 3, 4, 3, 4])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> keys
      array([1, 2, 3, 4])
      >>> counts
      array([1, 2, 4, 3])



   .. py:method:: std(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the standard deviation of
      each group's values.

      :param values: The values to group and find standard deviation
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_stds** (*pdarray, float64*) -- One std value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The standard deviation is the square root of the average of the squared
      deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

      The average squared deviation is normally calculated as
      ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
      the divisor ``N - ddof`` is used instead. In standard statistical
      practice, ``ddof=1`` provides an unbiased estimator of the variance
      of the infinite population. ``ddof=0`` provides a maximum likelihood
      estimate of the variance for normally distributed variables. The
      standard deviation computed in this function is the square root of
      the estimated variance, so even with ``ddof=1``, it will not be an
      unbiased estimate of the standard deviation per se.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.std(b)
      (array([2 3 4]), array([1.5275252316519465 1.0954451150103321 0]))



   .. py:method:: sum(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and sum each group's values.

      :param values: The values to group and sum
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_sums** (*pdarray*) -- One sum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The grouped sum of a boolean ``pdarray`` returns integers.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.sum(b)
      (array([2, 3, 4]), array([8, 14, 6]))



   .. py:method:: tail(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the last n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The last n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.tail(v, 2, return_indices=True)
      >>> _, values = g.tail(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([6 9 4 7 5 8])
      >>> values
      array([6 9 4 7 5 8])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.tail(v2, 2, return_indices=True)
      >>> _, values2 = g.tail(v2, 2, return_indices=False)
      >>> idx2
      array([6 9 4 7 5 8])
      >>> values2
      array([-12 -18 -8 -14 -10 -16])



   .. py:method:: to_hdf(prefix_path, dataset='groupby', mode='truncate', file_type='distribute')

      Save the GroupBy to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :returns: * *None*
                * *GroupBy is not currently supported by Parquet*



   .. py:method:: unique(values: groupable)

      Return the set of unique values in each group, as a SegArray.

      :param values: The values to unique
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) SegArray*) -- The unique values of each group

      :raises TypeError: Raised if values is or contains Strings or Categorical



   .. py:method:: unregister()

      Unregister this GroupBy object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_groupby_by_name(user_defined_name: str) -> None

      Function to unregister GroupBy object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the GroupBy object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'groupby', repack: bool = True)


   .. py:method:: var(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the variance of
      each group's values.

      :param values: The values to group and find variance
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_vars** (*pdarray, float64*) -- One var value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The variance is the average of the squared deviations from the mean,
      i.e.,  ``var = mean((x - x.mean())**2)``.

      The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
      If, however, `ddof` is specified, the divisor ``N - ddof`` is used
      instead.  In standard statistical practice, ``ddof=1`` provides an
      unbiased estimator of the variance of a hypothetical infinite population.
      ``ddof=0`` provides a maximum likelihood estimate of the variance for
      normally distributed variables.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.var(b)
      (array([2 3 4]), array([2.333333333333333 1.2 0]))



.. py:class:: GroupBy

   Group an array or list of arrays by value, usually in preparation
   for aggregating the within-group values of another array.

   :param keys: The array to group by value, or if list, the column arrays to group by row
   :type keys: (list of) pdarray, Strings, or Categorical
   :param assume_sorted: If True, assume keys is already sorted (Default: False)
   :type assume_sorted: bool

   .. attribute:: nkeys

      The number of key arrays (columns)

      :type: int

   .. attribute:: size

      The length of the input array(s), i.e. number of rows

      :type: int

   .. attribute:: permutation

      The permutation that sorts the keys array(s) by value (row)

      :type: pdarray

   .. attribute:: unique_keys

      The unique values of the keys array(s), in grouped order

      :type: (list of) pdarray, Strings, or Categorical

   .. attribute:: ngroups

      The length of the unique_keys array(s), i.e. number of groups

      :type: int

   .. attribute:: segments

      The start index of each group in the grouped array(s)

      :type: pdarray

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. attribute:: dropna

      If True, and the groupby keys contain NaN values,
      the NaN values together with the corresponding row will be dropped.
      Otherwise, the rows corresponding to NaN values will be kept.

      :type: bool (default=True)

   :raises TypeError: Raised if keys is a pdarray with a dtype other than int64

   .. rubric:: Notes

   Integral pdarrays, Strings, and Categoricals are natively supported, but
   float64 and bool arrays are not.

   For a user-defined class to be groupable, it must inherit from pdarray
   and define or overload the grouping API:
     1) a ._get_grouping_keys() method that returns a list of pdarrays
        that can be (co)argsorted.
     2) (Optional) a .group() method that returns the permutation that
        groups the array
   If the input is a single array with a .group() method defined, method 2
   will be used; otherwise, method 1 will be used.


   .. py:method:: AND(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise AND of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise AND reduction on
      each group.

      :param values: The values to group and reduce with AND
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise AND of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: OR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise OR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise OR reduction on
      each group.

      :param values: The values to group and reduce with OR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise OR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: Reductions(*args, **kwargs)

      frozenset() -> empty frozenset object
      frozenset(iterable) -> frozenset object

      Build an immutable unordered collection of unique elements.




   .. py:method:: XOR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise XOR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise XOR reduction on
      each group.

      :param values: The values to group and reduce with XOR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise XOR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: aggregate(values: groupable, operator: str, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, groupable]

      Using the permutation stored in the GroupBy instance, group another
      array of values and apply a reduction to each group's values.

      :param values: The values to group and reduce
      :type values: pdarray
      :param operator: The name of the reduction operator to use
      :type operator: str
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **aggregates** (*groupable*) -- One aggregate value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if the requested operator is not supported for the
          values dtype

      .. rubric:: Examples

      >>> keys = ak.arange(0, 10)
      >>> vals = ak.linspace(-1, 1, 10)
      >>> g = ak.GroupBy(keys)
      >>> g.aggregate(vals, 'sum')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777768,
      -0.55555555555555536, -0.33333333333333348, -0.11111111111111116,
      0.11111111111111116, 0.33333333333333348, 0.55555555555555536, 0.77777777777777768,
      1]))
      >>> g.aggregate(vals, 'min')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777779,
      -0.55555555555555558, -0.33333333333333337, -0.11111111111111116, 0.11111111111111116,
      0.33333333333333326, 0.55555555555555536, 0.77777777777777768, 1]))



   .. py:method:: all(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform an "and" reduction on
      each group.

      :param values: The values to group and reduce with "and"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: any(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and perform an "or" reduction on each group.

      :param values: The values to group and reduce with "or"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array



   .. py:method:: argmax(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      maximum of each group's values.

      :param values: The values to group and find argmax
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argmaxima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The returned indices refer to the original values array as passed in,
      not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmax(b)
      (array([2, 3, 4]), array([9, 3, 2]))



   .. py:method:: argmin(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      minimum of each group's values.

      :param values: The values to group and find argmin
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argminima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values
          size or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if argmin is not supported for the values dtype

      .. rubric:: Notes

      The returned indices refer to the original values array as
      passed in, not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmin(b)
      (array([2, 3, 4]), array([5, 4, 2]))



   .. py:method:: attach(user_defined_name: str) -> GroupBy

      Function to return a GroupBy object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which GroupBy object was registered under
      :type user_defined_name: str

      :returns: The GroupBy object created by re-attaching to the corresponding server components
      :rtype: GroupBy

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_groupby_by_name`



   .. py:method:: broadcast(values: Union[pdarray, Strings], permute: bool = True) -> Union[pdarray, Strings]

      Fill each group's segment with a constant value.

      :param values: The values to put in each group's segment
      :type values: pdarray, Strings
      :param permute: If True (default), permute broadcast values back to the ordering
                      of the original array on which GroupBy was called. If False, the
                      broadcast values are grouped by value.
      :type permute: bool

      :returns: The broadcasted values
      :rtype: pdarray, Strings

      :raises TypeError: Raised if value is not a pdarray object
      :raises ValueError: Raised if the values array does not have one
          value per segment

      .. rubric:: Notes

      This function is a sparse analog of ``np.broadcast``. If a
      GroupBy object represents a sparse matrix (tensor), then
      this function takes a (dense) column vector and replicates
      each value to the non-zero elements in the corresponding row.

      .. rubric:: Examples

      >>> a = ak.array([0, 1, 0, 1, 0])
      >>> values = ak.array([3, 5])
      >>> g = ak.GroupBy(a)
      # By default, result is in original order
      >>> g.broadcast(values)
      array([3, 5, 3, 5, 3])
      # With permute=False, result is in grouped order
      >>> g.broadcast(values, permute=False)
      array([3, 3, 3, 5, 5]
      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 1, 4, 4, 4, 1, 3, 3, 2, 2])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> g.broadcast(counts > 2)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts == 3)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts < 4)
      array([True True True True True True True True True True])



   .. py:method:: build_from_components(user_defined_name: Optional[str] = None, **kwargs) -> GroupBy

      function to build a new GroupBy object from component keys and permutation.

      :param user_defined_name: and assign it the given name
      :type user_defined_name: str (Optional) Passing a name will init the new GroupBy
      :param kwargs: Expected keys are "orig_keys", "permutation", "unique_keys", and "segments"
      :type kwargs: dict Dictionary of components required for rebuilding the GroupBy.

      :returns: The GroupBy object created by using the given components
      :rtype: GroupBy



   .. py:method:: count(values: pdarray) -> Tuple[groupable, pdarray]

      Count the number of elements in each group.  NaN values will be excluded from the total.

      :param values: The values to be count by group (excluding NaN values).
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears (excluding NaN values).

      .. rubric:: Examples

      >>> a = ak.array([1, 0, -1, 1, 0, -1])
      >>> a
      array([1 0 -1 1 0 -1])
      >>> b = ak.array([1, np.nan, -1, np.nan, np.nan, -1], dtype = "float64")
      >>> b
      array([1.00000000000000000 nan -1.00000000000000000 nan nan -1.00000000000000000])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.count(b)
      >>> keys
      array([-1 0 1])
      >>> counts
      array([2 0 1])



   .. py:method:: first(values: groupable_element_type) -> Tuple[groupable, groupable_element_type]

      First value in each group.

      :param values: The values from which to take the first of each group
      :type values: pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first value of each group



   .. py:method:: from_return_msg(rep_msg)


   .. py:method:: head(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the first n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.head(v, 2, return_indices=True)
      >>> _, values = g.head(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([0 3 1 4 2 5])
      >>> values
      array([0 3 1 4 2 5])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.head(v2, 2, return_indices=True)
      >>> _, values2 = g.head(v2, 2, return_indices=False)
      >>> idx2
      array([0 3 1 4 2 5])
      >>> values2
      array([0 -6 -2 -8 -4 -10])



   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_groupby_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: max(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the maximum of each
      group's values.

      :param values: The values to group and find maxima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_maxima** (*pdarray*) -- One maximum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if max is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if max is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.max(b)
      (array([2, 3, 4]), array([4, 4, 3]))



   .. py:method:: mean(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the mean of each group's
      values.

      :param values: The values to group and average
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_means** (*pdarray, float64*) -- One mean value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.mean(b)
      (array([2, 3, 4]), array([2.6666666666666665, 2.7999999999999998, 3]))



   .. py:method:: median(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the median of each group's
      values.

      :param values: The values to group and find median
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_medians** (*pdarray, float64*) -- One median value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,9)
      >>> a
      array([4 1 4 3 2 2 2 3 3])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([4 1 4 3 2 2 2 3 3])
      >>> b = ak.linspace(-5,5,9)
      >>> b
      array([-5 -3.75 -2.5 -1.25 0 1.25 2.5 3.75 5])
      >>> g.median(b)
      (array([1 2 3 4]), array([-3.75 1.25 3.75 -3.75]))



   .. py:method:: min(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the minimum of each group's
      values.

      :param values: The values to group and find minima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_minima** (*pdarray*) -- One minimum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if min is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if min is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.min(b)
      (array([2, 3, 4]), array([1, 1, 3]))



   .. py:method:: mode(values: groupable) -> Tuple[groupable, groupable]

      Most common value in each group. If a group is multi-modal, return the
      modal value that occurs first.

      :param values: The values from which to take the mode of each group
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) pdarray-like*) -- The most common value of each group



   .. py:method:: most_common(values)

      (Deprecated) See `GroupBy.mode()`.




   .. py:method:: nunique(values: groupable) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and return the number of unique values in each group.

      :param values: The values to group and find unique values
      :type values: pdarray, int64

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **group_nunique** (*groupable*) -- Number of unique values per unique key in the GroupBy instance

      :raises TypeError: Raised if the dtype(s) of values array(s) does/do not support
          the nunique method
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if nunique is not supported for the values dtype

      .. rubric:: Examples

      >>> data = ak.array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> data
      array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> labels = ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> labels
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g = ak.GroupBy(labels)
      >>> g.keys
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g.nunique(data)
      array([1,2,3,4]), array([2, 2, 3, 1])
      #    Group (1,1,1) has values [3,4,3] -> there are 2 unique values 3&4
      #    Group (2,2,2) has values [1,1,4] -> 2 unique values 1&4
      #    Group (3,3,3) has values [3,4,1] -> 3 unique values
      #    Group (4) has values [4] -> 1 unique value



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: prod(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the product of each group's
      values.

      :param values: The values to group and multiply
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_products** (*pdarray, float64*) -- One product per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if prod is not supported for the values dtype

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.prod(b)
      (array([2, 3, 4]), array([12, 108.00000000000003, 8.9999999999999982]))



   .. py:method:: register(user_defined_name: str) -> GroupBy

      Register this GroupBy object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the GroupBy is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same GroupBy which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different GroupBys with the same name.
      :rtype: GroupBy

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the GroupBy with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: sample(values: groupable, n=None, frac=None, replace=False, weights=None, random_state=None, return_indices=False, permute_samples=False)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param values: The values from which to sample, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the value more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the groupby keys
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional
      :param return_indices: if True, return the indices of the sampled values.
                             Otherwise, return the sample values.
      :type return_indices: bool, default False
      :param permute_samples: if True, return permute the samples according to group
                              Otherwise, keep samples in original order.
      :type permute_samples: bool, default False

      :returns: if return_indices is True, return the indices of the sampled values.
                Otherwise, return the sample values.
      :rtype: pdarray



   .. py:method:: size() -> Tuple[groupable, pdarray]

      Count the number of elements in each group, i.e. the number of times
      each key appears.  This counts the total number of rows (including NaN values).

      :param none:

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears

      .. seealso:: :obj:`count`

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 2, 3, 1, 2, 4, 3, 4, 3, 4])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> keys
      array([1, 2, 3, 4])
      >>> counts
      array([1, 2, 4, 3])



   .. py:method:: std(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the standard deviation of
      each group's values.

      :param values: The values to group and find standard deviation
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_stds** (*pdarray, float64*) -- One std value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The standard deviation is the square root of the average of the squared
      deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

      The average squared deviation is normally calculated as
      ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
      the divisor ``N - ddof`` is used instead. In standard statistical
      practice, ``ddof=1`` provides an unbiased estimator of the variance
      of the infinite population. ``ddof=0`` provides a maximum likelihood
      estimate of the variance for normally distributed variables. The
      standard deviation computed in this function is the square root of
      the estimated variance, so even with ``ddof=1``, it will not be an
      unbiased estimate of the standard deviation per se.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.std(b)
      (array([2 3 4]), array([1.5275252316519465 1.0954451150103321 0]))



   .. py:method:: sum(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and sum each group's values.

      :param values: The values to group and sum
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_sums** (*pdarray*) -- One sum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The grouped sum of a boolean ``pdarray`` returns integers.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.sum(b)
      (array([2, 3, 4]), array([8, 14, 6]))



   .. py:method:: tail(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the last n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The last n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.tail(v, 2, return_indices=True)
      >>> _, values = g.tail(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([6 9 4 7 5 8])
      >>> values
      array([6 9 4 7 5 8])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.tail(v2, 2, return_indices=True)
      >>> _, values2 = g.tail(v2, 2, return_indices=False)
      >>> idx2
      array([6 9 4 7 5 8])
      >>> values2
      array([-12 -18 -8 -14 -10 -16])



   .. py:method:: to_hdf(prefix_path, dataset='groupby', mode='truncate', file_type='distribute')

      Save the GroupBy to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :returns: * *None*
                * *GroupBy is not currently supported by Parquet*



   .. py:method:: unique(values: groupable)

      Return the set of unique values in each group, as a SegArray.

      :param values: The values to unique
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) SegArray*) -- The unique values of each group

      :raises TypeError: Raised if values is or contains Strings or Categorical



   .. py:method:: unregister()

      Unregister this GroupBy object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_groupby_by_name(user_defined_name: str) -> None

      Function to unregister GroupBy object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the GroupBy object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'groupby', repack: bool = True)


   .. py:method:: var(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the variance of
      each group's values.

      :param values: The values to group and find variance
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_vars** (*pdarray, float64*) -- One var value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The variance is the average of the squared deviations from the mean,
      i.e.,  ``var = mean((x - x.mean())**2)``.

      The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
      If, however, `ddof` is specified, the divisor ``N - ddof`` is used
      instead.  In standard statistical practice, ``ddof=1`` provides an
      unbiased estimator of the variance of a hypothetical infinite population.
      ``ddof=0`` provides a maximum likelihood estimate of the variance for
      normally distributed variables.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.var(b)
      (array([2 3 4]), array([2.333333333333333 1.2 0]))



.. py:class:: GroupBy

   Group an array or list of arrays by value, usually in preparation
   for aggregating the within-group values of another array.

   :param keys: The array to group by value, or if list, the column arrays to group by row
   :type keys: (list of) pdarray, Strings, or Categorical
   :param assume_sorted: If True, assume keys is already sorted (Default: False)
   :type assume_sorted: bool

   .. attribute:: nkeys

      The number of key arrays (columns)

      :type: int

   .. attribute:: size

      The length of the input array(s), i.e. number of rows

      :type: int

   .. attribute:: permutation

      The permutation that sorts the keys array(s) by value (row)

      :type: pdarray

   .. attribute:: unique_keys

      The unique values of the keys array(s), in grouped order

      :type: (list of) pdarray, Strings, or Categorical

   .. attribute:: ngroups

      The length of the unique_keys array(s), i.e. number of groups

      :type: int

   .. attribute:: segments

      The start index of each group in the grouped array(s)

      :type: pdarray

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. attribute:: dropna

      If True, and the groupby keys contain NaN values,
      the NaN values together with the corresponding row will be dropped.
      Otherwise, the rows corresponding to NaN values will be kept.

      :type: bool (default=True)

   :raises TypeError: Raised if keys is a pdarray with a dtype other than int64

   .. rubric:: Notes

   Integral pdarrays, Strings, and Categoricals are natively supported, but
   float64 and bool arrays are not.

   For a user-defined class to be groupable, it must inherit from pdarray
   and define or overload the grouping API:
     1) a ._get_grouping_keys() method that returns a list of pdarrays
        that can be (co)argsorted.
     2) (Optional) a .group() method that returns the permutation that
        groups the array
   If the input is a single array with a .group() method defined, method 2
   will be used; otherwise, method 1 will be used.


   .. py:method:: AND(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise AND of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise AND reduction on
      each group.

      :param values: The values to group and reduce with AND
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise AND of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: OR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise OR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise OR reduction on
      each group.

      :param values: The values to group and reduce with OR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise OR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: Reductions(*args, **kwargs)

      frozenset() -> empty frozenset object
      frozenset(iterable) -> frozenset object

      Build an immutable unordered collection of unique elements.




   .. py:method:: XOR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise XOR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise XOR reduction on
      each group.

      :param values: The values to group and reduce with XOR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise XOR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: aggregate(values: groupable, operator: str, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, groupable]

      Using the permutation stored in the GroupBy instance, group another
      array of values and apply a reduction to each group's values.

      :param values: The values to group and reduce
      :type values: pdarray
      :param operator: The name of the reduction operator to use
      :type operator: str
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **aggregates** (*groupable*) -- One aggregate value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if the requested operator is not supported for the
          values dtype

      .. rubric:: Examples

      >>> keys = ak.arange(0, 10)
      >>> vals = ak.linspace(-1, 1, 10)
      >>> g = ak.GroupBy(keys)
      >>> g.aggregate(vals, 'sum')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777768,
      -0.55555555555555536, -0.33333333333333348, -0.11111111111111116,
      0.11111111111111116, 0.33333333333333348, 0.55555555555555536, 0.77777777777777768,
      1]))
      >>> g.aggregate(vals, 'min')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777779,
      -0.55555555555555558, -0.33333333333333337, -0.11111111111111116, 0.11111111111111116,
      0.33333333333333326, 0.55555555555555536, 0.77777777777777768, 1]))



   .. py:method:: all(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform an "and" reduction on
      each group.

      :param values: The values to group and reduce with "and"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: any(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and perform an "or" reduction on each group.

      :param values: The values to group and reduce with "or"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array



   .. py:method:: argmax(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      maximum of each group's values.

      :param values: The values to group and find argmax
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argmaxima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The returned indices refer to the original values array as passed in,
      not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmax(b)
      (array([2, 3, 4]), array([9, 3, 2]))



   .. py:method:: argmin(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      minimum of each group's values.

      :param values: The values to group and find argmin
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argminima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values
          size or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if argmin is not supported for the values dtype

      .. rubric:: Notes

      The returned indices refer to the original values array as
      passed in, not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmin(b)
      (array([2, 3, 4]), array([5, 4, 2]))



   .. py:method:: attach(user_defined_name: str) -> GroupBy

      Function to return a GroupBy object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which GroupBy object was registered under
      :type user_defined_name: str

      :returns: The GroupBy object created by re-attaching to the corresponding server components
      :rtype: GroupBy

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_groupby_by_name`



   .. py:method:: broadcast(values: Union[pdarray, Strings], permute: bool = True) -> Union[pdarray, Strings]

      Fill each group's segment with a constant value.

      :param values: The values to put in each group's segment
      :type values: pdarray, Strings
      :param permute: If True (default), permute broadcast values back to the ordering
                      of the original array on which GroupBy was called. If False, the
                      broadcast values are grouped by value.
      :type permute: bool

      :returns: The broadcasted values
      :rtype: pdarray, Strings

      :raises TypeError: Raised if value is not a pdarray object
      :raises ValueError: Raised if the values array does not have one
          value per segment

      .. rubric:: Notes

      This function is a sparse analog of ``np.broadcast``. If a
      GroupBy object represents a sparse matrix (tensor), then
      this function takes a (dense) column vector and replicates
      each value to the non-zero elements in the corresponding row.

      .. rubric:: Examples

      >>> a = ak.array([0, 1, 0, 1, 0])
      >>> values = ak.array([3, 5])
      >>> g = ak.GroupBy(a)
      # By default, result is in original order
      >>> g.broadcast(values)
      array([3, 5, 3, 5, 3])
      # With permute=False, result is in grouped order
      >>> g.broadcast(values, permute=False)
      array([3, 3, 3, 5, 5]
      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 1, 4, 4, 4, 1, 3, 3, 2, 2])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> g.broadcast(counts > 2)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts == 3)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts < 4)
      array([True True True True True True True True True True])



   .. py:method:: build_from_components(user_defined_name: Optional[str] = None, **kwargs) -> GroupBy

      function to build a new GroupBy object from component keys and permutation.

      :param user_defined_name: and assign it the given name
      :type user_defined_name: str (Optional) Passing a name will init the new GroupBy
      :param kwargs: Expected keys are "orig_keys", "permutation", "unique_keys", and "segments"
      :type kwargs: dict Dictionary of components required for rebuilding the GroupBy.

      :returns: The GroupBy object created by using the given components
      :rtype: GroupBy



   .. py:method:: count(values: pdarray) -> Tuple[groupable, pdarray]

      Count the number of elements in each group.  NaN values will be excluded from the total.

      :param values: The values to be count by group (excluding NaN values).
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears (excluding NaN values).

      .. rubric:: Examples

      >>> a = ak.array([1, 0, -1, 1, 0, -1])
      >>> a
      array([1 0 -1 1 0 -1])
      >>> b = ak.array([1, np.nan, -1, np.nan, np.nan, -1], dtype = "float64")
      >>> b
      array([1.00000000000000000 nan -1.00000000000000000 nan nan -1.00000000000000000])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.count(b)
      >>> keys
      array([-1 0 1])
      >>> counts
      array([2 0 1])



   .. py:method:: first(values: groupable_element_type) -> Tuple[groupable, groupable_element_type]

      First value in each group.

      :param values: The values from which to take the first of each group
      :type values: pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first value of each group



   .. py:method:: from_return_msg(rep_msg)


   .. py:method:: head(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the first n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.head(v, 2, return_indices=True)
      >>> _, values = g.head(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([0 3 1 4 2 5])
      >>> values
      array([0 3 1 4 2 5])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.head(v2, 2, return_indices=True)
      >>> _, values2 = g.head(v2, 2, return_indices=False)
      >>> idx2
      array([0 3 1 4 2 5])
      >>> values2
      array([0 -6 -2 -8 -4 -10])



   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_groupby_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: max(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the maximum of each
      group's values.

      :param values: The values to group and find maxima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_maxima** (*pdarray*) -- One maximum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if max is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if max is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.max(b)
      (array([2, 3, 4]), array([4, 4, 3]))



   .. py:method:: mean(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the mean of each group's
      values.

      :param values: The values to group and average
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_means** (*pdarray, float64*) -- One mean value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.mean(b)
      (array([2, 3, 4]), array([2.6666666666666665, 2.7999999999999998, 3]))



   .. py:method:: median(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the median of each group's
      values.

      :param values: The values to group and find median
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_medians** (*pdarray, float64*) -- One median value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,9)
      >>> a
      array([4 1 4 3 2 2 2 3 3])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([4 1 4 3 2 2 2 3 3])
      >>> b = ak.linspace(-5,5,9)
      >>> b
      array([-5 -3.75 -2.5 -1.25 0 1.25 2.5 3.75 5])
      >>> g.median(b)
      (array([1 2 3 4]), array([-3.75 1.25 3.75 -3.75]))



   .. py:method:: min(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the minimum of each group's
      values.

      :param values: The values to group and find minima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_minima** (*pdarray*) -- One minimum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if min is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if min is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.min(b)
      (array([2, 3, 4]), array([1, 1, 3]))



   .. py:method:: mode(values: groupable) -> Tuple[groupable, groupable]

      Most common value in each group. If a group is multi-modal, return the
      modal value that occurs first.

      :param values: The values from which to take the mode of each group
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) pdarray-like*) -- The most common value of each group



   .. py:method:: most_common(values)

      (Deprecated) See `GroupBy.mode()`.




   .. py:method:: nunique(values: groupable) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and return the number of unique values in each group.

      :param values: The values to group and find unique values
      :type values: pdarray, int64

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **group_nunique** (*groupable*) -- Number of unique values per unique key in the GroupBy instance

      :raises TypeError: Raised if the dtype(s) of values array(s) does/do not support
          the nunique method
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if nunique is not supported for the values dtype

      .. rubric:: Examples

      >>> data = ak.array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> data
      array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> labels = ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> labels
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g = ak.GroupBy(labels)
      >>> g.keys
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g.nunique(data)
      array([1,2,3,4]), array([2, 2, 3, 1])
      #    Group (1,1,1) has values [3,4,3] -> there are 2 unique values 3&4
      #    Group (2,2,2) has values [1,1,4] -> 2 unique values 1&4
      #    Group (3,3,3) has values [3,4,1] -> 3 unique values
      #    Group (4) has values [4] -> 1 unique value



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: prod(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the product of each group's
      values.

      :param values: The values to group and multiply
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_products** (*pdarray, float64*) -- One product per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if prod is not supported for the values dtype

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.prod(b)
      (array([2, 3, 4]), array([12, 108.00000000000003, 8.9999999999999982]))



   .. py:method:: register(user_defined_name: str) -> GroupBy

      Register this GroupBy object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the GroupBy is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same GroupBy which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different GroupBys with the same name.
      :rtype: GroupBy

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the GroupBy with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: sample(values: groupable, n=None, frac=None, replace=False, weights=None, random_state=None, return_indices=False, permute_samples=False)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param values: The values from which to sample, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the value more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the groupby keys
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional
      :param return_indices: if True, return the indices of the sampled values.
                             Otherwise, return the sample values.
      :type return_indices: bool, default False
      :param permute_samples: if True, return permute the samples according to group
                              Otherwise, keep samples in original order.
      :type permute_samples: bool, default False

      :returns: if return_indices is True, return the indices of the sampled values.
                Otherwise, return the sample values.
      :rtype: pdarray



   .. py:method:: size() -> Tuple[groupable, pdarray]

      Count the number of elements in each group, i.e. the number of times
      each key appears.  This counts the total number of rows (including NaN values).

      :param none:

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears

      .. seealso:: :obj:`count`

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 2, 3, 1, 2, 4, 3, 4, 3, 4])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> keys
      array([1, 2, 3, 4])
      >>> counts
      array([1, 2, 4, 3])



   .. py:method:: std(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the standard deviation of
      each group's values.

      :param values: The values to group and find standard deviation
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_stds** (*pdarray, float64*) -- One std value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The standard deviation is the square root of the average of the squared
      deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

      The average squared deviation is normally calculated as
      ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
      the divisor ``N - ddof`` is used instead. In standard statistical
      practice, ``ddof=1`` provides an unbiased estimator of the variance
      of the infinite population. ``ddof=0`` provides a maximum likelihood
      estimate of the variance for normally distributed variables. The
      standard deviation computed in this function is the square root of
      the estimated variance, so even with ``ddof=1``, it will not be an
      unbiased estimate of the standard deviation per se.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.std(b)
      (array([2 3 4]), array([1.5275252316519465 1.0954451150103321 0]))



   .. py:method:: sum(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and sum each group's values.

      :param values: The values to group and sum
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_sums** (*pdarray*) -- One sum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The grouped sum of a boolean ``pdarray`` returns integers.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.sum(b)
      (array([2, 3, 4]), array([8, 14, 6]))



   .. py:method:: tail(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the last n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The last n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.tail(v, 2, return_indices=True)
      >>> _, values = g.tail(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([6 9 4 7 5 8])
      >>> values
      array([6 9 4 7 5 8])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.tail(v2, 2, return_indices=True)
      >>> _, values2 = g.tail(v2, 2, return_indices=False)
      >>> idx2
      array([6 9 4 7 5 8])
      >>> values2
      array([-12 -18 -8 -14 -10 -16])



   .. py:method:: to_hdf(prefix_path, dataset='groupby', mode='truncate', file_type='distribute')

      Save the GroupBy to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :returns: * *None*
                * *GroupBy is not currently supported by Parquet*



   .. py:method:: unique(values: groupable)

      Return the set of unique values in each group, as a SegArray.

      :param values: The values to unique
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) SegArray*) -- The unique values of each group

      :raises TypeError: Raised if values is or contains Strings or Categorical



   .. py:method:: unregister()

      Unregister this GroupBy object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_groupby_by_name(user_defined_name: str) -> None

      Function to unregister GroupBy object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the GroupBy object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'groupby', repack: bool = True)


   .. py:method:: var(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the variance of
      each group's values.

      :param values: The values to group and find variance
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_vars** (*pdarray, float64*) -- One var value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The variance is the average of the squared deviations from the mean,
      i.e.,  ``var = mean((x - x.mean())**2)``.

      The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
      If, however, `ddof` is specified, the divisor ``N - ddof`` is used
      instead.  In standard statistical practice, ``ddof=1`` provides an
      unbiased estimator of the variance of a hypothetical infinite population.
      ``ddof=0`` provides a maximum likelihood estimate of the variance for
      normally distributed variables.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.var(b)
      (array([2 3 4]), array([2.333333333333333 1.2 0]))



.. py:class:: GroupBy

   Group an array or list of arrays by value, usually in preparation
   for aggregating the within-group values of another array.

   :param keys: The array to group by value, or if list, the column arrays to group by row
   :type keys: (list of) pdarray, Strings, or Categorical
   :param assume_sorted: If True, assume keys is already sorted (Default: False)
   :type assume_sorted: bool

   .. attribute:: nkeys

      The number of key arrays (columns)

      :type: int

   .. attribute:: size

      The length of the input array(s), i.e. number of rows

      :type: int

   .. attribute:: permutation

      The permutation that sorts the keys array(s) by value (row)

      :type: pdarray

   .. attribute:: unique_keys

      The unique values of the keys array(s), in grouped order

      :type: (list of) pdarray, Strings, or Categorical

   .. attribute:: ngroups

      The length of the unique_keys array(s), i.e. number of groups

      :type: int

   .. attribute:: segments

      The start index of each group in the grouped array(s)

      :type: pdarray

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. attribute:: dropna

      If True, and the groupby keys contain NaN values,
      the NaN values together with the corresponding row will be dropped.
      Otherwise, the rows corresponding to NaN values will be kept.

      :type: bool (default=True)

   :raises TypeError: Raised if keys is a pdarray with a dtype other than int64

   .. rubric:: Notes

   Integral pdarrays, Strings, and Categoricals are natively supported, but
   float64 and bool arrays are not.

   For a user-defined class to be groupable, it must inherit from pdarray
   and define or overload the grouping API:
     1) a ._get_grouping_keys() method that returns a list of pdarrays
        that can be (co)argsorted.
     2) (Optional) a .group() method that returns the permutation that
        groups the array
   If the input is a single array with a .group() method defined, method 2
   will be used; otherwise, method 1 will be used.


   .. py:method:: AND(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise AND of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise AND reduction on
      each group.

      :param values: The values to group and reduce with AND
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise AND of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: OR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise OR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise OR reduction on
      each group.

      :param values: The values to group and reduce with OR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise OR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: Reductions(*args, **kwargs)

      frozenset() -> empty frozenset object
      frozenset(iterable) -> frozenset object

      Build an immutable unordered collection of unique elements.




   .. py:method:: XOR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise XOR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise XOR reduction on
      each group.

      :param values: The values to group and reduce with XOR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise XOR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: aggregate(values: groupable, operator: str, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, groupable]

      Using the permutation stored in the GroupBy instance, group another
      array of values and apply a reduction to each group's values.

      :param values: The values to group and reduce
      :type values: pdarray
      :param operator: The name of the reduction operator to use
      :type operator: str
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **aggregates** (*groupable*) -- One aggregate value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if the requested operator is not supported for the
          values dtype

      .. rubric:: Examples

      >>> keys = ak.arange(0, 10)
      >>> vals = ak.linspace(-1, 1, 10)
      >>> g = ak.GroupBy(keys)
      >>> g.aggregate(vals, 'sum')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777768,
      -0.55555555555555536, -0.33333333333333348, -0.11111111111111116,
      0.11111111111111116, 0.33333333333333348, 0.55555555555555536, 0.77777777777777768,
      1]))
      >>> g.aggregate(vals, 'min')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777779,
      -0.55555555555555558, -0.33333333333333337, -0.11111111111111116, 0.11111111111111116,
      0.33333333333333326, 0.55555555555555536, 0.77777777777777768, 1]))



   .. py:method:: all(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform an "and" reduction on
      each group.

      :param values: The values to group and reduce with "and"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: any(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and perform an "or" reduction on each group.

      :param values: The values to group and reduce with "or"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array



   .. py:method:: argmax(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      maximum of each group's values.

      :param values: The values to group and find argmax
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argmaxima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The returned indices refer to the original values array as passed in,
      not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmax(b)
      (array([2, 3, 4]), array([9, 3, 2]))



   .. py:method:: argmin(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      minimum of each group's values.

      :param values: The values to group and find argmin
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argminima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values
          size or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if argmin is not supported for the values dtype

      .. rubric:: Notes

      The returned indices refer to the original values array as
      passed in, not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmin(b)
      (array([2, 3, 4]), array([5, 4, 2]))



   .. py:method:: attach(user_defined_name: str) -> GroupBy

      Function to return a GroupBy object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which GroupBy object was registered under
      :type user_defined_name: str

      :returns: The GroupBy object created by re-attaching to the corresponding server components
      :rtype: GroupBy

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_groupby_by_name`



   .. py:method:: broadcast(values: Union[pdarray, Strings], permute: bool = True) -> Union[pdarray, Strings]

      Fill each group's segment with a constant value.

      :param values: The values to put in each group's segment
      :type values: pdarray, Strings
      :param permute: If True (default), permute broadcast values back to the ordering
                      of the original array on which GroupBy was called. If False, the
                      broadcast values are grouped by value.
      :type permute: bool

      :returns: The broadcasted values
      :rtype: pdarray, Strings

      :raises TypeError: Raised if value is not a pdarray object
      :raises ValueError: Raised if the values array does not have one
          value per segment

      .. rubric:: Notes

      This function is a sparse analog of ``np.broadcast``. If a
      GroupBy object represents a sparse matrix (tensor), then
      this function takes a (dense) column vector and replicates
      each value to the non-zero elements in the corresponding row.

      .. rubric:: Examples

      >>> a = ak.array([0, 1, 0, 1, 0])
      >>> values = ak.array([3, 5])
      >>> g = ak.GroupBy(a)
      # By default, result is in original order
      >>> g.broadcast(values)
      array([3, 5, 3, 5, 3])
      # With permute=False, result is in grouped order
      >>> g.broadcast(values, permute=False)
      array([3, 3, 3, 5, 5]
      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 1, 4, 4, 4, 1, 3, 3, 2, 2])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> g.broadcast(counts > 2)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts == 3)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts < 4)
      array([True True True True True True True True True True])



   .. py:method:: build_from_components(user_defined_name: Optional[str] = None, **kwargs) -> GroupBy

      function to build a new GroupBy object from component keys and permutation.

      :param user_defined_name: and assign it the given name
      :type user_defined_name: str (Optional) Passing a name will init the new GroupBy
      :param kwargs: Expected keys are "orig_keys", "permutation", "unique_keys", and "segments"
      :type kwargs: dict Dictionary of components required for rebuilding the GroupBy.

      :returns: The GroupBy object created by using the given components
      :rtype: GroupBy



   .. py:method:: count(values: pdarray) -> Tuple[groupable, pdarray]

      Count the number of elements in each group.  NaN values will be excluded from the total.

      :param values: The values to be count by group (excluding NaN values).
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears (excluding NaN values).

      .. rubric:: Examples

      >>> a = ak.array([1, 0, -1, 1, 0, -1])
      >>> a
      array([1 0 -1 1 0 -1])
      >>> b = ak.array([1, np.nan, -1, np.nan, np.nan, -1], dtype = "float64")
      >>> b
      array([1.00000000000000000 nan -1.00000000000000000 nan nan -1.00000000000000000])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.count(b)
      >>> keys
      array([-1 0 1])
      >>> counts
      array([2 0 1])



   .. py:method:: first(values: groupable_element_type) -> Tuple[groupable, groupable_element_type]

      First value in each group.

      :param values: The values from which to take the first of each group
      :type values: pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first value of each group



   .. py:method:: from_return_msg(rep_msg)


   .. py:method:: head(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the first n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.head(v, 2, return_indices=True)
      >>> _, values = g.head(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([0 3 1 4 2 5])
      >>> values
      array([0 3 1 4 2 5])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.head(v2, 2, return_indices=True)
      >>> _, values2 = g.head(v2, 2, return_indices=False)
      >>> idx2
      array([0 3 1 4 2 5])
      >>> values2
      array([0 -6 -2 -8 -4 -10])



   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_groupby_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: max(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the maximum of each
      group's values.

      :param values: The values to group and find maxima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_maxima** (*pdarray*) -- One maximum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if max is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if max is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.max(b)
      (array([2, 3, 4]), array([4, 4, 3]))



   .. py:method:: mean(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the mean of each group's
      values.

      :param values: The values to group and average
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_means** (*pdarray, float64*) -- One mean value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.mean(b)
      (array([2, 3, 4]), array([2.6666666666666665, 2.7999999999999998, 3]))



   .. py:method:: median(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the median of each group's
      values.

      :param values: The values to group and find median
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_medians** (*pdarray, float64*) -- One median value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,9)
      >>> a
      array([4 1 4 3 2 2 2 3 3])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([4 1 4 3 2 2 2 3 3])
      >>> b = ak.linspace(-5,5,9)
      >>> b
      array([-5 -3.75 -2.5 -1.25 0 1.25 2.5 3.75 5])
      >>> g.median(b)
      (array([1 2 3 4]), array([-3.75 1.25 3.75 -3.75]))



   .. py:method:: min(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the minimum of each group's
      values.

      :param values: The values to group and find minima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_minima** (*pdarray*) -- One minimum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if min is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if min is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.min(b)
      (array([2, 3, 4]), array([1, 1, 3]))



   .. py:method:: mode(values: groupable) -> Tuple[groupable, groupable]

      Most common value in each group. If a group is multi-modal, return the
      modal value that occurs first.

      :param values: The values from which to take the mode of each group
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) pdarray-like*) -- The most common value of each group



   .. py:method:: most_common(values)

      (Deprecated) See `GroupBy.mode()`.




   .. py:method:: nunique(values: groupable) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and return the number of unique values in each group.

      :param values: The values to group and find unique values
      :type values: pdarray, int64

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **group_nunique** (*groupable*) -- Number of unique values per unique key in the GroupBy instance

      :raises TypeError: Raised if the dtype(s) of values array(s) does/do not support
          the nunique method
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if nunique is not supported for the values dtype

      .. rubric:: Examples

      >>> data = ak.array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> data
      array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> labels = ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> labels
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g = ak.GroupBy(labels)
      >>> g.keys
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g.nunique(data)
      array([1,2,3,4]), array([2, 2, 3, 1])
      #    Group (1,1,1) has values [3,4,3] -> there are 2 unique values 3&4
      #    Group (2,2,2) has values [1,1,4] -> 2 unique values 1&4
      #    Group (3,3,3) has values [3,4,1] -> 3 unique values
      #    Group (4) has values [4] -> 1 unique value



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: prod(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the product of each group's
      values.

      :param values: The values to group and multiply
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_products** (*pdarray, float64*) -- One product per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if prod is not supported for the values dtype

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.prod(b)
      (array([2, 3, 4]), array([12, 108.00000000000003, 8.9999999999999982]))



   .. py:method:: register(user_defined_name: str) -> GroupBy

      Register this GroupBy object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the GroupBy is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same GroupBy which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different GroupBys with the same name.
      :rtype: GroupBy

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the GroupBy with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: sample(values: groupable, n=None, frac=None, replace=False, weights=None, random_state=None, return_indices=False, permute_samples=False)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param values: The values from which to sample, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the value more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the groupby keys
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional
      :param return_indices: if True, return the indices of the sampled values.
                             Otherwise, return the sample values.
      :type return_indices: bool, default False
      :param permute_samples: if True, return permute the samples according to group
                              Otherwise, keep samples in original order.
      :type permute_samples: bool, default False

      :returns: if return_indices is True, return the indices of the sampled values.
                Otherwise, return the sample values.
      :rtype: pdarray



   .. py:method:: size() -> Tuple[groupable, pdarray]

      Count the number of elements in each group, i.e. the number of times
      each key appears.  This counts the total number of rows (including NaN values).

      :param none:

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears

      .. seealso:: :obj:`count`

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 2, 3, 1, 2, 4, 3, 4, 3, 4])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> keys
      array([1, 2, 3, 4])
      >>> counts
      array([1, 2, 4, 3])



   .. py:method:: std(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the standard deviation of
      each group's values.

      :param values: The values to group and find standard deviation
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_stds** (*pdarray, float64*) -- One std value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The standard deviation is the square root of the average of the squared
      deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

      The average squared deviation is normally calculated as
      ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
      the divisor ``N - ddof`` is used instead. In standard statistical
      practice, ``ddof=1`` provides an unbiased estimator of the variance
      of the infinite population. ``ddof=0`` provides a maximum likelihood
      estimate of the variance for normally distributed variables. The
      standard deviation computed in this function is the square root of
      the estimated variance, so even with ``ddof=1``, it will not be an
      unbiased estimate of the standard deviation per se.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.std(b)
      (array([2 3 4]), array([1.5275252316519465 1.0954451150103321 0]))



   .. py:method:: sum(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and sum each group's values.

      :param values: The values to group and sum
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_sums** (*pdarray*) -- One sum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The grouped sum of a boolean ``pdarray`` returns integers.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.sum(b)
      (array([2, 3, 4]), array([8, 14, 6]))



   .. py:method:: tail(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the last n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The last n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.tail(v, 2, return_indices=True)
      >>> _, values = g.tail(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([6 9 4 7 5 8])
      >>> values
      array([6 9 4 7 5 8])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.tail(v2, 2, return_indices=True)
      >>> _, values2 = g.tail(v2, 2, return_indices=False)
      >>> idx2
      array([6 9 4 7 5 8])
      >>> values2
      array([-12 -18 -8 -14 -10 -16])



   .. py:method:: to_hdf(prefix_path, dataset='groupby', mode='truncate', file_type='distribute')

      Save the GroupBy to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :returns: * *None*
                * *GroupBy is not currently supported by Parquet*



   .. py:method:: unique(values: groupable)

      Return the set of unique values in each group, as a SegArray.

      :param values: The values to unique
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) SegArray*) -- The unique values of each group

      :raises TypeError: Raised if values is or contains Strings or Categorical



   .. py:method:: unregister()

      Unregister this GroupBy object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_groupby_by_name(user_defined_name: str) -> None

      Function to unregister GroupBy object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the GroupBy object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'groupby', repack: bool = True)


   .. py:method:: var(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the variance of
      each group's values.

      :param values: The values to group and find variance
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_vars** (*pdarray, float64*) -- One var value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The variance is the average of the squared deviations from the mean,
      i.e.,  ``var = mean((x - x.mean())**2)``.

      The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
      If, however, `ddof` is specified, the divisor ``N - ddof`` is used
      instead.  In standard statistical practice, ``ddof=1`` provides an
      unbiased estimator of the variance of a hypothetical infinite population.
      ``ddof=0`` provides a maximum likelihood estimate of the variance for
      normally distributed variables.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.var(b)
      (array([2 3 4]), array([2.333333333333333 1.2 0]))



.. py:class:: GroupBy

   Group an array or list of arrays by value, usually in preparation
   for aggregating the within-group values of another array.

   :param keys: The array to group by value, or if list, the column arrays to group by row
   :type keys: (list of) pdarray, Strings, or Categorical
   :param assume_sorted: If True, assume keys is already sorted (Default: False)
   :type assume_sorted: bool

   .. attribute:: nkeys

      The number of key arrays (columns)

      :type: int

   .. attribute:: size

      The length of the input array(s), i.e. number of rows

      :type: int

   .. attribute:: permutation

      The permutation that sorts the keys array(s) by value (row)

      :type: pdarray

   .. attribute:: unique_keys

      The unique values of the keys array(s), in grouped order

      :type: (list of) pdarray, Strings, or Categorical

   .. attribute:: ngroups

      The length of the unique_keys array(s), i.e. number of groups

      :type: int

   .. attribute:: segments

      The start index of each group in the grouped array(s)

      :type: pdarray

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. attribute:: dropna

      If True, and the groupby keys contain NaN values,
      the NaN values together with the corresponding row will be dropped.
      Otherwise, the rows corresponding to NaN values will be kept.

      :type: bool (default=True)

   :raises TypeError: Raised if keys is a pdarray with a dtype other than int64

   .. rubric:: Notes

   Integral pdarrays, Strings, and Categoricals are natively supported, but
   float64 and bool arrays are not.

   For a user-defined class to be groupable, it must inherit from pdarray
   and define or overload the grouping API:
     1) a ._get_grouping_keys() method that returns a list of pdarrays
        that can be (co)argsorted.
     2) (Optional) a .group() method that returns the permutation that
        groups the array
   If the input is a single array with a .group() method defined, method 2
   will be used; otherwise, method 1 will be used.


   .. py:method:: AND(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise AND of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise AND reduction on
      each group.

      :param values: The values to group and reduce with AND
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise AND of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: OR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise OR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise OR reduction on
      each group.

      :param values: The values to group and reduce with OR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise OR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: Reductions(*args, **kwargs)

      frozenset() -> empty frozenset object
      frozenset(iterable) -> frozenset object

      Build an immutable unordered collection of unique elements.




   .. py:method:: XOR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise XOR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise XOR reduction on
      each group.

      :param values: The values to group and reduce with XOR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise XOR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: aggregate(values: groupable, operator: str, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, groupable]

      Using the permutation stored in the GroupBy instance, group another
      array of values and apply a reduction to each group's values.

      :param values: The values to group and reduce
      :type values: pdarray
      :param operator: The name of the reduction operator to use
      :type operator: str
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **aggregates** (*groupable*) -- One aggregate value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if the requested operator is not supported for the
          values dtype

      .. rubric:: Examples

      >>> keys = ak.arange(0, 10)
      >>> vals = ak.linspace(-1, 1, 10)
      >>> g = ak.GroupBy(keys)
      >>> g.aggregate(vals, 'sum')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777768,
      -0.55555555555555536, -0.33333333333333348, -0.11111111111111116,
      0.11111111111111116, 0.33333333333333348, 0.55555555555555536, 0.77777777777777768,
      1]))
      >>> g.aggregate(vals, 'min')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777779,
      -0.55555555555555558, -0.33333333333333337, -0.11111111111111116, 0.11111111111111116,
      0.33333333333333326, 0.55555555555555536, 0.77777777777777768, 1]))



   .. py:method:: all(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform an "and" reduction on
      each group.

      :param values: The values to group and reduce with "and"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: any(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and perform an "or" reduction on each group.

      :param values: The values to group and reduce with "or"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array



   .. py:method:: argmax(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      maximum of each group's values.

      :param values: The values to group and find argmax
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argmaxima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The returned indices refer to the original values array as passed in,
      not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmax(b)
      (array([2, 3, 4]), array([9, 3, 2]))



   .. py:method:: argmin(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      minimum of each group's values.

      :param values: The values to group and find argmin
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argminima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values
          size or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if argmin is not supported for the values dtype

      .. rubric:: Notes

      The returned indices refer to the original values array as
      passed in, not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmin(b)
      (array([2, 3, 4]), array([5, 4, 2]))



   .. py:method:: attach(user_defined_name: str) -> GroupBy

      Function to return a GroupBy object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which GroupBy object was registered under
      :type user_defined_name: str

      :returns: The GroupBy object created by re-attaching to the corresponding server components
      :rtype: GroupBy

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_groupby_by_name`



   .. py:method:: broadcast(values: Union[pdarray, Strings], permute: bool = True) -> Union[pdarray, Strings]

      Fill each group's segment with a constant value.

      :param values: The values to put in each group's segment
      :type values: pdarray, Strings
      :param permute: If True (default), permute broadcast values back to the ordering
                      of the original array on which GroupBy was called. If False, the
                      broadcast values are grouped by value.
      :type permute: bool

      :returns: The broadcasted values
      :rtype: pdarray, Strings

      :raises TypeError: Raised if value is not a pdarray object
      :raises ValueError: Raised if the values array does not have one
          value per segment

      .. rubric:: Notes

      This function is a sparse analog of ``np.broadcast``. If a
      GroupBy object represents a sparse matrix (tensor), then
      this function takes a (dense) column vector and replicates
      each value to the non-zero elements in the corresponding row.

      .. rubric:: Examples

      >>> a = ak.array([0, 1, 0, 1, 0])
      >>> values = ak.array([3, 5])
      >>> g = ak.GroupBy(a)
      # By default, result is in original order
      >>> g.broadcast(values)
      array([3, 5, 3, 5, 3])
      # With permute=False, result is in grouped order
      >>> g.broadcast(values, permute=False)
      array([3, 3, 3, 5, 5]
      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 1, 4, 4, 4, 1, 3, 3, 2, 2])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> g.broadcast(counts > 2)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts == 3)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts < 4)
      array([True True True True True True True True True True])



   .. py:method:: build_from_components(user_defined_name: Optional[str] = None, **kwargs) -> GroupBy

      function to build a new GroupBy object from component keys and permutation.

      :param user_defined_name: and assign it the given name
      :type user_defined_name: str (Optional) Passing a name will init the new GroupBy
      :param kwargs: Expected keys are "orig_keys", "permutation", "unique_keys", and "segments"
      :type kwargs: dict Dictionary of components required for rebuilding the GroupBy.

      :returns: The GroupBy object created by using the given components
      :rtype: GroupBy



   .. py:method:: count(values: pdarray) -> Tuple[groupable, pdarray]

      Count the number of elements in each group.  NaN values will be excluded from the total.

      :param values: The values to be count by group (excluding NaN values).
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears (excluding NaN values).

      .. rubric:: Examples

      >>> a = ak.array([1, 0, -1, 1, 0, -1])
      >>> a
      array([1 0 -1 1 0 -1])
      >>> b = ak.array([1, np.nan, -1, np.nan, np.nan, -1], dtype = "float64")
      >>> b
      array([1.00000000000000000 nan -1.00000000000000000 nan nan -1.00000000000000000])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.count(b)
      >>> keys
      array([-1 0 1])
      >>> counts
      array([2 0 1])



   .. py:method:: first(values: groupable_element_type) -> Tuple[groupable, groupable_element_type]

      First value in each group.

      :param values: The values from which to take the first of each group
      :type values: pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first value of each group



   .. py:method:: from_return_msg(rep_msg)


   .. py:method:: head(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the first n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.head(v, 2, return_indices=True)
      >>> _, values = g.head(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([0 3 1 4 2 5])
      >>> values
      array([0 3 1 4 2 5])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.head(v2, 2, return_indices=True)
      >>> _, values2 = g.head(v2, 2, return_indices=False)
      >>> idx2
      array([0 3 1 4 2 5])
      >>> values2
      array([0 -6 -2 -8 -4 -10])



   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_groupby_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: max(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the maximum of each
      group's values.

      :param values: The values to group and find maxima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_maxima** (*pdarray*) -- One maximum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if max is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if max is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.max(b)
      (array([2, 3, 4]), array([4, 4, 3]))



   .. py:method:: mean(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the mean of each group's
      values.

      :param values: The values to group and average
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_means** (*pdarray, float64*) -- One mean value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.mean(b)
      (array([2, 3, 4]), array([2.6666666666666665, 2.7999999999999998, 3]))



   .. py:method:: median(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the median of each group's
      values.

      :param values: The values to group and find median
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_medians** (*pdarray, float64*) -- One median value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,9)
      >>> a
      array([4 1 4 3 2 2 2 3 3])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([4 1 4 3 2 2 2 3 3])
      >>> b = ak.linspace(-5,5,9)
      >>> b
      array([-5 -3.75 -2.5 -1.25 0 1.25 2.5 3.75 5])
      >>> g.median(b)
      (array([1 2 3 4]), array([-3.75 1.25 3.75 -3.75]))



   .. py:method:: min(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the minimum of each group's
      values.

      :param values: The values to group and find minima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_minima** (*pdarray*) -- One minimum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if min is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if min is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.min(b)
      (array([2, 3, 4]), array([1, 1, 3]))



   .. py:method:: mode(values: groupable) -> Tuple[groupable, groupable]

      Most common value in each group. If a group is multi-modal, return the
      modal value that occurs first.

      :param values: The values from which to take the mode of each group
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) pdarray-like*) -- The most common value of each group



   .. py:method:: most_common(values)

      (Deprecated) See `GroupBy.mode()`.




   .. py:method:: nunique(values: groupable) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and return the number of unique values in each group.

      :param values: The values to group and find unique values
      :type values: pdarray, int64

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **group_nunique** (*groupable*) -- Number of unique values per unique key in the GroupBy instance

      :raises TypeError: Raised if the dtype(s) of values array(s) does/do not support
          the nunique method
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if nunique is not supported for the values dtype

      .. rubric:: Examples

      >>> data = ak.array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> data
      array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> labels = ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> labels
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g = ak.GroupBy(labels)
      >>> g.keys
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g.nunique(data)
      array([1,2,3,4]), array([2, 2, 3, 1])
      #    Group (1,1,1) has values [3,4,3] -> there are 2 unique values 3&4
      #    Group (2,2,2) has values [1,1,4] -> 2 unique values 1&4
      #    Group (3,3,3) has values [3,4,1] -> 3 unique values
      #    Group (4) has values [4] -> 1 unique value



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: prod(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the product of each group's
      values.

      :param values: The values to group and multiply
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_products** (*pdarray, float64*) -- One product per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if prod is not supported for the values dtype

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.prod(b)
      (array([2, 3, 4]), array([12, 108.00000000000003, 8.9999999999999982]))



   .. py:method:: register(user_defined_name: str) -> GroupBy

      Register this GroupBy object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the GroupBy is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same GroupBy which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different GroupBys with the same name.
      :rtype: GroupBy

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the GroupBy with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: sample(values: groupable, n=None, frac=None, replace=False, weights=None, random_state=None, return_indices=False, permute_samples=False)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param values: The values from which to sample, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the value more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the groupby keys
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional
      :param return_indices: if True, return the indices of the sampled values.
                             Otherwise, return the sample values.
      :type return_indices: bool, default False
      :param permute_samples: if True, return permute the samples according to group
                              Otherwise, keep samples in original order.
      :type permute_samples: bool, default False

      :returns: if return_indices is True, return the indices of the sampled values.
                Otherwise, return the sample values.
      :rtype: pdarray



   .. py:method:: size() -> Tuple[groupable, pdarray]

      Count the number of elements in each group, i.e. the number of times
      each key appears.  This counts the total number of rows (including NaN values).

      :param none:

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears

      .. seealso:: :obj:`count`

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 2, 3, 1, 2, 4, 3, 4, 3, 4])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> keys
      array([1, 2, 3, 4])
      >>> counts
      array([1, 2, 4, 3])



   .. py:method:: std(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the standard deviation of
      each group's values.

      :param values: The values to group and find standard deviation
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_stds** (*pdarray, float64*) -- One std value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The standard deviation is the square root of the average of the squared
      deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

      The average squared deviation is normally calculated as
      ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
      the divisor ``N - ddof`` is used instead. In standard statistical
      practice, ``ddof=1`` provides an unbiased estimator of the variance
      of the infinite population. ``ddof=0`` provides a maximum likelihood
      estimate of the variance for normally distributed variables. The
      standard deviation computed in this function is the square root of
      the estimated variance, so even with ``ddof=1``, it will not be an
      unbiased estimate of the standard deviation per se.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.std(b)
      (array([2 3 4]), array([1.5275252316519465 1.0954451150103321 0]))



   .. py:method:: sum(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and sum each group's values.

      :param values: The values to group and sum
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_sums** (*pdarray*) -- One sum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The grouped sum of a boolean ``pdarray`` returns integers.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.sum(b)
      (array([2, 3, 4]), array([8, 14, 6]))



   .. py:method:: tail(values: groupable_element_type, n: int = 5, return_indices: bool = True) -> Tuple[groupable, groupable_element_type]

      Return the last n values from each group.

      :param values: The values from which to select, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Maximum number of items to return for each group.
                If the number of values in a group is less than n,
                all the values from that group will be returned.
      :type n: int, optional, default = 5
      :param return_indices: If True, return the indices of the sampled values.
                             Otherwise, return the selected values.
      :type return_indices: bool, default False

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The last n items of each group.
                  If return_indices is True, the result are indices.
                  O.W. the result are values.

      .. rubric:: Examples

      >>> a = ak.arange(10) %3
      >>> a
      array([0 1 2 0 1 2 0 1 2 0])
      >>> v = ak.arange(10)
      >>> v
      array([0 1 2 3 4 5 6 7 8 9])
      >>> g = GroupBy(a)
      >>> unique_keys, idx = g.tail(v, 2, return_indices=True)
      >>> _, values = g.tail(v, 2, return_indices=False)
      >>> unique_keys
      array([0 1 2])
      >>> idx
      array([6 9 4 7 5 8])
      >>> values
      array([6 9 4 7 5 8])

      >>> v2 =  -2 * ak.arange(10)
      >>> v2
      array([0 -2 -4 -6 -8 -10 -12 -14 -16 -18])
      >>> _, idx2 = g.tail(v2, 2, return_indices=True)
      >>> _, values2 = g.tail(v2, 2, return_indices=False)
      >>> idx2
      array([6 9 4 7 5 8])
      >>> values2
      array([-12 -18 -8 -14 -10 -16])



   .. py:method:: to_hdf(prefix_path, dataset='groupby', mode='truncate', file_type='distribute')

      Save the GroupBy to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :returns: * *None*
                * *GroupBy is not currently supported by Parquet*



   .. py:method:: unique(values: groupable)

      Return the set of unique values in each group, as a SegArray.

      :param values: The values to unique
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) SegArray*) -- The unique values of each group

      :raises TypeError: Raised if values is or contains Strings or Categorical



   .. py:method:: unregister()

      Unregister this GroupBy object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_groupby_by_name(user_defined_name: str) -> None

      Function to unregister GroupBy object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the GroupBy object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'groupby', repack: bool = True)


   .. py:method:: var(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the variance of
      each group's values.

      :param values: The values to group and find variance
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_vars** (*pdarray, float64*) -- One var value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The variance is the average of the squared deviations from the mean,
      i.e.,  ``var = mean((x - x.mean())**2)``.

      The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
      If, however, `ddof` is specified, the divisor ``N - ddof`` is used
      instead.  In standard statistical practice, ``ddof=1`` provides an
      unbiased estimator of the variance of a hypothetical infinite population.
      ``ddof=0`` provides a maximum likelihood estimate of the variance for
      normally distributed variables.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.var(b)
      (array([2 3 4]), array([2.333333333333333 1.2 0]))



.. py:class:: IPv4(values)

   Bases: :py:obj:`arkouda.pdarrayclass.pdarray`


   Represent integers as IPv4 addresses.

   :param values: The integer IP addresses
   :type values: pdarray, int64

   :returns: The same IP addresses
   :rtype: IPv4

   .. rubric:: Notes

   This class is a thin wrapper around pdarray that mostly affects
   how values are displayed to the user. Operators and methods will
   typically treat this class like an int64 pdarray.


   .. py:method:: export_uint()


   .. py:method:: format(x)

      Format a single integer IP address as a string.



   .. py:method:: normalize(x)

      Take in an IP address as a string, integer, or IPAddress object,
      and convert it to an integer.



   .. py:method:: opeq(other, op)


   .. py:method:: register(user_defined_name)

      Register this IPv4 object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the IPv4 is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same IPv4 which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different IPv4s with the same name.
      :rtype: IPv4

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the IPv4 with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: special_objType
      :value: 'IPv4'



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', file_type: str = 'distribute')

      Override of the pdarray to_hdf to store the special object type



   .. py:method:: to_list()

      Export array as a list of integers.



   .. py:method:: to_ndarray()

      Export array as a numpy array of integers.



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'array', repack: bool = True)

      Override the pdarray implementation so that the special object type will be used.



   .. py:attribute:: values


.. py:class:: Index(values: Union[List, arkouda.pdarrayclass.pdarray, arkouda.Strings, arkouda.Categorical, pandas.Index, Index, pandas.Categorical], name: Optional[str] = None, allow_list=False, max_list_size=1000)

   .. py:method:: argsort(ascending=True)


   .. py:method:: concat(other)


   .. py:method:: equals(other: Index) -> arkouda.numpy.dtypes.bool_scalars

      Whether Indexes are the same size, and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the Indexes are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> i = ak.Index([1, 2, 3])
      >>> i_cpy = ak.Index([1, 2, 3])
      >>> i.equals(i_cpy)
      True
      >>> i2 = ak.Index([1, 2, 4])
      >>> i.equals(i2)
      False

      MultiIndex case:

      >>> arrays = [ak.array([1, 1, 2, 2]), ak.array(["red", "blue", "red", "blue"])]
      >>> m = ak.MultiIndex(arrays, names=["numbers2", "colors2"])
      >>> m.equals(m)
      True
      >>> arrays2 = [ak.array([1, 1, 2, 2]), ak.array(["red", "blue", "red", "green"])]
      >>> m2 = ak.MultiIndex(arrays2, names=["numbers2", "colors2"])
      >>> m.equals(m2)
      False



   .. py:method:: factory(index)
      :staticmethod:



   .. py:method:: from_return_msg(rep_msg)
      :classmethod:



   .. py:property:: index

      This is maintained to support older code


   .. py:property:: inferred_type
      :type: str


      Return a string of the type inferred from the values.


   .. py:method:: is_registered()

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: is_unique

      Property indicating if all values in the index are unique
      :rtype: bool - True if all values are unique, False otherwise.


   .. py:method:: lookup(key)


   .. py:method:: map(arg: Union[dict, arkouda.series.Series]) -> Index

      Map values of Index according to an input mapping.

      :param arg: The mapping correspondence.
      :type arg: dict or Series

      :returns: A new index with the values transformed by the mapping correspondence.
      :rtype: arkouda.index.Index

      :raises TypeError: Raised if arg is not of type dict or arkouda.Series.
          Raised if index values not of type pdarray, Categorical, or Strings.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> idx = ak.Index(ak.array([2, 3, 2, 3, 4]))
      >>> display(idx)
      Index(array([2 3 2 3 4]), dtype='int64')
      >>> idx.map({4: 25.0, 2: 30.0, 1: 7.0, 3: 5.0})
      Index(array([30.00000000000000000 5.00000000000000000 30.00000000000000000
      5.00000000000000000 25.00000000000000000]), dtype='float64')
      >>> s2 = ak.Series(ak.array(["a","b","c","d"]), index = ak.array([4,2,1,3]))
      >>> idx.map(s2)
      Index(array(['b', 'b', 'd', 'd', 'a']), dtype='<U0')



   .. py:attribute:: max_list_size
      :value: 1000



   .. py:method:: memory_usage(unit='B')

      Return the memory usage of the Index values.

      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: Bytes of memory consumed.
      :rtype: int

      .. seealso:: :obj:`arkouda.pdarrayclass.nbytes`, :obj:`arkouda.index.MultiIndex.memory_usage`, :obj:`arkouda.series.Series.memory_usage`, :obj:`arkouda.dataframe.DataFrame.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> idx = Index(ak.array([1, 2, 3]))
      >>> idx.memory_usage()
      24



   .. py:property:: names

      Return Index or MultiIndex names.


   .. py:property:: ndim

      Number of dimensions of the underlying data, by definition 1.

      .. seealso:: :obj:`MultiIndex.ndim`


   .. py:property:: nlevels

      Integer number of levels in this Index.
      An Index will always have 1 level.
      .. seealso:: :obj:`MultiIndex.nlevels`


   .. py:attribute:: objType
      :value: 'Index'


      Sequence used for indexing and alignment.

      The basic object storing axis labels for all DataFrame objects.

      :param values:
      :type values: List, pdarray, Strings, Categorical, pandas.Categorical, pandas.Index, or Index
      :param name: Name to be stored in the index.
      :type name: str, default=None
      :param allow_list = False: If False, list values will be converted to a pdarray.
                                 If True, list values will remain as a list, provided the data length is less than max_list_size.
      :param : If False, list values will be converted to a pdarray.
               If True, list values will remain as a list, provided the data length is less than max_list_size.
      :param max_list_size = 1000: This is the maximum allowed data length for the values to be stored as a list object.

      :raises ValueError: Raised if allow_list=True and the size of values is > max_list_size.

      .. seealso:: :obj:`MultiIndex`

      .. rubric:: Examples

      >>> ak.Index([1, 2, 3])
      Index(array([1 2 3]), dtype='int64')

      >>> ak.Index(list('abc'))
      Index(array(['a', 'b', 'c']), dtype='<U0')

      >>> ak.Index([1, 2, 3], allow_list=True)
      Index([1, 2, 3], dtype='int64')


   .. py:method:: register(user_defined_name)

      Register this Index object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Index is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Index which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Indexes with the same name.
      :rtype: Index

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Index with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: save(prefix_path: str, dataset: str = 'index', mode: str = 'truncate', compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the index to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str {'HDF5', 'Parquet'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: Raised if there is an error in parsing the prefix path pointing to
          file write location or if the mode parameter is neither truncate
          nor append
      :raises TypeError: Raised if any one of the prefix_path, dataset, or mode parameters
          is not a string.
          Raised if the Index values are a list.

      .. seealso:: :obj:`save_all`, :obj:`load`, :obj:`read`, :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Notes

      The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales``. If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      Previously all files saved in Parquet format were saved with a ``.parquet`` file extension.
      This will require you to use load as if you saved the file with the extension. Try this if
      an older file is not being found.
      Any file extension can be used. The file I/O does not rely on the extension to determine the
      file format.



   .. py:method:: set_dtype(dtype)

      Change the data type of the index

      Currently only aku.ip_address and ak.array are supported.



   .. py:property:: shape


   .. py:method:: to_csv(prefix_path: str, dataset: str = 'index', col_delim: str = ',', overwrite: bool = False)

              Write Index to CSV file(s). File will contain a single column with the pdarray data.
              All CSV Files written by Arkouda include a header denoting data types of the columns.

              Parameters
              -----------
              prefix_path: str
                  The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                  when they are written to disk.
              dataset: str
                  Column name to save the pdarray under. Defaults to "array".
              col_delim: str
                  Defaults to ",". Value to be used to separate columns within the file.
                  Please be sure that the value used DOES NOT appear in your dataset.
              overwrite: bool
                  Defaults to False. If True, any existing files matching your provided prefix_path will
                  be overwritten. If False, an error will be returned if existing files are found.

              Returns
              --------
              str reponse message

              Raises
              ------
              ValueError
                  Raised if all datasets are not present in all parquet files or if one or
                  more of the specified files do not exist.
              RuntimeError
                  Raised if one or more of the specified files cannot be opened.
                  If `allow_errors` is true this may be raised if no values are returned
                  from the server.
              TypeError
                  Raised if we receive an unknown arkouda_type returned from the server.
                  Raised if the Index values are a list.

              Notes
              ------
              - CSV format is not currently supported by load/load_all operations
              - The column delimiter is expected to be the same for column names and data
              - Be sure that column delimiters are not found within your data.
              - All CSV files must delimit rows using newline (`
      `) at this time.




   .. py:method:: to_dict(label)


   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'index', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the Index to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises TypeError: Raised if the Index values are a list.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.



   .. py:method:: to_list()


   .. py:method:: to_ndarray()


   .. py:method:: to_pandas()

      Return the equivalent Pandas Index.



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'index', mode: str = 'truncate', compression: Optional[str] = None)

      Save the Index to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises TypeError: Raised if the Index values are a list.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.



   .. py:method:: unregister()

      Unregister this Index object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'index', repack: bool = True)

      Overwrite the dataset with the name provided with this Index object. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the index

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, this will create a copy of the
        file with the new data



.. py:data:: Inf
   :type:  float

.. py:data:: Infinity
   :type:  float

.. py:class:: Int16DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: Int32DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: Int64DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: Int8DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: IntDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:data:: LEN_SUFFIX
   :value: '_lengths'


.. py:class:: LogLevel

   Bases: :py:obj:`enum.Enum`


   Generic enumeration.

   Derive from this class to define new enumerations.


   .. py:attribute:: CRITICAL
      :value: 'CRITICAL'



   .. py:attribute:: DEBUG
      :value: 'DEBUG'



   .. py:attribute:: ERROR
      :value: 'ERROR'



   .. py:attribute:: INFO
      :value: 'INFO'



   .. py:attribute:: WARN
      :value: 'WARN'



.. py:class:: LongDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: LongDoubleDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: LongLongDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: MultiIndex(data: Union[list, tuple, pandas.MultiIndex, MultiIndex], name: Optional[str] = None, names: Optional[list[str]] = None)

   Bases: :py:obj:`Index`


   .. py:method:: argsort(ascending=True)


   .. py:method:: concat(other)


   .. py:property:: dtype
      :type: numpy.dtype


      Return the dtype object of the underlying data.


   .. py:method:: equal_levels(other: MultiIndex) -> bool

      Return True if the levels of both MultiIndex objects are the same




   .. py:method:: get_level_values(level: Union[str, int])


   .. py:property:: index

      This is maintained to support older code


   .. py:property:: inferred_type
      :type: str


      Return a string of the type inferred from the values.


   .. py:method:: is_registered()

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: levels
      :type:  list


   .. py:method:: lookup(key)


   .. py:method:: memory_usage(unit='B')

      Return the memory usage of the MultiIndex levels.

      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: Bytes of memory consumed.
      :rtype: int

      .. seealso:: :obj:`arkouda.pdarrayclass.nbytes`, :obj:`arkouda.index.Index.memory_usage`, :obj:`arkouda.series.Series.memory_usage`, :obj:`arkouda.dataframe.DataFrame.memory_usage`

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> m = ak.index.MultiIndex([ak.array([1,2,3]),ak.array([4,5,6])])
      >>> m.memory_usage()
      48



   .. py:property:: name

      Return Index or MultiIndex name.


   .. py:property:: names

      Return Index or MultiIndex names.


   .. py:property:: ndim

      Number of dimensions of the underlying data, by definition 1.

      .. seealso:: :obj:`Index.ndim`


   .. py:property:: nlevels
      :type: int


      Integer number of levels in this MultiIndex.

      .. seealso:: :obj:`Index.nlevels`


   .. py:attribute:: objType
      :value: 'MultiIndex'


      Sequence used for indexing and alignment.

      The basic object storing axis labels for all DataFrame objects.

      :param values:
      :type values: List, pdarray, Strings, Categorical, pandas.Categorical, pandas.Index, or Index
      :param name: Name to be stored in the index.
      :type name: str, default=None
      :param allow_list = False: If False, list values will be converted to a pdarray.
                                 If True, list values will remain as a list, provided the data length is less than max_list_size.
      :param : If False, list values will be converted to a pdarray.
               If True, list values will remain as a list, provided the data length is less than max_list_size.
      :param max_list_size = 1000: This is the maximum allowed data length for the values to be stored as a list object.

      :raises ValueError: Raised if allow_list=True and the size of values is > max_list_size.

      .. seealso:: :obj:`MultiIndex`

      .. rubric:: Examples

      >>> ak.Index([1, 2, 3])
      Index(array([1 2 3]), dtype='int64')

      >>> ak.Index(list('abc'))
      Index(array(['a', 'b', 'c']), dtype='<U0')

      >>> ak.Index([1, 2, 3], allow_list=True)
      Index([1, 2, 3], dtype='int64')


   .. py:method:: register(user_defined_name)

      Register this Index object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Index is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Index which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Indexes with the same name.
      :rtype: MultiIndex

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Index with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: set_dtype(dtype)

      Change the data type of the index

      Currently only aku.ip_address and ak.array are supported.



   .. py:method:: to_dict(labels=None)


   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'index', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the Index to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.



   .. py:method:: to_list()


   .. py:method:: to_ndarray()


   .. py:method:: to_pandas()

      Return the equivalent Pandas Index.



   .. py:method:: unregister()

      Unregister this Index object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'index', repack: bool = True)

      Overwrite the dataset with the name provided with this Index object. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the index
      :raises TypeError: Raised if the Index levels are a list.

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, this will create a copy of the
        file with the new data



.. py:data:: NAN
   :type:  float

.. py:data:: NINF
   :type:  float

.. py:class:: NUMBER_FORMAT_STRINGS

   dict() -> new empty dictionary
   dict(mapping) -> new dictionary initialized from a mapping object's
       (key, value) pairs
   dict(iterable) -> new dictionary initialized as if via:
       d = {}
       for k, v in iterable:
           d[k] = v
   dict(**kwargs) -> new dictionary initialized with the name=value pairs
       in the keyword argument list.  For example:  dict(one=1, two=2)



   .. py:method:: clear(*args, **kwargs)

      D.clear() -> None.  Remove all items from D.




   .. py:method:: copy(*args, **kwargs)

      D.copy() -> a shallow copy of D




   .. py:method:: fromkeys(iterable, value=None, /)

      Create a new dictionary with keys from iterable and values set to value.




   .. py:method:: get(key, default=None, /)

      Return the value for key if key is in the dictionary, else default.




   .. py:method:: items(*args, **kwargs)

      D.items() -> a set-like object providing a view on D's items




   .. py:method:: keys(*args, **kwargs)

      D.keys() -> a set-like object providing a view on D's keys




   .. py:method:: pop(*args, **kwargs)

      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

      If key is not found, default is returned if given, otherwise KeyError is raised




   .. py:method:: popitem()

      Remove and return a (key, value) pair as a 2-tuple.

      Pairs are returned in LIFO (last-in, first-out) order.
      Raises KeyError if the dict is empty.




   .. py:method:: setdefault(key, default=None, /)

      Insert key with a value of default if key is not in the dictionary.

      Return the value for key if key is in the dictionary, else default.




   .. py:method:: update(*args, **kwargs)

      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
      In either case, this is followed by: for k in F:  D[k] = F[k]




   .. py:method:: values(*args, **kwargs)

      D.values() -> an object providing a view on D's values




.. py:data:: NZERO
   :type:  float

.. py:data:: NaN
   :type:  float

.. py:exception:: NonUniqueError

   Bases: :py:obj:`ValueError`


   Inappropriate argument value (of correct type).


.. py:class:: NumericDTypes

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: ObjectDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:data:: PINF
   :type:  float

.. py:data:: PZERO
   :type:  float

.. py:class:: Power_divergenceResult

   Bases: :py:obj:`Power_divergenceResult`


   The results of a power divergence statistical test.

   .. attribute:: statistic



      :type: numpy.float64

   .. attribute:: pvalue



      :type: numpy.float64


.. py:class:: Properties

.. py:class:: RankWarning

   Issued by `polyfit` when the Vandermonde matrix is rank deficient.

   For more information, a way to suppress the warning, and an example of
   `RankWarning` being issued, see `polyfit`.




.. py:data:: RegisteredSymbols
   :value: '__RegisteredSymbols__'


.. py:exception:: RegistrationError

   Bases: :py:obj:`Exception`


   Error/Exception used when the Arkouda Server cannot register an object


.. py:exception:: RegistrationError

   Bases: :py:obj:`Exception`


   Error/Exception used when the Arkouda Server cannot register an object


.. py:exception:: RegistrationError

   Bases: :py:obj:`Exception`


   Error/Exception used when the Arkouda Server cannot register an object


.. py:exception:: RegistrationError

   Bases: :py:obj:`Exception`


   Error/Exception used when the Arkouda Server cannot register an object


.. py:exception:: RegistrationError

   Bases: :py:obj:`Exception`


   Error/Exception used when the Arkouda Server cannot register an object


.. py:class:: Row(dict=None, /, **kwargs)

   Bases: :py:obj:`collections.UserDict`


   This class is useful for printing and working with individual rows of a
   of an aku.DataFrame.


.. py:data:: SEG_SUFFIX
   :value: '_segments'


.. py:class:: ScalarDTypes

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: ScalarType

   Built-in immutable sequence.

   If no argument is given, the constructor returns an empty tuple.
   If iterable is specified the tuple is initialized from iterable's items.

   If the argument is a tuple, the return value is the same object.



   .. py:method:: count(value, /)

      Return number of occurrences of value.




   .. py:method:: index(value, start=0, stop=9223372036854775807, /)

      Return first index of value.

      Raises ValueError if the value is not present.




.. py:class:: SegArray(segments, values, lengths=None, grouping=None)

   .. py:method:: AND(x=None)


   .. py:method:: OR(x=None)


   .. py:method:: XOR(x=None)


   .. py:method:: aggregate(op, x=None)


   .. py:method:: all(x=None)


   .. py:method:: any(x=None)


   .. py:method:: append(other, axis=0)

      Append other to self, either vertically (axis=0, length of resulting SegArray
      increases), or horizontally (axis=1, each sub-array of other appends to the
      corresponding sub-array of self).

      :param other: Array of sub-arrays to append
      :type other: SegArray
      :param axis: Whether to append vertically (0) or horizontally (1). If axis=1, other
                   must be same size as self.
      :type axis: 0 or 1

      :returns: axis=0: New SegArray containing all sub-arrays
                axis=1: New SegArray of same length, with pairs of sub-arrays concatenated
      :rtype: SegArray



   .. py:method:: append_single(x, prepend=False)

      Append a single value to each sub-array.

      :param x: Single value to append to each sub-array
      :type x: pdarray or scalar

      :returns: Copy of original SegArray with values from x appended to each sub-array
      :rtype: SegArray



   .. py:method:: argmax(x=None)


   .. py:method:: argmin(x=None)


   .. py:method:: attach(user_defined_name)
      :classmethod:


      Using the defined name, attach to a SegArray that has been registered to the Symbol Table

      :param user_defined_name: user defined name which the SegArray object was registered under
      :type user_defined_name: str

      :returns: The resulting SegArray
      :rtype: SegArray

      :raises RuntimeError: Raised if the server could not attach to the SegArray object

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`



   .. py:method:: concat(x, axis=0, ordered=True)
      :classmethod:


      Concatenate a sequence of SegArrays

      :param x: The SegArrays to concatenate
      :type x: sequence of SegArray
      :param axis: Select vertical (0) or horizontal (1) concatenation. If axis=1, all
                   SegArrays must have same size.
      :type axis: 0 or 1
      :param ordered: Must be True. This option is present for compatibility only, because unordered
                      concatenation is not yet supported.
      :type ordered: bool

      :returns: The input arrays joined into one SegArray
      :rtype: SegArray



   .. py:method:: copy()

      Return a deep copy.



   .. py:attribute:: dtype


   .. py:method:: filter(filter, discard_empty: bool = False)

      Filter values out of the SegArray object

      :param filter: The value/s to be filtered out of the SegArray
      :type filter: pdarray, list, or value
      :param discard_empty: Defaults to False. When True, empty segments are removed from
                            the return SegArray
      :type discard_empty: bool

      :rtype: SegArray



   .. py:method:: from_multi_array(m)
      :classmethod:


      Construct a SegArray from a list of columns. This essentially transposes the input,
      resulting in an array of rows.

      :param m: List of columns, the rows of which will form the sub-arrays of the output
      :type m: list of pdarray or Strings

      :returns: Array of rows of input
      :rtype: SegArray



   .. py:method:: from_parts(segments, values, lengths=None, grouping=None) -> SegArray
      :classmethod:


      DEPRECATED
      Construct a SegArray object from its parts

      :param segments: Start index of each sub-array in the flattened values array
      :type segments: pdarray, int64
      :param values: The flattened values of all sub-arrays
      :type values: pdarray
      :param lengths: The length of each segment
      :type lengths: pdarray
      :param grouping: grouping of segments
      :type grouping: GroupBy

      :returns: Data structure representing an array whose elements are variable-length arrays.
      :rtype: SegArray

      .. rubric:: Notes

      Keyword args 'lengths' and 'grouping' are not user-facing. They are used by the
      attach method.



   .. py:method:: from_return_msg(rep_msg) -> SegArray
      :classmethod:



   .. py:method:: get_jth(j, return_origins=True, compressed=False, default=0)

      Select the j-th element of each sub-array, where possible.

      :param j: The index of the value to get from each sub-array. If j is negative,
                it counts backwards from the end of each sub-array.
      :type j: int
      :param return_origins: If True, return a logical index indicating where j is in bounds
      :type return_origins: bool
      :param compressed: If False, return array is same size as self, with default value
                         where j is out of bounds. If True, the return array only contains
                         values where j is in bounds.
      :type compressed: bool
      :param default: When compressed=False, the value to return when j is out of bounds
                      for the sub-array
      :type default: scalar

      :returns: * **val** (*pdarray*) -- compressed=False: The j-th value of each sub-array where j is in
                  bounds and the default value where j is out of bounds.
                  compressed=True: The j-th values of only the sub-arrays where j is
                  in bounds
                * **origin_indices** (*pdarray, bool*) -- A Boolean array that is True where j is in bounds for the sub-array.

      .. rubric:: Notes

      If values are Strings, only the compressed format is supported.



   .. py:method:: get_length_n(n, return_origins=True)

      Return all sub-arrays of length n, as a list of columns.

      :param n: Length of sub-arrays to select
      :type n: int
      :param return_origins: Return a logical index indicating which sub-arrays are length n
      :type return_origins: bool

      :returns: * **columns** (*list of pdarray*) -- An n-long list of pdarray, where each row is one of the n-long
                  sub-arrays from the SegArray. The number of rows is the number of
                  True values in the returned mask.
                * **origin_indices** (*pdarray, bool*) -- Array of bool for each element of the SegArray, True where sub-array
                  has length n.



   .. py:method:: get_ngrams(n, return_origins=True)

      Return all n-grams from all sub-arrays.

      :param n: Length of n-gram
      :type n: int
      :param return_origins: If True, return an int64 array indicating which sub-array
                             each returned n-gram came from.
      :type return_origins: bool

      :returns: * **ngrams** (*list of pdarray*) -- An n-long list of pdarrays, essentially a table where each row is an n-gram.
                * **origin_indices** (*pdarray, int*) -- The index of the sub-array from which the corresponding n-gram originated



   .. py:method:: get_prefixes(n, return_origins=True, proper=True)

      Return all sub-array prefixes of length n (for sub-arrays that are at least n+1 long)

      :param n: Length of suffix
      :type n: int
      :param return_origins: If True, return a logical index indicating which sub-arrays
                             were long enough to return an n-prefix
      :type return_origins: bool
      :param proper: If True, only return proper prefixes, i.e. from sub-arrays
                     that are at least n+1 long. If False, allow the entire
                     sub-array to be returned as a prefix.
      :type proper: bool

      :returns: * **prefixes** (*list of pdarray*) -- An n-long list of pdarrays, essentially a table where each row is an n-prefix.
                  The number of rows is the number of True values in the returned mask.
                * **origin_indices** (*pdarray, bool*) -- Boolean array that is True where the sub-array was long enough to return
                  an n-suffix, False otherwise.



   .. py:method:: get_suffixes(n, return_origins=True, proper=True)

      Return the n-long suffix of each sub-array, where possible

      :param n: Length of suffix
      :type n: int
      :param return_origins: If True, return a logical index indicating which sub-arrays
                             were long enough to return an n-suffix
      :type return_origins: bool
      :param proper: If True, only return proper suffixes, i.e. from sub-arrays
                     that are at least n+1 long. If False, allow the entire
                     sub-array to be returned as a suffix.
      :type proper: bool

      :returns: * **suffixes** (*list of pdarray*) -- An n-long list of pdarrays, essentially a table where each row is an n-suffix.
                  The number of rows is the number of True values in the returned mask.
                * **origin_indices** (*pdarray, bool*) -- Boolean array that is True where the sub-array was long enough to return
                  an n-suffix, False otherwise.



   .. py:property:: grouping


   .. py:method:: hash() -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

      Compute a 128-bit hash of each segment.

      :returns: A tuple of two int64 pdarrays. The ith hash value is the concatenation
                of the ith values from each array.
      :rtype: Tuple[pdarray,pdarray]



   .. py:method:: intersect(other)

      Computes the intersection of 2 SegArrays.

      :param other: SegArray to compute against
      :type other: SegArray

      :returns: Segments are the 1d intersections of the segments of self and other
      :rtype: SegArray

      .. seealso:: :obj:`pdarraysetops.intersect1d`

      .. rubric:: Examples

      >>> a = [1, 2, 3, 1, 4]
      >>> b = [3, 1, 4, 5]
      >>> c = [1, 3, 3, 5]
      >>> d = [2, 2, 4]
      >>> seg_a = ak.segarray(ak.array([0, len(a)]), ak.array(a+b))
      >>> seg_b = ak.segarray(ak.array([0, len(c)]), ak.array(c+d))
      >>> seg_a.intersect(seg_b)
      SegArray([
      [1, 3],
      [4]
      ])



   .. py:method:: is_registered() -> bool

      Checks if the name of the SegArray object is registered in the Symbol Table

      :returns: True if SegArray is registered, false if not
      :rtype: bool

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`



   .. py:method:: load(prefix_path, dataset='segarray', segment_name='segments', value_name='values')
      :classmethod:



   .. py:attribute:: logger


   .. py:method:: max(x=None)


   .. py:method:: mean(x=None)


   .. py:method:: min(x=None)


   .. py:property:: nbytes

      The size of the segarray in bytes.

      :returns: The size of the segarray in bytes.
      :rtype: int


   .. py:property:: non_empty


   .. py:method:: nunique(x=None)


   .. py:attribute:: objType
      :value: 'SegArray'



   .. py:method:: prepend_single(x)


   .. py:method:: prod(x=None)


   .. py:method:: read_hdf(prefix_path, dataset='segarray')
      :classmethod:


      Load a saved SegArray from HDF5. All arguments must match what
      was supplied to SegArray.save()

      :param prefix_path: Directory and filename prefix
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 files
      :type dataset: str

      :rtype: SegArray



   .. py:method:: register(user_defined_name)

      Register this SegArray object and underlying components with the Arkouda server

      :param user_defined_name: user defined name which this SegArray object will be registered under
      :type user_defined_name: str

      :returns: The same SegArray which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different SegArrays with the same name.
      :rtype: SegArray

      :raises RegistrationError: Raised if the server could not register the SegArray object

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: remove_repeats(return_multiplicity=False)

      Condense sequences of repeated values within a sub-array to a single value.

      :param return_multiplicity: If True, also return the number of times each value was repeated.
      :type return_multiplicity: bool

      :returns: * **norepeats** (*SegArray*) -- Sub-arrays with runs of repeated values replaced with single value
                * **multiplicity** (*SegArray*) -- If return_multiplicity=True, this array contains the number of times
                  each value in the returned SegArray was repeated in the original SegArray.



   .. py:method:: save(prefix_path, dataset='segarray', mode='truncate', file_type='distribute')

      DEPRECATED
      Save the SegArray to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. seealso:: :obj:`to_hdf`, :obj:`load`



   .. py:attribute:: segments


   .. py:method:: set_jth(i, j, v)

      Set the j-th element of each sub-array in a subset.

      :param i: Indices of sub-arrays to set j-th element
      :type i: pdarray, int
      :param j: Index of value to set in each sub-array. If j is negative, it counts
                backwards from the end of the sub-array.
      :type j: int
      :param v: The value(s) to set. If v is a pdarray, it must have same length as i.
      :type v: pdarray or scalar

      :raises ValueError: If j is out of bounds in any of the sub-arrays specified by i.



   .. py:method:: setdiff(other)

      Computes the set difference of 2 SegArrays.

      :param other: SegArray to compute against
      :type other: SegArray

      :returns: Segments are the 1d set difference of the segments of self and other
      :rtype: SegArray

      .. seealso:: :obj:`pdarraysetops.setdiff1d`

      .. rubric:: Examples

      >>> a = [1, 2, 3, 1, 4]
      >>> b = [3, 1, 4, 5]
      >>> c = [1, 3, 3, 5]
      >>> d = [2, 2, 4]
      >>> seg_a = ak.segarray(ak.array([0, len(a)]), ak.array(a+b))
      >>> seg_b = ak.segarray(ak.array([0, len(c)]), ak.array(c+d))
      >>> seg_a.setdiff(seg_b)
      SegArray([
      [2, 4],
      [1, 3, 5]
      ])



   .. py:method:: setxor(other)

      Computes the symmetric difference of 2 SegArrays.

      :param other: SegArray to compute against
      :type other: SegArray

      :returns: Segments are the 1d symmetric difference of the segments of self and other
      :rtype: SegArray

      .. seealso:: :obj:`pdarraysetops.setxor1d`

      .. rubric:: Examples

      >>> a = [1, 2, 3, 1, 4]
      >>> b = [3, 1, 4, 5]
      >>> c = [1, 3, 3, 5]
      >>> d = [2, 2, 4]
      >>> seg_a = ak.segarray(ak.array([0, len(a)]), ak.array(a+b))
      >>> seg_b = ak.segarray(ak.array([0, len(c)]), ak.array(c+d))
      >>> seg_a.setxor(seg_b)
      SegArray([
      [2, 4, 5],
      [1, 3, 5, 2]
      ])



   .. py:attribute:: size


   .. py:method:: sum(x=None)


   .. py:method:: to_hdf(prefix_path, dataset='segarray', mode='truncate', file_type='distribute')

      Save the SegArray to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: None

      .. seealso:: :obj:`load`



   .. py:method:: to_list()

      Convert the segarray into a list containing sub-arrays

      :returns: A list with the same sub-arrays (also list) as this segarray
      :rtype: list

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> segarr = ak.SegArray(ak.array([0, 4, 7]), ak.arange(12))
      >>> segarr.to_list()
      [[0, 1, 2, 3], [4, 5, 6], [7, 8, 9, 10, 11]]
      >>> type(segarr.to_list())
      list



   .. py:method:: to_ndarray()

      Convert the array into a numpy.ndarray containing sub-arrays

      :returns: A numpy ndarray with the same sub-arrays (also numpy.ndarray) as this array
      :rtype: np.ndarray

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> segarr = ak.SegArray(ak.array([0, 4, 7]), ak.arange(12))
      >>> segarr.to_ndarray()
      array([array([1, 2, 3, 4]), array([5, 6, 7]), array([8, 9, 10, 11, 12])])
      >>> type(segarr.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path, dataset='segarray', mode: str = 'truncate', compression: Optional[str] = None)

      Save the SegArray object to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the object to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: Deprecated.
                   Parameter kept to maintain functionality of other calls. Only Truncate
                   supported.
                   By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: If write mode is not Truncate.

      .. rubric:: Notes

      - Append mode for Parquet has been deprecated. It was not implemented for SegArray.
      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a Segmented Array to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the Segmented Array is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: union(other)

      Computes the union of 2 SegArrays.

      :param other: SegArray to compute against
      :type other: SegArray

      :returns: Segments are the 1d union of the segments of self and other
      :rtype: SegArray

      .. seealso:: :obj:`pdarraysetops.union1d`

      .. rubric:: Examples

      >>> a = [1, 2, 3, 1, 4]
      >>> b = [3, 1, 4, 5]
      >>> c = [1, 3, 3, 5]
      >>> d = [2, 2, 4]
      >>> seg_a = ak.segarray(ak.array([0, len(a)]), ak.array(a+b))
      >>> seg_b = ak.segarray(ak.array([0, len(c)]), ak.array(c+d))
      >>> seg_a.union(seg_b)
      SegArray([
      [1, 2, 3, 4, 5],
      [1, 2, 3, 4, 5]
      ])



   .. py:method:: unique(x=None)

      Return sub-arrays of unique values.

      :param x: The values to unique, per group. By default, the values of this
                SegArray's sub-arrays.
      :type x: pdarray

      :returns: Same number of sub-arrays as original SegArray, but elements in sub-array
                are unique and in sorted order.
      :rtype: SegArray



   .. py:method:: unregister()

      Unregister this SegArray object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :rtype: None

      :raises RuntimeError: Raised if the server could not unregister the SegArray object from the Symbol Table

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: unregister_segarray_by_name(user_defined_name)
      :staticmethod:


      Using the defined name, remove the registered SegArray object from the Symbol Table

      :param user_defined_name: user defined name which the SegArray object was registered under
      :type user_defined_name: str

      :rtype: None

      :raises RuntimeError: Raised if the server could not unregister the SegArray object from the Symbol Table

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'segarray', repack: bool = True)

      Overwrite the dataset with the name provided with this SegArray object. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the SegArray

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, this will create a copy of the
        file with the new data



   .. py:attribute:: valsize


   .. py:attribute:: values


.. py:class:: Series

   One-dimensional arkouda array with axis labels.

   :param index: an array of indices associated with the data array.
                 If empty, it will default to a range of ints whose size match the size of the data.
                 optional
   :type index: pdarray, Strings
   :param data: a 1D array. Must not be None.
   :type data: Tuple, List, groupable_element_type, Series, SegArray

   :raises TypeError: Raised if index is not a pdarray or Strings object
       Raised if data is not a pdarray, Strings, or Categorical object
   :raises ValueError: Raised if the index size does not match data size

   .. rubric:: Notes

   The Series class accepts either positional arguments or keyword arguments.
   If entering positional arguments,
       2 arguments entered:
           argument 1 - data
           argument 2 - index
       1 argument entered:
           argument 1 - data
   If entering 1 positional argument, it is assumed that this is the data argument.
   If only 'data' argument is passed in, Index will automatically be generated.
   If entering keywords,
       'data' (see Parameters)
       'index' (optional) must match size of 'data'


   .. py:method:: add(b: Series) -> Series


   .. py:method:: argmax()


   .. py:method:: argmin()


   .. py:property:: at

      Accesses entries of a Series by label

      :param key: The key or container of keys to access entries for
      :type key: pdarray, Strings, Series, list, supported_scalars


   .. py:method:: attach(label: str, nkeys: int = 1) -> Series

      DEPRECATED
      Retrieve a series registered with arkouda

      :param label:
      :type label: name used to register the series
      :param nkeys:
      :type nkeys: number of keys, if a multi-index was registerd



   .. py:method:: concat(arrays: List, axis: int = 0, index_labels: Union[List[str], None] = None, value_labels: Union[List[str], None] = None, ordered=False) -> Union[arkouda.dataframe.DataFrame, Series]

      Concatenate in arkouda a list of arkouda Series or grouped arkouda arrays horizontally or
              vertically. If a list of grouped arkouda arrays is passed they are converted to a series. Each
              grouping is a 2-tuple with the first item being the key(s) and the second being the value.
              If horizontal, each series or grouping must have the same length and the same index. The index
              of the series is converted to a column in the dataframe.  If it is a multi-index,each level is
              converted to a column.

              Parameters
              ----------
              arrays:  The list of series/groupings to concat.
              axis  :  Whether or not to do a verticle (axis=0) or horizontal (axis=1) concatenation
              index_labels:  column names(s) to label the index.
              value_labels:  column names to label values of each series.
              ordered:  If True (default), the arrays will be appended in the order given. If False, array
                          data may be interleaved in blocks, which can greatly improve performance but
                          results in non-deterministic ordering of elements.

              Returns
              -------
              axis=0: an arkouda series.
              axis=1: an arkouda dataframe.





   .. py:method:: diff() -> Series

      Diffs consecutive values of the series.

              Returns a new series with the same index and length.  First value is set to NaN.





   .. py:method:: dt(series)


   .. py:property:: dtype


   .. py:method:: fillna(value) -> Series

      Fill NA/NaN values using the specified method.

      :param value: Value to use to fill holes (e.g. 0), alternately a
                    Series of values specifying which value to use for
                    each index.  Values not in the Series will not be filled.
                    This value cannot be a list.
      :type value: scalar, Series, or pdarray

      :returns: Object with missing values filled.
      :rtype: Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> from arkouda import Series

      >>> data = ak.Series([1, np.nan, 3, np.nan, 5])
      >>> data

      +----+-----+
      |    |   0 |
      +====+=====+
      |  0 |   1 |
      +----+-----+
      |  1 | nan |
      +----+-----+
      |  2 |   3 |
      +----+-----+
      |  3 | nan |
      +----+-----+
      |  4 |   5 |
      +----+-----+

      >>> fill_values1 = ak.ones(5)
      >>> data.fillna(fill_values1)

      +----+-----+
      |    |   0 |
      +====+=====+
      |  0 |   1 |
      +----+-----+
      |  1 |   1 |
      +----+-----+
      |  2 |   3 |
      +----+-----+
      |  3 |   1 |
      +----+-----+
      |  4 |   5 |
      +----+-----+

      >>> fill_values2 = Series(ak.ones(5))
      >>> data.fillna(fill_values2)

      +----+-----+
      |    |   0 |
      +====+=====+
      |  0 |   1 |
      +----+-----+
      |  1 |   1 |
      +----+-----+
      |  2 |   3 |
      +----+-----+
      |  3 |   1 |
      +----+-----+
      |  4 |   5 |
      +----+-----+

      >>> fill_values3 = 100.0
      >>> data.fillna(fill_values3)

      +----+-----+
      |    |   0 |
      +====+=====+
      |  0 |   1 |
      +----+-----+
      |  1 | 100 |
      +----+-----+
      |  2 |   3 |
      +----+-----+
      |  3 | 100 |
      +----+-----+
      |  4 |   5 |
      +----+-----+



   .. py:method:: from_return_msg(repMsg: str) -> Series

      Return a Series instance pointing to components created by the arkouda server.
      The user should not call this function directly.

      :param repMsg:
                     + delimited string containing the values and indexes
      :type repMsg: str

      :returns: A Series representing a set of pdarray components on the server
      :rtype: Series

      :raises RuntimeError: Raised if a server-side error is thrown in the process of creating
          the Series instance



   .. py:method:: has_repeat_labels() -> bool

      Returns whether the Series has any labels that appear more than once




   .. py:method:: hasnans() -> bool_scalars

      Return True if there are any NaNs.

      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = ak.Series(ak.array([1, 2, 3, np.nan]))
      >>> s

      >>> s.hasnans
      True



   .. py:method:: head(n: int = 10) -> Series

      Return the first n values of the series




   .. py:property:: iat
      :type: Series


      Accesses entries of a Series by position

      :param key: The positions or container of positions to access entries for
      :type key: int


   .. py:property:: iloc
      :type: Series


      Accesses entries of a Series by position

      :param key: The positions or container of positions to access entries for
      :type key: int


   .. py:method:: is_registered() -> bool

      Return True iff the object is contained in the registry or is a component of a
      registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isin(lst: Union[pdarray, Strings, List]) -> Series

      Find series elements whose values are in the specified list

              Input
              -----
              Either a python list or an arkouda array.

              Returns
              -------
              Arkouda boolean which is true for elements that are in the list and false otherwise.





   .. py:method:: isna() -> Series

      Detect missing values.

      Return a boolean same-sized object indicating if the values are NA. NA values,
      such as numpy.NaN, gets mapped to True values.
      Everything else gets mapped to False values.
      Characters such as empty strings '' are not considered NA values.

      :returns: Mask of bool values for each element in Series
                that indicates whether an element is an NA value.
      :rtype: arkouda.series.Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = Series(ak.array([1, 2, np.nan]), index = ak.array([1, 2, 4]))
      >>> s.isna()

      +----+---------+
      |    |   0     |
      +====+=========+
      |  1 |   False |
      +----+---------+
      |  2 |   False |
      +----+---------+
      |  4 |   True  |
      +----+---------+



   .. py:method:: isnull() -> Series

      Series.isnull is an alias for Series.isna.

      Detect missing values.

      Return a boolean same-sized object indicating if the values are NA. NA values,
      such as numpy.NaN, gets mapped to True values.
      Everything else gets mapped to False values.
      Characters such as empty strings '' are not considered NA values.

      :returns: Mask of bool values for each element in Series
                that indicates whether an element is an NA value.
      :rtype: arkouda.series.Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = Series(ak.array([1, 2, np.nan]), index = ak.array([1, 2, 4]))
      >>> s.isnull()

      +----+---------+
      |    |   0     |
      +====+=========+
      |  1 |   False |
      +----+---------+
      |  2 |   False |
      +----+---------+
      |  4 |   True  |
      +----+---------+



   .. py:property:: loc
      :type: Series


      Accesses entries of a Series by label

      :param key: The key or container of keys to access entries for
      :type key: pdarray, Strings, Series, list, supported_scalars


   .. py:method:: locate(key: Union[int, pdarray, Index, Series, List, Tuple]) -> Series

      Lookup values by index label

              The input can be a scalar, a list of scalers, or a list of lists (if the series has a
              MultiIndex). As a special case, if a Series is used as the key, the series labels are
              preserved with its values use as the key.

              Keys will be turned into arkouda arrays as needed.

              Returns
              -------
              A Series containing the values corresponding to the key.





   .. py:method:: map(arg: Union[dict, Series]) -> Series

      Map values of Series according to an input mapping.

      :param arg: The mapping correspondence.
      :type arg: dict or Series

      :returns: A new series with the same index as the caller.
                When the input Series has Categorical values,
                the return Series will have Strings values.
                Otherwise, the return type will match the input type.
      :rtype: arkouda.series.Series

      :raises TypeError: Raised if arg is not of type dict or arkouda.Series.
          Raised if series values not of type pdarray, Categorical, or Strings.

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> s = ak.Series(ak.array([2, 3, 2, 3, 4]))
      >>> display(s)

      +----+-----+
      |    | 0   |
      +====+=====+
      |  0 | 2   |
      +----+-----+
      |  1 | 3   |
      +----+-----+
      |  2 | 2   |
      +----+-----+
      |  3 | 3   |
      +----+-----+
      |  4 | 4   |
      +----+-----+

      >>> s.map({4: 25.0, 2: 30.0, 1: 7.0, 3: 5.0})

      +----+-----+
      |    | 0   |
      +====+=====+
      |  0 | 30.0|
      +----+-----+
      |  1 | 5.0 |
      +----+-----+
      |  2 | 30.0|
      +----+-----+
      |  3 | 5.0 |
      +----+-----+
      |  4 | 25.0|
      +----+-----+

      >>> s2 = ak.Series(ak.array(["a","b","c","d"]), index = ak.array([4,2,1,3]))
      >>> s.map(s2)

      +----+-----+
      |    | 0   |
      +====+=====+
      |  0 | b   |
      +----+-----+
      |  1 | b   |
      +----+-----+
      |  2 | d   |
      +----+-----+
      |  3 | d   |
      +----+-----+
      |  4 | a   |
      +----+-----+



   .. py:method:: max()


   .. py:method:: mean()


   .. py:method:: memory_usage(index: bool = True, unit='B') -> int

      Return the memory usage of the Series.

      The memory usage can optionally include the contribution of
      the index.

      :param index: Specifies whether to include the memory usage of the Series index.
      :type index: bool, default True
      :param unit: Unit to return. One of {'B', 'KB', 'MB', 'GB'}.
      :type unit: str, default = "B"

      :returns: Bytes of memory consumed.
      :rtype: int

      .. seealso:: :obj:`arkouda.pdarrayclass.nbytes`, :obj:`arkouda.index.Index.memory_usage`, :obj:`arkouda.series.Series.memory_usage`, :obj:`arkouda.dataframe.DataFrame.memory_usage`

      .. rubric:: Examples

      >>> from arkouda.series import Series
      >>> s = ak.Series(ak.arange(3))
      >>> s.memory_usage()
      48

      Not including the index gives the size of the rest of the data, which
      is necessarily smaller:

      >>> s.memory_usage(index=False)
      24

      Select the units:

      >>> s = ak.Series(ak.arange(3000))
      >>> s.memory_usage(unit="KB")
      46.875



   .. py:method:: min()


   .. py:property:: ndim


   .. py:method:: notna() -> Series

      Detect existing (non-missing) values.

      Return a boolean same-sized object indicating if the values are not NA.
      Non-missing values get mapped to True.
      Characters such as empty strings '' are not considered NA values.
      NA values, such as numpy.NaN, get mapped to False values.

      :returns: Mask of bool values for each element in Series
                that indicates whether an element is not an NA value.
      :rtype: arkouda.series.Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = Series(ak.array([1, 2, np.nan]), index = ak.array([1, 2, 4]))
      >>> s.notna()

      +----+---------+
      |    |   0     |
      +====+=========+
      |  1 |   True  |
      +----+---------+
      |  2 |   True  |
      +----+---------+
      |  4 |   False |
      +----+---------+



   .. py:method:: notnull() -> Series

      Series.notnull is an alias for Series.notna.

      Detect existing (non-missing) values.

      Return a boolean same-sized object indicating if the values are not NA.
      Non-missing values get mapped to True.
      Characters such as empty strings '' are not considered NA values.
      NA values, such as numpy.NaN, get mapped to False values.

      :returns: Mask of bool values for each element in Series
                that indicates whether an element is not an NA value.
      :rtype: arkouda.series.Series

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> from arkouda import Series
      >>> import numpy as np

      >>> s = Series(ak.array([1, 2, np.nan]), index = ak.array([1, 2, 4]))
      >>> s.notnull()

      +----+---------+
      |    |   0     |
      +====+=========+
      |  1 |   True  |
      +----+---------+
      |  2 |   True  |
      +----+---------+
      |  4 |   False |
      +----+---------+



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: pdconcat(arrays: List, axis: int = 0, labels: Union[Strings, None] = None) -> Union[pd.Series, pd.DataFrame]

      Concatenate a list of arkouda Series or grouped arkouda arrays, returning a PANDAS object.

              If a list of grouped arkouda arrays is passed they are converted to a series. Each grouping
              is a 2-tuple with the first item being the key(s) and the second being the value.

              If horizontal, each series or grouping must have the same length and the same index. The index of
              the series is converted to a column in the dataframe.  If it is a multi-index,each level is
              converted to a column.

              Parameters
              ----------
              arrays:  The list of series/groupings to concat.
              axis  :  Whether or not to do a verticle (axis=0) or horizontal (axis=1) concatenation
              labels:  names to give the columns of the data frame.

              Returns
              -------
              axis=0: a local PANDAS series
              axis=1: a local PANDAS dataframe





   .. py:method:: prod()


   .. py:method:: register(user_defined_name: str)

      Register this Series object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Series is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Series which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Series with the same name.
      :rtype: Series

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Series with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: shape


   .. py:method:: sort_index(ascending: bool = True) -> Series

      Sort the series by its index

              Parameters
              ----------
              ascending : bool
                  Sort values in ascending (default) or descending order.

              Returns
              -------
              A new Series sorted.





   .. py:method:: sort_values(ascending: bool = True) -> Series

      Sort the series numerically

              Parameters
              ----------
              ascending : bool
                  Sort values in ascending (default) or descending order.

              Returns
              -------
              A new Series sorted smallest to largest





   .. py:method:: std()


   .. py:method:: str_acc(series)


   .. py:method:: sum()


   .. py:method:: tail(n: int = 10) -> Series

      Return the last n values of the series




   .. py:method:: to_dataframe(index_labels: Union[List[str], None] = None, value_label: Union[str, None] = None) -> arkouda.dataframe.DataFrame

      Converts series to an arkouda data frame

              Parameters
              ----------
              index_labels:  column names(s) to label the index.
              value_label:  column name to label values.

              Returns
              -------
              An arkouda dataframe.





   .. py:method:: to_list() -> list


   .. py:method:: to_markdown(mode='wt', index=True, tablefmt='grid', storage_options=None, **kwargs)

      Print Series in Markdown-friendly format.

      :param mode: Mode in which file is opened, "wt" by default.
      :type mode: str, optional
      :param index: Add index (row) labels.
      :type index: bool, optional, default True
      :param tablefmt: Table format to call from tablulate:
                       https://pypi.org/project/tabulate/
      :type tablefmt: str = "grid"
      :param storage_options: Extra options that make sense for a particular storage connection,
                              e.g. host, port, username, password, etc., if using a URL that will be parsed by fsspec,
                              e.g., starting s3://, gcs://.
                              An error will be raised if providing this argument with a non-fsspec URL.
                              See the fsspec and backend storage implementation docs for the set
                              of allowed keys and values.
      :type storage_options: dict, optional
      :param \*\*kwargs: These parameters will be passed to tabulate.

      .. note::

         This function should only be called on small Series as it calls pandas.Series.to_markdown:
         https://pandas.pydata.org/docs/reference/api/pandas.Series.to_markdown.html

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> s = ak.Series(["elk", "pig", "dog", "quetzal"], name="animal")
      >>> print(s.to_markdown())
      |    | animal   |
      |---:|:---------|
      |  0 | elk      |
      |  1 | pig      |
      |  2 | dog      |
      |  3 | quetzal  |

      Output markdown with a tabulate option.

      >>> print(s.to_markdown(tablefmt="grid"))
      +----+----------+
      |    | animal   |
      +====+==========+
      |  0 | elk      |
      +----+----------+
      |  1 | pig      |
      +----+----------+
      |  2 | dog      |
      +----+----------+
      |  3 | quetzal  |
      +----+----------+



   .. py:method:: to_ndarray()


   .. py:method:: to_pandas() -> pd.Series

      Convert the series to a local PANDAS series




   .. py:method:: topn(n: int = 10) -> Series

      Return the top values of the series

              Parameters
              ----------
              n: Number of values to return

              Returns
              -------
              A new Series with the top values





   .. py:method:: unregister()

      Unregister this Series object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: validate_key(key: Union[Series, pdarray, Strings, Categorical, List, supported_scalars, SegArray]) -> Union[pdarray, Strings, Categorical, supported_scalars, SegArray]

      Validates type requirements for keys when reading or writing the Series.
      Also converts list and tuple arguments into pdarrays.

      :param key: The key or container of keys that might be used to index into the Series.
      :type key: Series, pdarray, Strings, Categorical, List, supported_scalars

      :rtype: The validated key(s), with lists and tuples converted to pdarrays

      :raises TypeError: Raised if keys are not boolean values or the type of the labels
          Raised if key is not one of the supported types
      :raises KeyError: Raised if container of keys has keys not present in the Series
      :raises IndexError: Raised if the length of a boolean key array is different
          from the Series



   .. py:method:: validate_val(val: Union[pdarray, Strings, supported_scalars, List]) -> Union[pdarray, Strings, supported_scalars]

      Validates type requirements for values being written into the Series.
      Also converts list and tuple arguments into pdarrays.

      :param val: The value or container of values that might be assigned into the Series.
      :type val: pdarray, Strings, list, supported_scalars

      :rtype: The validated value, with lists converted to pdarrays

      :raises TypeError: Raised if val is not the same type or a container with elements
            of the same time as the Series
          Raised if val is a string or Strings type.
          Raised if val is not one of the supported types



   .. py:method:: value_counts(sort: bool = True) -> Series

      Return a Series containing counts of unique values.

              The resulting object will be in descending order so that the
              first element is the most frequently-occurring element.

              Parameters
              ----------
              sort : Boolean. Whether or not to sort the results.  Default is true.





   .. py:method:: var()


.. py:class:: SeriesDTypes

   dict() -> new empty dictionary
   dict(mapping) -> new dictionary initialized from a mapping object's
       (key, value) pairs
   dict(iterable) -> new dictionary initialized as if via:
       d = {}
       for k, v in iterable:
           d[k] = v
   dict(**kwargs) -> new dictionary initialized with the name=value pairs
       in the keyword argument list.  For example:  dict(one=1, two=2)



   .. py:method:: clear(*args, **kwargs)

      D.clear() -> None.  Remove all items from D.




   .. py:method:: copy(*args, **kwargs)

      D.copy() -> a shallow copy of D




   .. py:method:: fromkeys(iterable, value=None, /)

      Create a new dictionary with keys from iterable and values set to value.




   .. py:method:: get(key, default=None, /)

      Return the value for key if key is in the dictionary, else default.




   .. py:method:: items(*args, **kwargs)

      D.items() -> a set-like object providing a view on D's items




   .. py:method:: keys(*args, **kwargs)

      D.keys() -> a set-like object providing a view on D's keys




   .. py:method:: pop(*args, **kwargs)

      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

      If key is not found, default is returned if given, otherwise KeyError is raised




   .. py:method:: popitem()

      Remove and return a (key, value) pair as a 2-tuple.

      Pairs are returned in LIFO (last-in, first-out) order.
      Raises KeyError if the dict is empty.




   .. py:method:: setdefault(key, default=None, /)

      Insert key with a value of default if key is not in the dictionary.

      Return the value for key if key is in the dictionary, else default.




   .. py:method:: update(*args, **kwargs)

      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
      In either case, this is followed by: for k in F:  D[k] = F[k]




   .. py:method:: values(*args, **kwargs)

      D.values() -> an object providing a view on D's values




.. py:class:: ShortDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:data:: SortingAlgorithm

.. py:class:: StrDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: StringAccessor(series)

   Bases: :py:obj:`Properties`


   .. py:attribute:: series


.. py:class:: TimeDelta64DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: Timedelta(pda, unit: str = _BASE_UNIT)

   Bases: :py:obj:`_AbstractBaseTime`


   Represents a duration, the difference between two dates or times.

   Timedelta is the Arkouda equivalent of pandas.TimedeltaIndex.

   :param pda:
   :type pda: int64 pdarray, pd.TimedeltaIndex, pd.Series, or np.timedelta64 array
   :param unit: For int64 pdarray, denotes the unit of the input. Ignored for pandas
                and numpy arrays, which carry their own unit. Not case-sensitive;
                prefixes of full names (like 'sec') are accepted.

                Possible values:

                * 'weeks' or 'w'
                * 'days' or 'd'
                * 'hours' or 'h'
                * 'minutes', 'm', or 't'
                * 'seconds' or 's'
                * 'milliseconds', 'ms', or 'l'
                * 'microseconds', 'us', or 'u'
                * 'nanoseconds', 'ns', or 'n'

                Unlike in pandas, units cannot be combined or mixed with integers
   :type unit: str, default 'ns'

   .. rubric:: Notes

   The ``.values`` attribute is always in nanoseconds with int64 dtype.


   .. py:method:: abs()

      Absolute value of time interval.



   .. py:property:: components


   .. py:property:: days


   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: microseconds


   .. py:property:: nanoseconds


   .. py:method:: register(user_defined_name)

      Register this Timedelta object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the timedelta is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Timedelta which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Timedeltas with the same name.
      :rtype: Timedelta

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the timedelta with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: seconds


   .. py:attribute:: special_objType
      :value: 'Timedelta'



   .. py:method:: std(ddof: arkouda.numpy.dtypes.int_scalars = 0)

      Returns the standard deviation as a pd.Timedelta object



   .. py:method:: sum()

      Return the sum of all elements in the array.



   .. py:attribute:: supported_opeq


   .. py:attribute:: supported_with_datetime


   .. py:attribute:: supported_with_pdarray


   .. py:attribute:: supported_with_r_datetime


   .. py:attribute:: supported_with_r_pdarray


   .. py:attribute:: supported_with_r_timedelta


   .. py:attribute:: supported_with_timedelta


   .. py:method:: to_pandas()

      Convert array to a pandas TimedeltaIndex. Note: if the array size
      exceeds client.maxTransferBytes, a RuntimeError is raised.

      .. seealso:: :obj:`to_ndarray`



   .. py:method:: total_seconds()


   .. py:method:: unregister()

      Unregister this timedelta object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



.. py:class:: Timedelta(pda, unit: str = _BASE_UNIT)

   Bases: :py:obj:`_AbstractBaseTime`


   Represents a duration, the difference between two dates or times.

   Timedelta is the Arkouda equivalent of pandas.TimedeltaIndex.

   :param pda:
   :type pda: int64 pdarray, pd.TimedeltaIndex, pd.Series, or np.timedelta64 array
   :param unit: For int64 pdarray, denotes the unit of the input. Ignored for pandas
                and numpy arrays, which carry their own unit. Not case-sensitive;
                prefixes of full names (like 'sec') are accepted.

                Possible values:

                * 'weeks' or 'w'
                * 'days' or 'd'
                * 'hours' or 'h'
                * 'minutes', 'm', or 't'
                * 'seconds' or 's'
                * 'milliseconds', 'ms', or 'l'
                * 'microseconds', 'us', or 'u'
                * 'nanoseconds', 'ns', or 'n'

                Unlike in pandas, units cannot be combined or mixed with integers
   :type unit: str, default 'ns'

   .. rubric:: Notes

   The ``.values`` attribute is always in nanoseconds with int64 dtype.


   .. py:method:: abs()

      Absolute value of time interval.



   .. py:property:: components


   .. py:property:: days


   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: microseconds


   .. py:property:: nanoseconds


   .. py:method:: register(user_defined_name)

      Register this Timedelta object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the timedelta is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Timedelta which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Timedeltas with the same name.
      :rtype: Timedelta

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the timedelta with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:property:: seconds


   .. py:attribute:: special_objType
      :value: 'Timedelta'



   .. py:method:: std(ddof: arkouda.numpy.dtypes.int_scalars = 0)

      Returns the standard deviation as a pd.Timedelta object



   .. py:method:: sum()

      Return the sum of all elements in the array.



   .. py:attribute:: supported_opeq


   .. py:attribute:: supported_with_datetime


   .. py:attribute:: supported_with_pdarray


   .. py:attribute:: supported_with_r_datetime


   .. py:attribute:: supported_with_r_pdarray


   .. py:attribute:: supported_with_r_timedelta


   .. py:attribute:: supported_with_timedelta


   .. py:method:: to_pandas()

      Convert array to a pandas TimedeltaIndex. Note: if the array size
      exceeds client.maxTransferBytes, a RuntimeError is raised.

      .. seealso:: :obj:`to_ndarray`



   .. py:method:: total_seconds()


   .. py:method:: unregister()

      Unregister this timedelta object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



.. py:class:: TooHardError

   max_work was exceeded.

       This is raised whenever the maximum number of candidate solutions
       to consider specified by the ``max_work`` parameter is exceeded.
       Assigning a finite number to max_work may have caused the operation
       to fail.





.. py:class:: True_(value)

   Bases: :py:obj:`numpy.generic`


   Boolean type (True or False), stored as a byte.

       .. warning::

          The :class:`bool_` type is not a subclass of the :class:`int_` type
          (the :class:`bool_` is not even a number type). This is different
          than Python's default implementation of :class:`bool` as a
          sub-class of :class:`int`.

       :Character code: ``'?'``



.. py:class:: UByteDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: UInt16DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: UInt32DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: UInt64DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: UInt8DType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: UIntDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: ULongDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: ULongLongDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:class:: UShortDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:data:: VAL_SUFFIX
   :value: '_values'


.. py:class:: VoidDType(obj, align=False, copy=False)

   Bases: :py:obj:`numpy.dtype`


   DType class corresponding to the scalar type and dtype of the same name.

   Please see `numpy.dtype` for the typical way to create
   dtype instances and :ref:`arrays.dtypes` for additional
   information.



.. py:function:: abs(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise absolute value of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing absolute values of the input array elements
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.abs(ak.arange(-5,-1))
   array([5 4 3 2])

   >>> ak.abs(ak.linspace(-5,-1,5))
   array([5.00000000000000000 4.00000000000000000 3.00000000000000000
   2.00000000000000000 1.00000000000000000])


.. py:function:: add_newdoc(place, obj, doc, warn_on_python=True)

   Add documentation to an existing object, typically one defined in C

   The purpose is to allow easier editing of the docstrings without requiring
   a re-compile. This exists primarily for internal use within numpy itself.

   :param place: The absolute name of the module to import from
   :type place: str
   :param obj: The name of the object to add documentation to, typically a class or
               function name
   :type obj: str
   :param doc: If a string, the documentation to apply to `obj`

               If a tuple, then the first element is interpreted as an attribute of
               `obj` and the second as the docstring to apply - ``(method, docstring)``

               If a list, then each element of the list should be a tuple of length
               two - ``[(method1, docstring1), (method2, docstring2), ...]``
   :type doc: {str, Tuple[str, str], List[Tuple[str, str]]}
   :param warn_on_python: If True, the default, emit `UserWarning` if this is used to attach
                          documentation to a pure-python object.
   :type warn_on_python: bool

   .. rubric:: Notes

   This routine never raises an error if the docstring can't be written, but
   will raise an error if the object being documented does not exist.

   This routine cannot modify read-only docstrings, as appear
   in new-style classes or built-in functions. Because this
   routine never raises an error the caller must check manually
   that the docstrings were changed.

   Since this function grabs the ``char *`` from a c-level str object and puts
   it into the ``tp_doc`` slot of the type of `obj`, it violates a number of
   C-API best-practices, by:

   - modifying a `PyTypeObject` after calling `PyType_Ready`
   - calling `Py_INCREF` on the str and losing the reference, so the str
     will never be released

   If possible it should be avoided.


.. py:function:: akabs(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise absolute value of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing absolute values of the input array elements
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.abs(ak.arange(-5,-1))
   array([5 4 3 2])

   >>> ak.abs(ak.linspace(-5,-1,5))
   array([5.00000000000000000 4.00000000000000000 3.00000000000000000
   2.00000000000000000 1.00000000000000000])


.. py:class:: akbool(value)

   Bases: :py:obj:`numpy.generic`


   Boolean type (True or False), stored as a byte.

       .. warning::

          The :class:`bool_` type is not a subclass of the :class:`int_` type
          (the :class:`bool_` is not even a number type). This is different
          than Python's default implementation of :class:`bool` as a
          sub-class of :class:`int`.

       :Character code: ``'?'``



.. py:class:: akbool(value)

   Bases: :py:obj:`numpy.generic`


   Boolean type (True or False), stored as a byte.

       .. warning::

          The :class:`bool_` type is not a subclass of the :class:`int_` type
          (the :class:`bool_` is not even a number type). This is different
          than Python's default implementation of :class:`bool` as a
          sub-class of :class:`int`.

       :Character code: ``'?'``



.. py:function:: akcast(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical], dt: Union[numpy.dtype, type, str, dtypes.dtypes.bigint], errors: _numeric.ErrorMode = ErrorMode.strict) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical, Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]]

   Cast an array to another dtype.

   :param pda: The array of values to cast
   :type pda: pdarray, Strings, or Categorical
   :param dt: The target dtype to cast values to
   :type dt: np.dtype, type, str, or bigint
   :param errors: Controls how errors are handled when casting strings to a numeric type
                  (ignored for casts from numeric types).
                      - strict: raise RuntimeError if *any* string cannot be converted
                      - ignore: never raise an error. Uninterpretable strings get
                          converted to NaN (float64), -2**63 (int64), zero (uint64 and
                          uint8), or False (bool)
                      - return_validity: in addition to returning the same output as
                        "ignore", also return a bool array indicating where the cast
                        was successful.
                  Default set to strict.
   :type errors: {strict, ignore, return_validity}, default=ErrorMode.strict

   :returns: * *pdarray or Strings* -- Array of values cast to desired dtype
             * **[validity** (*pdarray(bool)]*) -- If errors="return_validity" and input is Strings, a second array is
               returned with True where the cast succeeded and False where it failed.

   .. rubric:: Notes

   The cast is performed according to Chapel's casting rules and is NOT safe
   from overflows or underflows. The user must ensure that the target dtype
   has the precision and capacity to hold the desired result.

   .. rubric:: Examples

   >>> ak.cast(ak.linspace(1.0,5.0,5), dt=ak.int64)
   array([1 2 3 4 5])

   >>> ak.cast(ak.arange(0,5), dt=ak.float64).dtype
   dtype('float64')

   >>> ak.cast(ak.arange(0,5), dt=ak.bool_)
   array([False True True True True])

   >>> ak.cast(ak.linspace(0,4,5), dt=ak.bool_)
   array([False True True True True])


.. py:function:: akcast(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical], dt: Union[numpy.dtype, type, str, dtypes.dtypes.bigint], errors: _numeric.ErrorMode = ErrorMode.strict) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical, Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]]

   Cast an array to another dtype.

   :param pda: The array of values to cast
   :type pda: pdarray, Strings, or Categorical
   :param dt: The target dtype to cast values to
   :type dt: np.dtype, type, str, or bigint
   :param errors: Controls how errors are handled when casting strings to a numeric type
                  (ignored for casts from numeric types).
                      - strict: raise RuntimeError if *any* string cannot be converted
                      - ignore: never raise an error. Uninterpretable strings get
                          converted to NaN (float64), -2**63 (int64), zero (uint64 and
                          uint8), or False (bool)
                      - return_validity: in addition to returning the same output as
                        "ignore", also return a bool array indicating where the cast
                        was successful.
                  Default set to strict.
   :type errors: {strict, ignore, return_validity}, default=ErrorMode.strict

   :returns: * *pdarray or Strings* -- Array of values cast to desired dtype
             * **[validity** (*pdarray(bool)]*) -- If errors="return_validity" and input is Strings, a second array is
               returned with True where the cast succeeded and False where it failed.

   .. rubric:: Notes

   The cast is performed according to Chapel's casting rules and is NOT safe
   from overflows or underflows. The user must ensure that the target dtype
   has the precision and capacity to hold the desired result.

   .. rubric:: Examples

   >>> ak.cast(ak.linspace(1.0,5.0,5), dt=ak.int64)
   array([1 2 3 4 5])

   >>> ak.cast(ak.arange(0,5), dt=ak.float64).dtype
   dtype('float64')

   >>> ak.cast(ak.arange(0,5), dt=ak.bool_)
   array([False True True True True])

   >>> ak.cast(ak.linspace(0,4,5), dt=ak.bool_)
   array([False True True True True])


.. py:class:: akfloat64(value)

   Bases: :py:obj:`numpy.floating`


   Double-precision floating-point number type, compatible with Python `float`
       and C ``double``.

       :Character code: ``'d'``
       :Canonical name: `numpy.double`
       :Alias: `numpy.float_`
       :Alias on this platform (Linux x86_64): `numpy.float64`: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      double.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.double(10.0).as_integer_ratio()
              (10, 1)
              >>> np.double(0.0).as_integer_ratio()
              (0, 1)
              >>> np.double(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: fromhex(string, /)

      Create a floating-point number from a hexadecimal string.

      >>> float.fromhex('0x1.ffffp10')
      2047.984375
      >>> float.fromhex('-0x1p-1074')
      -5e-324




   .. py:method:: hex(/)

      Return a hexadecimal representation of a floating-point number.

      >>> (-0.1).hex()
      '-0x1.999999999999ap-4'
      >>> 3.14159.hex()
      '0x1.921f9f01b866ep+1'




   .. py:method:: is_integer(*args, **kwargs)

      double.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.double(-2.0).is_integer()
              True
              >>> np.double(3.2).is_integer()
              False




.. py:class:: akfloat64(value)

   Bases: :py:obj:`numpy.floating`


   Double-precision floating-point number type, compatible with Python `float`
       and C ``double``.

       :Character code: ``'d'``
       :Canonical name: `numpy.double`
       :Alias: `numpy.float_`
       :Alias on this platform (Linux x86_64): `numpy.float64`: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      double.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.double(10.0).as_integer_ratio()
              (10, 1)
              >>> np.double(0.0).as_integer_ratio()
              (0, 1)
              >>> np.double(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: fromhex(string, /)

      Create a floating-point number from a hexadecimal string.

      >>> float.fromhex('0x1.ffffp10')
      2047.984375
      >>> float.fromhex('-0x1p-1074')
      -5e-324




   .. py:method:: hex(/)

      Return a hexadecimal representation of a floating-point number.

      >>> (-0.1).hex()
      '-0x1.999999999999ap-4'
      >>> 3.14159.hex()
      '0x1.921f9f01b866ep+1'




   .. py:method:: is_integer(*args, **kwargs)

      double.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.double(-2.0).is_integer()
              True
              >>> np.double(3.2).is_integer()
              False




.. py:class:: akint64(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with Python `int` and C ``long``.

       :Character code: ``'l'``
       :Canonical name: `numpy.int_`
       :Alias on this platform (Linux x86_64): `numpy.int64`: 64-bit signed integer (``-9_223_372_036_854_775_808`` to ``9_223_372_036_854_775_807``).
       :Alias on this platform (Linux x86_64): `numpy.intp`: Signed integer large enough to fit pointer, compatible with C ``intptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      int64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int64(127).bit_count()
              7
              >>> np.int64(-127).bit_count()
              7




.. py:class:: akint64(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with Python `int` and C ``long``.

       :Character code: ``'l'``
       :Canonical name: `numpy.int_`
       :Alias on this platform (Linux x86_64): `numpy.int64`: 64-bit signed integer (``-9_223_372_036_854_775_808`` to ``9_223_372_036_854_775_807``).
       :Alias on this platform (Linux x86_64): `numpy.intp`: Signed integer large enough to fit pointer, compatible with C ``intptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      int64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int64(127).bit_count()
              7
              >>> np.int64(-127).bit_count()
              7




.. py:class:: akint64(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with Python `int` and C ``long``.

       :Character code: ``'l'``
       :Canonical name: `numpy.int_`
       :Alias on this platform (Linux x86_64): `numpy.int64`: 64-bit signed integer (``-9_223_372_036_854_775_808`` to ``9_223_372_036_854_775_807``).
       :Alias on this platform (Linux x86_64): `numpy.intp`: Signed integer large enough to fit pointer, compatible with C ``intptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      int64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int64(127).bit_count()
              7
              >>> np.int64(-127).bit_count()
              7




.. py:class:: akuint64(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned long``.

       :Character code: ``'L'``
       :Canonical name: `numpy.uint`
       :Alias on this platform (Linux x86_64): `numpy.uint64`: 64-bit unsigned integer (``0`` to ``18_446_744_073_709_551_615``).
       :Alias on this platform (Linux x86_64): `numpy.uintp`: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      uint64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint64(127).bit_count()
              7




.. py:class:: akuint64(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned long``.

       :Character code: ``'L'``
       :Canonical name: `numpy.uint`
       :Alias on this platform (Linux x86_64): `numpy.uint64`: 64-bit unsigned integer (``0`` to ``18_446_744_073_709_551_615``).
       :Alias on this platform (Linux x86_64): `numpy.uintp`: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      uint64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint64(127).bit_count()
              7




.. py:class:: akuint64(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned long``.

       :Character code: ``'L'``
       :Canonical name: `numpy.uint`
       :Alias on this platform (Linux x86_64): `numpy.uint64`: 64-bit unsigned integer (``0`` to ``18_446_744_073_709_551_615``).
       :Alias on this platform (Linux x86_64): `numpy.uintp`: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      uint64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint64(127).bit_count()
              7




.. py:function:: align(*args)

   Map multiple arrays of sparse identifiers to a common 0-up index.

   :param \*args: Arrays to map to dense index
   :type \*args: pdarrays or sequences of pdarrays

   :returns: **aligned** -- Arrays with values replaced by 0-up indices
   :rtype: list of pdarrays


.. py:class:: all_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:function:: apply(arr: arkouda.pdarrayclass.pdarray, func: Union[Callable, str], result_dtype: Optional[Union[numpy.dtype, str]] = None) -> arkouda.pdarrayclass.pdarray

   Apply a python function to a pdarray. The function should take one argument
   and return a new value. The function will then be called on each element in
   the pdarray.

   For example,
   >>> ak.apply(ak.array([1, 2, 3]), lambda x: x+1)

   Or,
   >>> import math
   >>> arr = ak.randint(0, 10, 10_000)
   >>> def times_pi(x):
           return x*math.pi
   >>> ak.apply(arr, times_pi, "float64")

   Warning: This function is experimental and may not work as expected.
   Known limitations:
   - Any python modules used inside of the function must be installed on the server.

   :param arr: The pdarray to which the function is applied
   :type arr: pdarray
   :param func: The function to apply to the array. This can be a callable function or
                a string, but either way it should take a single argument and return a
                single value. If a string, it should be a lambda function that takes a
                single argument, e.g. "lambda x,: x+1". Note the dangling comma after
                the argument, this is required for string functions.
   :type func: Union[Callable, str]
   :param result_dtype: The dtype of the resulting pdarray. If None, the dtype of the resulting
                        pdarray will be the same as the input pdarray. If a string, it should be
                        a valid numpy dtype string, e.g. "float64". If a numpy dtype, it should
                        be a valid numpy dtype object, e.g. np.float64. This is not supported
                        for functions passed as strings.
   :type result_dtype: Optional[Union[np.dtype, str]]

   :returns: The pdarray resulting from applying the function to the input array
   :rtype: pdarray


.. py:function:: arange(*args, **kwargs) -> arkouda.pdarrayclass.pdarray

   arange([start,] stop[, stride,] dtype=int64)

   Create a pdarray of consecutive integers within the interval [start, stop).
   If only one arg is given then arg is the stop parameter. If two args are
   given, then the first arg is start and second is stop. If three args are
   given, then the first arg is start, second is stop, third is stride.

   The return value is cast to type dtype

   :param start: Starting value (inclusive)
   :type start: int_scalars, optional
   :param stop: Stopping value (exclusive)
   :type stop: int_scalars
   :param stride: The difference between consecutive elements, the default stride is 1,
                  if stride is specified then start must also be specified.
   :type stride: int_scalars, optional
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Integers from start (inclusive) to stop (exclusive) by stride
   :rtype: pdarray, dtype

   :raises TypeError: Raised if start, stop, or stride is not an int object
   :raises ZeroDivisionError: Raised if stride == 0
   :raises ValueError: Raised if (stop - start) and stride are not the same sign, or if stop==start

   .. seealso:: :obj:`linspace`, :obj:`zeros`, :obj:`ones`, :obj:`randint`

   .. rubric:: Notes

   Negative strides result in decreasing values. Currently, only int64
   pdarrays can be created with this method. For float64 arrays, use
   the linspace method.

   .. rubric:: Examples

   >>> ak.arange(0, 5, 1)
   array([0 1 2 3 4])

   >>> ak.arange(5, 0, -1)
   array([5 4 3 2 1])

   >>> ak.arange(0, 10, 2)
   array([0 2 4 6 8])

   >>> ak.arange(-5, -10, -1)
   array([-5 -6 -7 -8 -9])


.. py:function:: arange(*args, **kwargs) -> arkouda.pdarrayclass.pdarray

   arange([start,] stop[, stride,] dtype=int64)

   Create a pdarray of consecutive integers within the interval [start, stop).
   If only one arg is given then arg is the stop parameter. If two args are
   given, then the first arg is start and second is stop. If three args are
   given, then the first arg is start, second is stop, third is stride.

   The return value is cast to type dtype

   :param start: Starting value (inclusive)
   :type start: int_scalars, optional
   :param stop: Stopping value (exclusive)
   :type stop: int_scalars
   :param stride: The difference between consecutive elements, the default stride is 1,
                  if stride is specified then start must also be specified.
   :type stride: int_scalars, optional
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Integers from start (inclusive) to stop (exclusive) by stride
   :rtype: pdarray, dtype

   :raises TypeError: Raised if start, stop, or stride is not an int object
   :raises ZeroDivisionError: Raised if stride == 0
   :raises ValueError: Raised if (stop - start) and stride are not the same sign, or if stop==start

   .. seealso:: :obj:`linspace`, :obj:`zeros`, :obj:`ones`, :obj:`randint`

   .. rubric:: Notes

   Negative strides result in decreasing values. Currently, only int64
   pdarrays can be created with this method. For float64 arrays, use
   the linspace method.

   .. rubric:: Examples

   >>> ak.arange(0, 5, 1)
   array([0 1 2 3 4])

   >>> ak.arange(5, 0, -1)
   array([5 4 3 2 1])

   >>> ak.arange(0, 10, 2)
   array([0 2 4 6 8])

   >>> ak.arange(-5, -10, -1)
   array([-5 -6 -7 -8 -9])


.. py:function:: arange(*args, **kwargs) -> arkouda.pdarrayclass.pdarray

   arange([start,] stop[, stride,] dtype=int64)

   Create a pdarray of consecutive integers within the interval [start, stop).
   If only one arg is given then arg is the stop parameter. If two args are
   given, then the first arg is start and second is stop. If three args are
   given, then the first arg is start, second is stop, third is stride.

   The return value is cast to type dtype

   :param start: Starting value (inclusive)
   :type start: int_scalars, optional
   :param stop: Stopping value (exclusive)
   :type stop: int_scalars
   :param stride: The difference between consecutive elements, the default stride is 1,
                  if stride is specified then start must also be specified.
   :type stride: int_scalars, optional
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Integers from start (inclusive) to stop (exclusive) by stride
   :rtype: pdarray, dtype

   :raises TypeError: Raised if start, stop, or stride is not an int object
   :raises ZeroDivisionError: Raised if stride == 0
   :raises ValueError: Raised if (stop - start) and stride are not the same sign, or if stop==start

   .. seealso:: :obj:`linspace`, :obj:`zeros`, :obj:`ones`, :obj:`randint`

   .. rubric:: Notes

   Negative strides result in decreasing values. Currently, only int64
   pdarrays can be created with this method. For float64 arrays, use
   the linspace method.

   .. rubric:: Examples

   >>> ak.arange(0, 5, 1)
   array([0 1 2 3 4])

   >>> ak.arange(5, 0, -1)
   array([5 4 3 2 1])

   >>> ak.arange(0, 10, 2)
   array([0 2 4 6 8])

   >>> ak.arange(-5, -10, -1)
   array([-5 -6 -7 -8 -9])


.. py:function:: arange(*args, **kwargs) -> arkouda.pdarrayclass.pdarray

   arange([start,] stop[, stride,] dtype=int64)

   Create a pdarray of consecutive integers within the interval [start, stop).
   If only one arg is given then arg is the stop parameter. If two args are
   given, then the first arg is start and second is stop. If three args are
   given, then the first arg is start, second is stop, third is stride.

   The return value is cast to type dtype

   :param start: Starting value (inclusive)
   :type start: int_scalars, optional
   :param stop: Stopping value (exclusive)
   :type stop: int_scalars
   :param stride: The difference between consecutive elements, the default stride is 1,
                  if stride is specified then start must also be specified.
   :type stride: int_scalars, optional
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Integers from start (inclusive) to stop (exclusive) by stride
   :rtype: pdarray, dtype

   :raises TypeError: Raised if start, stop, or stride is not an int object
   :raises ZeroDivisionError: Raised if stride == 0
   :raises ValueError: Raised if (stop - start) and stride are not the same sign, or if stop==start

   .. seealso:: :obj:`linspace`, :obj:`zeros`, :obj:`ones`, :obj:`randint`

   .. rubric:: Notes

   Negative strides result in decreasing values. Currently, only int64
   pdarrays can be created with this method. For float64 arrays, use
   the linspace method.

   .. rubric:: Examples

   >>> ak.arange(0, 5, 1)
   array([0 1 2 3 4])

   >>> ak.arange(5, 0, -1)
   array([5 4 3 2 1])

   >>> ak.arange(0, 10, 2)
   array([0 2 4 6 8])

   >>> ak.arange(-5, -10, -1)
   array([-5 -6 -7 -8 -9])


.. py:function:: arange(*args, **kwargs) -> arkouda.pdarrayclass.pdarray

   arange([start,] stop[, stride,] dtype=int64)

   Create a pdarray of consecutive integers within the interval [start, stop).
   If only one arg is given then arg is the stop parameter. If two args are
   given, then the first arg is start and second is stop. If three args are
   given, then the first arg is start, second is stop, third is stride.

   The return value is cast to type dtype

   :param start: Starting value (inclusive)
   :type start: int_scalars, optional
   :param stop: Stopping value (exclusive)
   :type stop: int_scalars
   :param stride: The difference between consecutive elements, the default stride is 1,
                  if stride is specified then start must also be specified.
   :type stride: int_scalars, optional
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Integers from start (inclusive) to stop (exclusive) by stride
   :rtype: pdarray, dtype

   :raises TypeError: Raised if start, stop, or stride is not an int object
   :raises ZeroDivisionError: Raised if stride == 0
   :raises ValueError: Raised if (stop - start) and stride are not the same sign, or if stop==start

   .. seealso:: :obj:`linspace`, :obj:`zeros`, :obj:`ones`, :obj:`randint`

   .. rubric:: Notes

   Negative strides result in decreasing values. Currently, only int64
   pdarrays can be created with this method. For float64 arrays, use
   the linspace method.

   .. rubric:: Examples

   >>> ak.arange(0, 5, 1)
   array([0 1 2 3 4])

   >>> ak.arange(5, 0, -1)
   array([5 4 3 2 1])

   >>> ak.arange(0, 10, 2)
   array([0 2 4 6 8])

   >>> ak.arange(-5, -10, -1)
   array([-5 -6 -7 -8 -9])


.. py:function:: arange(*args, **kwargs) -> arkouda.pdarrayclass.pdarray

   arange([start,] stop[, stride,] dtype=int64)

   Create a pdarray of consecutive integers within the interval [start, stop).
   If only one arg is given then arg is the stop parameter. If two args are
   given, then the first arg is start and second is stop. If three args are
   given, then the first arg is start, second is stop, third is stride.

   The return value is cast to type dtype

   :param start: Starting value (inclusive)
   :type start: int_scalars, optional
   :param stop: Stopping value (exclusive)
   :type stop: int_scalars
   :param stride: The difference between consecutive elements, the default stride is 1,
                  if stride is specified then start must also be specified.
   :type stride: int_scalars, optional
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Integers from start (inclusive) to stop (exclusive) by stride
   :rtype: pdarray, dtype

   :raises TypeError: Raised if start, stop, or stride is not an int object
   :raises ZeroDivisionError: Raised if stride == 0
   :raises ValueError: Raised if (stop - start) and stride are not the same sign, or if stop==start

   .. seealso:: :obj:`linspace`, :obj:`zeros`, :obj:`ones`, :obj:`randint`

   .. rubric:: Notes

   Negative strides result in decreasing values. Currently, only int64
   pdarrays can be created with this method. For float64 arrays, use
   the linspace method.

   .. rubric:: Examples

   >>> ak.arange(0, 5, 1)
   array([0 1 2 3 4])

   >>> ak.arange(5, 0, -1)
   array([5 4 3 2 1])

   >>> ak.arange(0, 10, 2)
   array([0 2 4 6 8])

   >>> ak.arange(-5, -10, -1)
   array([-5 -6 -7 -8 -9])


.. py:function:: arccos(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise inverse cosine of the array. The result is between 0 and pi.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the inverse cosine will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing inverse cosine for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: arccosh(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise inverse hyperbolic cosine of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the inverse hyperbolic cosine will be applied to the corresponding value. Elsewhere, it will
                 retain its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing inverse hyperbolic cosine for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: arcsin(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise inverse sine of the array. The result is between -pi/2 and pi/2.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the inverse sine will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing inverse sine for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: arcsinh(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise inverse hyperbolic sine of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the inverse hyperbolic sine will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing inverse hyperbolic sine for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: arctan(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise inverse tangent of the array. The result is between -pi/2 and pi/2.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the inverse tangent will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing inverse tangent for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: arctan2(num: Union[arkouda.pdarrayclass.pdarray, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64], denom: Union[arkouda.pdarrayclass.pdarray, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64], where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise inverse tangent of the array pair. The result chosen is the
   signed angle in radians between the ray ending at the origin and passing through the
   point (1,0), and the ray ending at the origin and passing through the point (denom, num).
   The result is between -pi and pi.

   :param num: Numerator of the arctan2 argument.
   :type num: pdarray or numeric_scalars
   :param denom: Denominator of the arctan2 argument.
   :type denom: pdarray or numeric_scalars
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the inverse tangent will be applied to the corresponding values. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing inverse tangent for each corresponding element pair
             of the original pdarray, using the signed values or the numerator and
             denominator to get proper placement on unit circle.
   :rtype: pdarray

   :raises TypeError: | Raised if any parameter fails the typechecking
       | Raised if any element of pdarrays num and denom is not a supported type
       | Raised if both num and denom are scalars
       | Raised if where is neither boolean nor a pdarray of boolean


.. py:function:: arctanh(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise inverse hyperbolic tangent of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the inverse hyperbolic tangent will be applied to the corresponding value. Elsewhere,
                 it will retain its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing inverse hyperbolic tangent for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameters are not a pdarray or numeric scalar.


.. py:function:: argmaxk(pda: pdarray, k: arkouda.numpy.dtypes.int_scalars) -> pdarray

   Find the indices corresponding to the `k` maximum values of an array.

   Returns the largest `k` values of an array, sorted

   :param pda: Input array.
   :type pda: pdarray
   :param k: The desired count of indices corresponding to maxmum array values
   :type k: int_scalars

   :returns: The indices of the maximum `k` values from the pda, sorted
   :rtype: pdarray, int

   :raises TypeError: Raised if pda is not a pdarray or k is not an integer
   :raises ValueError: Raised if the pda is empty or k < 1

   .. rubric:: Notes

   This call is equivalent in value to:

       ak.argsort(a)[k:]

   and generally outperforms this operation.

   This reduction will see a significant drop in performance as `k` grows
   beyond a certain value. This value is system dependent, but generally
   about a `k` of 5 million is where performance degradation has been observed.

   .. rubric:: Examples

   >>> A = ak.array([10,5,1,3,7,2,9,0])
   >>> ak.argmaxk(A, 3)
   array([4, 6, 0])
   >>> ak.argmaxk(A, 4)
   array([1, 4, 6, 0])


.. py:function:: argmink(pda: pdarray, k: arkouda.numpy.dtypes.int_scalars) -> pdarray

   Finds the indices corresponding to the `k` minimum values of an array.

   :param pda: Input array.
   :type pda: pdarray
   :param k: The desired count of indices corresponding to minimum array values
   :type k: int_scalars

   :returns: The indices of the minimum `k` values from the pda, sorted
   :rtype: pdarray, int

   :raises TypeError: Raised if pda is not a pdarray or k is not an integer
   :raises ValueError: Raised if the pda is empty or k < 1

   .. rubric:: Notes

   This call is equivalent in value to:

       ak.argsort(a)[:k]

   and generally outperforms this operation.

   This reduction will see a significant drop in performance as `k` grows
   beyond a certain value. This value is system dependent, but generally
   about a `k` of 5 million is where performance degradation has been observed.

   .. rubric:: Examples

   >>> A = ak.array([10,5,1,3,7,2,9,0])
   >>> ak.argmink(A, 3)
   array([7, 2, 5])
   >>> ak.argmink(A, 4)
   array([7, 2, 5, 3])


.. py:function:: argsort(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical], algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD, axis: arkouda.numpy.dtypes.int_scalars = 0) -> arkouda.pdarrayclass.pdarray

   Return the permutation that sorts the array.

   :param pda: The array to sort (int64, uint64, or float64)
   :type pda: pdarray, Strings, or Categorical
   :param algorithm: The algorithm to be used for sorting the array.
   :type algorithm: SortingAlgorithm, default=SortingAlgorithm.RadixSortLSD
   :param axis: The axis to sort over.
   :type axis: int_scalars, default=0

   :returns: The indices such that ``pda[indices]`` is sorted
   :rtype: pdarray of int64

   :raises TypeError: Raised if the parameter is other than a pdarray, Strings or Categorical

   .. seealso:: :obj:`coargsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and
   resilient to non-uniformity in data but communication intensive.

   .. rubric:: Examples

   >>> a = ak.randint(0, 10, 10)
   >>> perm = ak.argsort(a)
   >>> a[perm]
   array([0 1 3 3 5 5 5 6 6 6])

   >>> ak.argsort(a, ak.sorting.SortingAlgorithm["RadixSortLSD"])
   array([0 2 9 6 8 1 3 5 7 4])

   >>> ak.argsort(a, ak.sorting.SortingAlgorithm["TwoArrayRadixSort"])
   array([0 2 9 6 8 1 3 5 7 4])


.. py:function:: argsort(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical], algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD, axis: arkouda.numpy.dtypes.int_scalars = 0) -> arkouda.pdarrayclass.pdarray

   Return the permutation that sorts the array.

   :param pda: The array to sort (int64, uint64, or float64)
   :type pda: pdarray, Strings, or Categorical
   :param algorithm: The algorithm to be used for sorting the array.
   :type algorithm: SortingAlgorithm, default=SortingAlgorithm.RadixSortLSD
   :param axis: The axis to sort over.
   :type axis: int_scalars, default=0

   :returns: The indices such that ``pda[indices]`` is sorted
   :rtype: pdarray of int64

   :raises TypeError: Raised if the parameter is other than a pdarray, Strings or Categorical

   .. seealso:: :obj:`coargsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and
   resilient to non-uniformity in data but communication intensive.

   .. rubric:: Examples

   >>> a = ak.randint(0, 10, 10)
   >>> perm = ak.argsort(a)
   >>> a[perm]
   array([0 1 3 3 5 5 5 6 6 6])

   >>> ak.argsort(a, ak.sorting.SortingAlgorithm["RadixSortLSD"])
   array([0 2 9 6 8 1 3 5 7 4])

   >>> ak.argsort(a, ak.sorting.SortingAlgorithm["TwoArrayRadixSort"])
   array([0 2 9 6 8 1 3 5 7 4])


.. py:function:: argsort(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical], algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD, axis: arkouda.numpy.dtypes.int_scalars = 0) -> arkouda.pdarrayclass.pdarray

   Return the permutation that sorts the array.

   :param pda: The array to sort (int64, uint64, or float64)
   :type pda: pdarray, Strings, or Categorical
   :param algorithm: The algorithm to be used for sorting the array.
   :type algorithm: SortingAlgorithm, default=SortingAlgorithm.RadixSortLSD
   :param axis: The axis to sort over.
   :type axis: int_scalars, default=0

   :returns: The indices such that ``pda[indices]`` is sorted
   :rtype: pdarray of int64

   :raises TypeError: Raised if the parameter is other than a pdarray, Strings or Categorical

   .. seealso:: :obj:`coargsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and
   resilient to non-uniformity in data but communication intensive.

   .. rubric:: Examples

   >>> a = ak.randint(0, 10, 10)
   >>> perm = ak.argsort(a)
   >>> a[perm]
   array([0 1 3 3 5 5 5 6 6 6])

   >>> ak.argsort(a, ak.sorting.SortingAlgorithm["RadixSortLSD"])
   array([0 2 9 6 8 1 3 5 7 4])

   >>> ak.argsort(a, ak.sorting.SortingAlgorithm["TwoArrayRadixSort"])
   array([0 2 9 6 8 1 3 5 7 4])


.. py:function:: array(a: Union[arkouda.pdarrayclass.pdarray, numpy.ndarray, Iterable], dtype: Union[numpy.dtype, type, str, None] = None, max_bits: int = -1) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Convert a Python or Numpy Iterable to a pdarray or Strings object, sending
   the corresponding data to the arkouda server.

   :param a: Rank-1 array of a supported dtype
   :type a: Union[pdarray, np.ndarray]
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: A pdarray instance stored on arkouda server or Strings instance, which
             is composed of two pdarrays stored on arkouda server
   :rtype: pdarray or Strings

   :raises TypeError: Raised if a is not a pdarray, np.ndarray, or Python Iterable such as a
       list, array, tuple, or deque
   :raises RuntimeError: Raised if nbytes > maxTransferBytes, a.dtype is not supported (not in DTypes),
       or if the product of a size and a.itemsize > maxTransferBytes
   :raises ValueError: Raised if a has rank > get_max_array_rank(), or if the returned message is malformed or does
       not contain the fields required to generate the array.

   .. seealso:: :obj:`pdarray.to_ndarray`

   .. rubric:: Notes

   The number of bytes in the input array cannot exceed `ak.client.maxTransferBytes`,
   otherwise a RuntimeError will be raised. This is to protect the user
   from overwhelming the connection between the Python client and the arkouda
   server, under the assumption that it is a low-bandwidth connection. The user
   may override this limit by setting ak.client.maxTransferBytes to a larger value,
   but should proceed with caution.

   If the pdrray or ndarray is of type U, this method is called twice recursively
   to create the Strings object and the two corresponding pdarrays for string
   bytes and offsets, respectively.

   .. rubric:: Examples

   >>> ak.array(np.arange(1,10))
   array([1 2 3 4 5 6 7 8 9])

   >>> ak.array(range(1,10))
   array([1 2 3 4 5 6 7 8 9])

   >>> strings = ak.array([f'string {i}' for i in range(0,5)])
   >>> type(strings)
   <class 'arkouda.strings.Strings'>


.. py:function:: array(a: Union[arkouda.pdarrayclass.pdarray, numpy.ndarray, Iterable], dtype: Union[numpy.dtype, type, str, None] = None, max_bits: int = -1) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Convert a Python or Numpy Iterable to a pdarray or Strings object, sending
   the corresponding data to the arkouda server.

   :param a: Rank-1 array of a supported dtype
   :type a: Union[pdarray, np.ndarray]
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: A pdarray instance stored on arkouda server or Strings instance, which
             is composed of two pdarrays stored on arkouda server
   :rtype: pdarray or Strings

   :raises TypeError: Raised if a is not a pdarray, np.ndarray, or Python Iterable such as a
       list, array, tuple, or deque
   :raises RuntimeError: Raised if nbytes > maxTransferBytes, a.dtype is not supported (not in DTypes),
       or if the product of a size and a.itemsize > maxTransferBytes
   :raises ValueError: Raised if a has rank > get_max_array_rank(), or if the returned message is malformed or does
       not contain the fields required to generate the array.

   .. seealso:: :obj:`pdarray.to_ndarray`

   .. rubric:: Notes

   The number of bytes in the input array cannot exceed `ak.client.maxTransferBytes`,
   otherwise a RuntimeError will be raised. This is to protect the user
   from overwhelming the connection between the Python client and the arkouda
   server, under the assumption that it is a low-bandwidth connection. The user
   may override this limit by setting ak.client.maxTransferBytes to a larger value,
   but should proceed with caution.

   If the pdrray or ndarray is of type U, this method is called twice recursively
   to create the Strings object and the two corresponding pdarrays for string
   bytes and offsets, respectively.

   .. rubric:: Examples

   >>> ak.array(np.arange(1,10))
   array([1 2 3 4 5 6 7 8 9])

   >>> ak.array(range(1,10))
   array([1 2 3 4 5 6 7 8 9])

   >>> strings = ak.array([f'string {i}' for i in range(0,5)])
   >>> type(strings)
   <class 'arkouda.strings.Strings'>


.. py:function:: array(a: Union[arkouda.pdarrayclass.pdarray, numpy.ndarray, Iterable], dtype: Union[numpy.dtype, type, str, None] = None, max_bits: int = -1) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Convert a Python or Numpy Iterable to a pdarray or Strings object, sending
   the corresponding data to the arkouda server.

   :param a: Rank-1 array of a supported dtype
   :type a: Union[pdarray, np.ndarray]
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: A pdarray instance stored on arkouda server or Strings instance, which
             is composed of two pdarrays stored on arkouda server
   :rtype: pdarray or Strings

   :raises TypeError: Raised if a is not a pdarray, np.ndarray, or Python Iterable such as a
       list, array, tuple, or deque
   :raises RuntimeError: Raised if nbytes > maxTransferBytes, a.dtype is not supported (not in DTypes),
       or if the product of a size and a.itemsize > maxTransferBytes
   :raises ValueError: Raised if a has rank > get_max_array_rank(), or if the returned message is malformed or does
       not contain the fields required to generate the array.

   .. seealso:: :obj:`pdarray.to_ndarray`

   .. rubric:: Notes

   The number of bytes in the input array cannot exceed `ak.client.maxTransferBytes`,
   otherwise a RuntimeError will be raised. This is to protect the user
   from overwhelming the connection between the Python client and the arkouda
   server, under the assumption that it is a low-bandwidth connection. The user
   may override this limit by setting ak.client.maxTransferBytes to a larger value,
   but should proceed with caution.

   If the pdrray or ndarray is of type U, this method is called twice recursively
   to create the Strings object and the two corresponding pdarrays for string
   bytes and offsets, respectively.

   .. rubric:: Examples

   >>> ak.array(np.arange(1,10))
   array([1 2 3 4 5 6 7 8 9])

   >>> ak.array(range(1,10))
   array([1 2 3 4 5 6 7 8 9])

   >>> strings = ak.array([f'string {i}' for i in range(0,5)])
   >>> type(strings)
   <class 'arkouda.strings.Strings'>


.. py:function:: array(a: Union[arkouda.pdarrayclass.pdarray, numpy.ndarray, Iterable], dtype: Union[numpy.dtype, type, str, None] = None, max_bits: int = -1) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Convert a Python or Numpy Iterable to a pdarray or Strings object, sending
   the corresponding data to the arkouda server.

   :param a: Rank-1 array of a supported dtype
   :type a: Union[pdarray, np.ndarray]
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: A pdarray instance stored on arkouda server or Strings instance, which
             is composed of two pdarrays stored on arkouda server
   :rtype: pdarray or Strings

   :raises TypeError: Raised if a is not a pdarray, np.ndarray, or Python Iterable such as a
       list, array, tuple, or deque
   :raises RuntimeError: Raised if nbytes > maxTransferBytes, a.dtype is not supported (not in DTypes),
       or if the product of a size and a.itemsize > maxTransferBytes
   :raises ValueError: Raised if a has rank > get_max_array_rank(), or if the returned message is malformed or does
       not contain the fields required to generate the array.

   .. seealso:: :obj:`pdarray.to_ndarray`

   .. rubric:: Notes

   The number of bytes in the input array cannot exceed `ak.client.maxTransferBytes`,
   otherwise a RuntimeError will be raised. This is to protect the user
   from overwhelming the connection between the Python client and the arkouda
   server, under the assumption that it is a low-bandwidth connection. The user
   may override this limit by setting ak.client.maxTransferBytes to a larger value,
   but should proceed with caution.

   If the pdrray or ndarray is of type U, this method is called twice recursively
   to create the Strings object and the two corresponding pdarrays for string
   bytes and offsets, respectively.

   .. rubric:: Examples

   >>> ak.array(np.arange(1,10))
   array([1 2 3 4 5 6 7 8 9])

   >>> ak.array(range(1,10))
   array([1 2 3 4 5 6 7 8 9])

   >>> strings = ak.array([f'string {i}' for i in range(0,5)])
   >>> type(strings)
   <class 'arkouda.strings.Strings'>


.. py:function:: array_equal(pda_a: arkouda.pdarrayclass.pdarray, pda_b: arkouda.pdarrayclass.pdarray, equal_nan: bool = False) -> bool

   Compares two pdarrays for equality.
   If neither array has any nan elements, then if all elements are pairwise equal,
   it returns True.
   If equal_Nan is False, then any nan element in either array gives a False return.
   If equal_Nan is True, then pairwise-corresponding nans are considered equal.

   :param pda_a:
   :type pda_a: pdarray
   :param pda_b:
   :type pda_b: pdarray
   :param equal_nan: Determines how to handle nans
   :type equal_nan: bool, default=False

   :returns:

             With string data:
                False if one array is type ak.str_ & the other isn't, True if both are ak.str_ & they match.

             With numeric data:
                True if neither array has any nan elements, and all elements pairwise equal.

                True if equal_Nan True, all non-nans pairwise equal & nans in pda_a correspond to nans in pda_b

                False if equal_Nan False, & either array has any nan element.
   :rtype: boolean

   .. rubric:: Examples

   >>> a = ak.randint(0,10,10,dtype=ak.float64)
   >>> b = a
   >>> ak.array_equal(a,b)
   True
   >>> b[9] = np.nan
   >>> ak.array_equal(a,b)
   False
   >>> a[9] = np.nan
   >>> ak.array_equal(a,b)
   False
   >>> ak.array_equal(a,b,True)
   True


.. py:function:: assert_almost_equal(left, right, rtol: float = 1e-05, atol: float = 1e-08, **kwargs) -> None

   Check that the left and right objects are approximately equal.

   By approximately equal, we refer to objects that are numbers or that
   contain numbers which may be equivalent to specific levels of precision.

   :param left:
   :type left: object
   :param right:
   :type right: object
   :param rtol: Relative tolerance.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance.
   :type atol: float, default 1e-8

   .. warning::

      This function cannot be used on pdarray of size > ak.client.maxTransferBytes
      because it converts pdarrays to numpy arrays and calls np.allclose.


.. py:function:: assert_almost_equivalent(left, right, rtol: float = 1e-05, atol: float = 1e-08) -> None

   Check that the left and right objects are approximately equal.

   By approximately equal, we refer to objects that are numbers or that
   contain numbers which may be equivalent to specific levels of precision.

   If the objects are pandas or numpy objects, they are converted to arkouda objects.
   Then assert_almost_equal is applied to the result.

   :param left:
   :type left: object
   :param right:
   :type right: object
   :param rtol: Relative tolerance.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance.
   :type atol: float, default 1e-8

   .. warning::

      This function cannot be used on pdarray of size > ak.client.maxTransferBytes
      because it converts pdarrays to numpy arrays and calls np.allclose.

   .. seealso:: :obj:`assert_almost_equal`


.. py:function:: assert_arkouda_array_equal(left: arkouda.pdarray | arkouda.Strings | arkouda.Categorical | arkouda.SegArray, right: arkouda.pdarray | arkouda.Strings | arkouda.Categorical | arkouda.SegArray, check_dtype: bool = True, err_msg=None, check_same=None, obj: str = 'pdarray', index_values=None) -> None

   Check that 'ak.pdarray' or 'ak.Strings', 'ak.Categorical', or 'ak.SegArray' is equivalent.

   :param left: The two arrays to be compared.
   :type left: arkouda.pdarray or arkouda.Strings or arkouda.Categorical or arkouda.SegArray
   :param right: The two arrays to be compared.
   :type right: arkouda.pdarray or arkouda.Strings or arkouda.Categorical or arkouda.SegArray
   :param check_dtype: Check dtype if both a and b are ak.pdarray.
   :type check_dtype: bool, default True
   :param err_msg: If provided, used as assertion message.
   :type err_msg: str, default None
   :param check_same: Ensure left and right refer/do not refer to the same memory area.
   :type check_same: None|'copy'|'same', default None
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'numpy array'
   :param index_values: optional index (shared by both left and right), used in output.
   :type index_values: Index | arkouda.pdarray, default None


.. py:function:: assert_arkouda_array_equivalent(left: arkouda.pdarray | arkouda.Strings | arkouda.Categorical | arkouda.SegArray | numpy.ndarray | pandas.Categorical, right: arkouda.pdarray | arkouda.Strings | arkouda.Categorical | arkouda.SegArray | numpy.ndarray | pandas.Categorical, check_dtype: bool = True, err_msg=None, check_same=None, obj: str = 'pdarray', index_values=None) -> None

   Check that 'np.array', 'pd.Categorical', 'ak.pdarray', 'ak.Strings',
   'ak.Categorical', or 'ak.SegArray' is equivalent.

   np.nparray's and pd.Categorical's will be converted to the arkouda equivalent.
   Then assert_arkouda_pdarray_equal will be applied to the result.

   :param left: The two arrays to be compared.
   :type left: np.ndarray, pd.Categorical, arkouda.pdarray or arkouda.Strings or arkouda.Categorical
   :param right: The two arrays to be compared.
   :type right: np.ndarray, pd.Categorical, arkouda.pdarray or arkouda.Strings or arkouda.Categorical
   :param check_dtype: Check dtype if both a and b are ak.pdarray or np.ndarray.
   :type check_dtype: bool, default True
   :param err_msg: If provided, used as assertion message.
   :type err_msg: str, default None
   :param check_same: Ensure left and right refer/do not refer to the same memory area.
   :type check_same: None|'copy'|'same', default None
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'numpy array'
   :param index_values: optional index (shared by both left and right), used in output.
   :type index_values: Index | arkouda.pdarray, default None

   .. seealso:: :obj:`assert_arkouda_array_equal`


.. py:function:: assert_arkouda_pdarray_equal(left: arkouda.pdarray, right: arkouda.pdarray, check_dtype: bool = True, err_msg=None, check_same=None, obj: str = 'pdarray', index_values=None) -> None

   Check that the two 'ak.pdarray's are equivalent.

   :param left: The two arrays to be compared.
   :type left: arkouda.pdarray
   :param right: The two arrays to be compared.
   :type right: arkouda.pdarray
   :param check_dtype: Check dtype if both a and b are ak.pdarray.
   :type check_dtype: bool, default True
   :param err_msg: If provided, used as assertion message.
   :type err_msg: str, default None
   :param check_same: Ensure left and right refer/do not refer to the same memory area.
   :type check_same: None|'copy'|'same', default None
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'pdarray'
   :param index_values: optional index (shared by both left and right), used in output.
   :type index_values: Index | arkouda.pdarray, default None


.. py:function:: assert_arkouda_segarray_equal(left: arkouda.SegArray, right: arkouda.SegArray, check_dtype: bool = True, err_msg=None, check_same=None, obj: str = 'segarray') -> None

   Check that the two 'ak.segarray's are equivalent.

   :param left: The two segarrays to be compared.
   :type left: arkouda.segarray
   :param right: The two segarrays to be compared.
   :type right: arkouda.segarray
   :param check_dtype: Check dtype if both a and b are ak.pdarray.
   :type check_dtype: bool, default True
   :param err_msg: If provided, used as assertion message.
   :type err_msg: str, default None
   :param check_same: Ensure left and right refer/do not refer to the same memory area.
   :type check_same: None|'copy'|'same', default None
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'pdarray'


.. py:function:: assert_arkouda_strings_equal(left, right, err_msg=None, check_same=None, obj: str = 'Strings', index_values=None) -> None

   Check that 'ak.Strings' is equivalent.

   :param left: The two Strings to be compared.
   :type left: arkouda.Strings
   :param right: The two Strings to be compared.
   :type right: arkouda.Strings
   :param err_msg: If provided, used as assertion message.
   :type err_msg: str, default None
   :param check_same: Ensure left and right refer/do not refer to the same memory area.
   :type check_same: None|'copy'|'same', default None
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'Strings'
   :param index_values: optional index (shared by both left and right), used in output.
   :type index_values: Index | arkouda.pdarray, default None


.. py:function:: assert_attr_equal(attr: str, left, right, obj: str = 'Attributes') -> None

   Check attributes are equal. Both objects must have attribute.

   :param attr: Attribute name being compared.
   :type attr: str
   :param left:
   :type left: object
   :param right:
   :type right: object
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message
   :type obj: str, default 'Attributes'


.. py:function:: assert_categorical_equal(left, right, check_dtype: bool = True, check_category_order: bool = True, obj: str = 'Categorical') -> None

   Test that Categoricals are equivalent.

   :param left:
   :type left: Categorical
   :param right:
   :type right: Categorical
   :param check_dtype: Check that integer dtype of the codes are the same.
   :type check_dtype: bool, default True
   :param check_category_order: Whether the order of the categories should be compared, which
                                implies identical integer codes.  If False, only the resulting
                                values are compared.  The ordered attribute is
                                checked regardless.
   :type check_category_order: bool, default True
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'Categorical'


.. py:function:: assert_class_equal(left, right, exact: bool = True, obj: str = 'Input') -> None

   Checks classes are equal.


.. py:function:: assert_contains_all(iterable, dic) -> None

   Assert that a dictionary contains all the elements of an iterable.
   :param iterable:
   :type iterable: iterable
   :param dic:
   :type dic: dict


.. py:function:: assert_copy(iter1, iter2, **eql_kwargs) -> None

   Checks that the elements are equal, but not the same object.
   (Does not check that items in sequences are also not the same object.)

   :param iter1: Iterables that produce elements comparable with assert_almost_equal.
   :type iter1: iterable
   :param iter2: Iterables that produce elements comparable with assert_almost_equal.
   :type iter2: iterable


.. py:function:: assert_dict_equal(left, right, compare_keys: bool = True) -> None

   Assert that two dictionaries are equal.
   Values must be arkouda objects.
   :param left: The dictionaries to be compared.
   :type left: dict
   :param right: The dictionaries to be compared.
   :type right: dict
   :param compare_keys: Whether to compare the keys.
                        If False, only the values are compared.
   :type compare_keys: bool, default = True


.. py:function:: assert_equal(left, right, **kwargs) -> None

   Wrapper for tm.assert_*_equal to dispatch to the appropriate test function.

   :param left: The two items to be compared.
   :type left: Index, Series, DataFrame, or pdarray
   :param right: The two items to be compared.
   :type right: Index, Series, DataFrame, or pdarray
   :param \*\*kwargs: All keyword arguments are passed through to the underlying assert method.


.. py:function:: assert_equivalent(left, right, **kwargs) -> None

   Wrapper for tm.assert_*_equivalent to dispatch to the appropriate test function.

   :param left:
   :type left: Index, pd.Index, Series, pd.Series, DataFrame, pd.DataFrame,
   :param right:
   :type right: Index, pd.Index, Series, pd.Series, DataFrame, pd.DataFrame,
   :param Strings: The two items to be compared.
   :param Categorical: The two items to be compared.
   :param pd.Categorical: The two items to be compared.
   :param SegArray: The two items to be compared.
   :param pdarray: The two items to be compared.
   :param np.ndarray: The two items to be compared.
   :param : The two items to be compared.
   :param \*\*kwargs: All keyword arguments are passed through to the underlying assert method.


.. py:function:: assert_frame_equal(left: arkouda.DataFrame, right: arkouda.DataFrame, check_dtype: bool = True, check_index_type: bool = True, check_column_type: bool = True, check_frame_type: bool = True, check_names: bool = True, check_exact: bool = True, check_categorical: bool = True, check_like: bool = False, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'DataFrame') -> None

   Check that left and right DataFrame are equal.

   This function is intended to compare two DataFrames and output any
   differences. It is mostly intended for use in unit tests.
   Additional parameters allow varying the strictness of the
   equality checks performed.

   :param left: First DataFrame to compare.
   :type left: DataFrame
   :param right: Second DataFrame to compare.
   :type right: DataFrame
   :param check_dtype: Whether to check the DataFrame dtype is identical.
   :type check_dtype: bool, default True
   :param check_index_type: Whether to check the Index class, dtype and inferred_type
                            are identical.
   :type check_index_type: bool, default = True
   :param check_column_type: Whether to check the columns class, dtype and inferred_type
                             are identical. Is passed as the ``exact`` argument of
                             :func:`assert_index_equal`.
   :type check_column_type: bool or {'equiv'}, default 'equiv'
   :param check_frame_type: Whether to check the DataFrame class is identical.
   :type check_frame_type: bool, default True
   :param check_names: Whether to check that the `names` attribute for both the `index`
                       and `column` attributes of the DataFrame is identical.
   :type check_names: bool, default True
   :param check_exact: Whether to compare number exactly.
   :type check_exact: bool, default False
   :param check_categorical: Whether to compare internal Categorical exactly.
   :type check_categorical: bool, default True
   :param check_like: If True, ignore the order of index & columns.
                      Note: index labels must match their respective rows
                      (same as in columns) - same labels must be with the same data.
   :type check_like: bool, default False
   :param rtol: Relative tolerance. Only used when check_exact is False.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance. Only used when check_exact is False.
   :type atol: float, default 1e-8
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'DataFrame'

   .. seealso::

      :obj:`assert_series_equal`
          Equivalent method for asserting Series equality.

   .. rubric:: Examples

   This example shows comparing two DataFrames that are equal
   but with columns of differing dtypes.

   >>> from arkouda.testing import assert_frame_equal
   >>> df1 = ak.DataFrame({'a': [1, 2], 'b': [3, 4]})
   >>> df2 = ak.DataFrame({'a': [1, 2], 'b': [3.0, 4.0]})

   df1 equals itself.

   >>> assert_frame_equal(df1, df1)

   df1 differs from df2 as column 'b' is of a different type.

   >>> assert_frame_equal(df1, df2)
   Traceback (most recent call last):
   ...
   AssertionError: Attributes of DataFrame.iloc[:, 1] (column name="b") are different

   Attribute "dtype" are different
   [left]:  int64
   [right]: float64

   Ignore differing dtypes in columns with check_dtype.

   >>> assert_frame_equal(df1, df2, check_dtype=False)


.. py:function:: assert_frame_equivalent(left: arkouda.DataFrame | pandas.DataFrame, right: arkouda.DataFrame | pandas.DataFrame, check_dtype: bool = True, check_index_type: bool = True, check_column_type: bool = True, check_frame_type: bool = True, check_names: bool = True, check_exact: bool = True, check_categorical: bool = True, check_like: bool = False, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'DataFrame') -> None

   Check that left and right DataFrame are equal.

   This function is intended to compare two DataFrames and output any
   differences. It is mostly intended for use in unit tests.
   Additional parameters allow varying the strictness of the
   equality checks performed.

   pd.DataFrame's will be converted to the arkouda equivalent.
   Then assert_frame_equal will be applied to the result.

   :param left: First DataFrame to compare.
   :type left: DataFrame or pd.DataFrame
   :param right: Second DataFrame to compare.
   :type right: DataFrame or pd.DataFrame
   :param check_dtype: Whether to check the DataFrame dtype is identical.
   :type check_dtype: bool, default True
   :param check_index_type: Whether to check the Index class, dtype and inferred_type
                            are identical.
   :type check_index_type: bool, default = True
   :param check_column_type: Whether to check the columns class, dtype and inferred_type
                             are identical. Is passed as the ``exact`` argument of
                             :func:`assert_index_equal`.
   :type check_column_type: bool or {'equiv'}, default 'equiv'
   :param check_frame_type: Whether to check the DataFrame class is identical.
   :type check_frame_type: bool, default True
   :param check_names: Whether to check that the `names` attribute for both the `index`
                       and `column` attributes of the DataFrame is identical.
   :type check_names: bool, default True
   :param check_exact: Whether to compare number exactly.
   :type check_exact: bool, default False
   :param check_categorical: Whether to compare internal Categorical exactly.
   :type check_categorical: bool, default True
   :param check_like: If True, ignore the order of index & columns.
                      Note: index labels must match their respective rows
                      (same as in columns) - same labels must be with the same data.
   :type check_like: bool, default False
   :param rtol: Relative tolerance. Only used when check_exact is False.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance. Only used when check_exact is False.
   :type atol: float, default 1e-8
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'DataFrame'

   .. seealso:: :obj:`assert_frame_equal`

   .. rubric:: Examples

   This example shows comparing two DataFrames that are equal
   but with columns of differing dtypes.

   >>> from arkouda.testing import assert_frame_equivalent
   >>> import pandas as pd
   >>> df1 = ak.DataFrame({'a': [1, 2], 'b': [3, 4]})
   >>> df2 = pd.DataFrame({'a': [1, 2], 'b': [3.0, 4.0]})
   >>> assert_frame_equivalent(df1, df1)


.. py:function:: assert_index_equal(left: arkouda.Index, right: arkouda.Index, exact: bool = True, check_names: bool = True, check_exact: bool = True, check_categorical: bool = True, check_order: bool = True, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'Index') -> None

   Check that left and right Index are equal.

   :param left:
   :type left: Index
   :param right:
   :type right: Index
   :param exact: Whether to check the Index class, dtype and inferred_type
                 are identical.
   :type exact: True
   :param check_names: Whether to check the names attribute.
   :type check_names: bool, default True
   :param check_exact: Whether to compare number exactly.
   :type check_exact: bool, default True
   :param check_categorical: Whether to compare internal Categorical exactly.
   :type check_categorical: bool, default True
   :param check_order: Whether to compare the order of index entries as well as their values.
                       If True, both indexes must contain the same elements, in the same order.
                       If False, both indexes must contain the same elements, but in any order.
   :type check_order: bool, default True
   :param rtol: Relative tolerance. Only used when check_exact is False.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance. Only used when check_exact is False.
   :type atol: float, default 1e-8
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'Index'

   .. rubric:: Examples

   >>> from arkouda import testing as tm
   >>> a = ak.Index([1, 2, 3])
   >>> b = ak.Index([1, 2, 3])
   >>> tm.assert_index_equal(a, b)


.. py:function:: assert_index_equivalent(left: arkouda.Index | pandas.Index, right: arkouda.Index | pandas.Index, exact: bool = True, check_names: bool = True, check_exact: bool = True, check_categorical: bool = True, check_order: bool = True, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'Index') -> None

   Check that left and right Index are equal.

   If the objects are pandas.Index, they are converted to arkouda.Index.
   Then assert_almost_equal is applied to the result.

   :param left:
   :type left: Index or pandas.Index
   :param right:
   :type right: Index or pandas.Index
   :param exact: Whether to check the Index class, dtype and inferred_type
                 are identical.
   :type exact: True
   :param check_names: Whether to check the names attribute.
   :type check_names: bool, default True
   :param check_exact: Whether to compare number exactly.
   :type check_exact: bool, default True
   :param check_categorical: Whether to compare internal Categorical exactly.
   :type check_categorical: bool, default True
   :param check_order: Whether to compare the order of index entries as well as their values.
                       If True, both indexes must contain the same elements, in the same order.
                       If False, both indexes must contain the same elements, but in any order.
   :type check_order: bool, default True
   :param rtol: Relative tolerance. Only used when check_exact is False.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance. Only used when check_exact is False.
   :type atol: float, default 1e-8
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'Index'

   .. seealso:: :obj:`assert_index_equal`

   .. rubric:: Examples

   >>> from arkouda import testing as tm
   >>> import pandas as pd
   >>> a = ak.Index([1, 2, 3])
   >>> b = pd.Index([1, 2, 3])
   >>> tm.assert_index_equivalent(a, b)


.. py:function:: assert_is_sorted(seq) -> None

   Assert that the sequence is sorted.


.. py:function:: assert_series_equal(left, right, check_dtype: bool = True, check_index_type: bool = True, check_series_type: bool = True, check_names: bool = True, check_exact: bool = False, check_categorical: bool = True, check_category_order: bool = True, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'Series', *, check_index: bool = True, check_like: bool = False) -> None

   Check that left and right Series are equal.

   :param left:
   :type left: Series
   :param right:
   :type right: Series
   :param check_dtype: Whether to check the Series dtype is identical.
   :type check_dtype: bool, default True
   :param check_index_type: Whether to check the Index class, dtype and inferred_type
                            are identical.
   :type check_index_type: bool, default True
   :param check_series_type: Whether to check the Series class is identical.
   :type check_series_type: bool, default True
   :param check_names: Whether to check the Series and Index names attribute.
   :type check_names: bool, default True
   :param check_exact: Whether to compare number exactly.
   :type check_exact: bool, default False
   :param check_categorical: Whether to compare internal Categorical exactly.
   :type check_categorical: bool, default True
   :param check_category_order: Whether to compare category order of internal Categoricals.
   :type check_category_order: bool, default True
   :param rtol: Relative tolerance. Only used when check_exact is False.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance. Only used when check_exact is False.
   :type atol: float, default 1e-8
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'Series'
   :param check_index: Whether to check index equivalence. If False, then compare only values.
   :type check_index: bool, default True
   :param check_like: If True, ignore the order of the index. Must be False if check_index is False.
                      Note: same labels must be with the same data.
   :type check_like: bool, default False

   .. rubric:: Examples

   >>> from arkouda import testing as tm
   >>> a = ak.Series([1, 2, 3, 4])
   >>> b = ak.Series([1, 2, 3, 4])
   >>> tm.assert_series_equal(a, b)


.. py:function:: assert_series_equivalent(left: arkouda.Series | pandas.Series, right: arkouda.Series | pandas.Series, check_dtype: bool = True, check_index_type: bool = True, check_series_type: bool = True, check_names: bool = True, check_exact: bool = False, check_categorical: bool = True, check_category_order: bool = True, rtol: float = 1e-05, atol: float = 1e-08, obj: str = 'Series', *, check_index: bool = True, check_like: bool = False) -> None

   Check that left and right Series are equal.

   pd.Series's will be converted to the arkouda equivalent.
   Then assert_series_equal will be applied to the result.

   :param left:
   :type left: Series or pd.Series
   :param right:
   :type right: Series or pd.Series
   :param check_dtype: Whether to check the Series dtype is identical.
   :type check_dtype: bool, default True
   :param check_index_type: Whether to check the Index class, dtype and inferred_type
                            are identical.
   :type check_index_type: bool, default True
   :param check_series_type: Whether to check the Series class is identical.
   :type check_series_type: bool, default True
   :param check_names: Whether to check the Series and Index names attribute.
   :type check_names: bool, default True
   :param check_exact: Whether to compare number exactly.
   :type check_exact: bool, default False
   :param check_categorical: Whether to compare internal Categorical exactly.
   :type check_categorical: bool, default True
   :param check_category_order: Whether to compare category order of internal Categoricals.
   :type check_category_order: bool, default True
   :param rtol: Relative tolerance. Only used when check_exact is False.
   :type rtol: float, default 1e-5
   :param atol: Absolute tolerance. Only used when check_exact is False.
   :type atol: float, default 1e-8
   :param obj: Specify object name being compared, internally used to show appropriate
               assertion message.
   :type obj: str, default 'Series'
   :param check_index: Whether to check index equivalence. If False, then compare only values.
   :type check_index: bool, default True
   :param check_like: If True, ignore the order of the index. Must be False if check_index is False.
                      Note: same labels must be with the same data.
   :type check_like: bool, default False

   .. seealso:: :obj:`assert_series_equal`

   .. rubric:: Examples

   >>> from arkouda import testing as tm
   >>> import pandas as pd
   >>> a = ak.Series([1, 2, 3, 4])
   >>> b = pd.Series([1, 2, 3, 4])
   >>> tm.assert_series_equivalent(a, b)


.. py:function:: attach(name: str)

.. py:function:: attach_all(names: list)

   Attach to all objects registered with the names provide

   :param names: List of names to attach to
   :type names: list

   :rtype: dict


.. py:function:: attach_pdarray(user_defined_name: str) -> pdarray

   class method to return a pdarray attached to the registered name in the arkouda
   server which was registered using register()

   :param user_defined_name: user defined name which array was registered under
   :type user_defined_name: str

   :returns: pdarray which is bound to the corresponding server side component which was registered
             with user_defined_name
   :rtype: pdarray

   :raises TypeError: Raised if user_defined_name is not a str

   .. seealso:: :obj:`attach`, :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

   .. rubric:: Notes

   Registered names/pdarrays in the server are immune to deletion
   until they are unregistered.

   .. rubric:: Examples

   >>> a = zeros(100)
   >>> a.register("my_zeros")
   >>> # potentially disconnect from server and reconnect to server
   >>> b = ak.attach_pdarray("my_zeros")
   >>> # ...other work...
   >>> b.unregister()


.. py:function:: base_repr(number, base=2, padding=0)

   Return a string representation of a number in the given base system.

   :param number: The value to convert. Positive and negative values are handled.
   :type number: int
   :param base: Convert `number` to the `base` number system. The valid range is 2-36,
                the default value is 2.
   :type base: int, optional
   :param padding: Number of zeros padded on the left. Default is 0 (no padding).
   :type padding: int, optional

   :returns: **out** -- String representation of `number` in `base` system.
   :rtype: str

   .. seealso::

      :obj:`binary_repr`
          Faster version of `base_repr` for base 2.

   .. rubric:: Examples

   >>> np.base_repr(5)
   '101'
   >>> np.base_repr(6, 5)
   '11'
   >>> np.base_repr(7, base=5, padding=3)
   '00012'

   >>> np.base_repr(10, base=16)
   'A'
   >>> np.base_repr(32, base=16)
   '20'


.. py:class:: bigint

   .. py:method:: itemsize(*args, **kwargs)

      int([x]) -> integer
      int(x, base=10) -> integer

      Convert a number or string to an integer, or return 0 if no arguments
      are given.  If x is a number, return x.__int__().  For floating point
      numbers, this truncates towards zero.

      If x is not a number or if base is given, then x must be a string,
      bytes, or bytearray instance representing an integer literal in the
      given base.  The literal can be preceded by '+' or '-' and be surrounded
      by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
      Base 0 means to interpret the base from the string as an integer literal.
      >>> int('0b100', base=0)
      4




   .. py:method:: name(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: ndim(*args, **kwargs)

      int([x]) -> integer
      int(x, base=10) -> integer

      Convert a number or string to an integer, or return 0 if no arguments
      are given.  If x is a number, return x.__int__().  For floating point
      numbers, this truncates towards zero.

      If x is not a number or if base is given, then x must be a string,
      bytes, or bytearray instance representing an integer literal in the
      given base.  The literal can be preceded by '+' or '-' and be surrounded
      by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
      Base 0 means to interpret the base from the string as an integer literal.
      >>> int('0b100', base=0)
      4




   .. py:method:: shape(*args, **kwargs)

      Built-in immutable sequence.

      If no argument is given, the constructor returns an empty tuple.
      If iterable is specified the tuple is initialized from iterable's items.

      If the argument is a tuple, the return value is the same object.




   .. py:method:: type(x)


.. py:class:: bigint

   .. py:method:: itemsize(*args, **kwargs)

      int([x]) -> integer
      int(x, base=10) -> integer

      Convert a number or string to an integer, or return 0 if no arguments
      are given.  If x is a number, return x.__int__().  For floating point
      numbers, this truncates towards zero.

      If x is not a number or if base is given, then x must be a string,
      bytes, or bytearray instance representing an integer literal in the
      given base.  The literal can be preceded by '+' or '-' and be surrounded
      by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
      Base 0 means to interpret the base from the string as an integer literal.
      >>> int('0b100', base=0)
      4




   .. py:method:: name(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: ndim(*args, **kwargs)

      int([x]) -> integer
      int(x, base=10) -> integer

      Convert a number or string to an integer, or return 0 if no arguments
      are given.  If x is a number, return x.__int__().  For floating point
      numbers, this truncates towards zero.

      If x is not a number or if base is given, then x must be a string,
      bytes, or bytearray instance representing an integer literal in the
      given base.  The literal can be preceded by '+' or '-' and be surrounded
      by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.
      Base 0 means to interpret the base from the string as an integer literal.
      >>> int('0b100', base=0)
      4




   .. py:method:: shape(*args, **kwargs)

      Built-in immutable sequence.

      If no argument is given, the constructor returns an empty tuple.
      If iterable is specified the tuple is initialized from iterable's items.

      If the argument is a tuple, the return value is the same object.




   .. py:method:: type(x)


.. py:function:: bigint_from_uint_arrays(arrays, max_bits=-1)

   Create a bigint pdarray from an iterable of uint pdarrays.
   The first item in arrays will be the highest 64 bits and
   the last item will be the lowest 64 bits.

   :param arrays: An iterable of uint pdarrays used to construct the bigint pdarray.
                  The first item in arrays will be the highest 64 bits and
                  the last item will be the lowest 64 bits.
   :type arrays: Sequence[pdarray]
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: bigint pdarray constructed from uint arrays
   :rtype: pdarray

   :raises TypeError: Raised if any pdarray in arrays has a dtype other than uint or
       if the pdarrays are not the same size.
   :raises RuntimeError: Raised if there is a server-side error thrown

   .. seealso:: :obj:`pdarray.bigint_to_uint_arrays`

   .. rubric:: Examples

   >>> a = ak.bigint_from_uint_arrays([ak.ones(5, dtype=ak.uint64), ak.arange(5, dtype=ak.uint64)])
   >>> a
   array([18446744073709551616 18446744073709551617 18446744073709551618
   18446744073709551619 18446744073709551620])

   >>> a.dtype
   dtype(bigint)

   >>> all(a[i] == 2**64 + i for i in range(5))
   True


.. py:function:: binary_repr(num, width=None)

   Return the binary representation of the input number as a string.

   For negative numbers, if width is not given, a minus sign is added to the
   front. If width is given, the two's complement of the number is
   returned, with respect to that width.

   In a two's-complement system negative numbers are represented by the two's
   complement of the absolute value. This is the most common method of
   representing signed integers on computers [1]_. A N-bit two's-complement
   system can represent every integer in the range
   :math:`-2^{N-1}` to :math:`+2^{N-1}-1`.

   :param num: Only an integer decimal number can be used.
   :type num: int
   :param width: The length of the returned string if `num` is positive, or the length
                 of the two's complement if `num` is negative, provided that `width` is
                 at least a sufficient number of bits for `num` to be represented in the
                 designated form.

                 If the `width` value is insufficient, it will be ignored, and `num` will
                 be returned in binary (`num` > 0) or two's complement (`num` < 0) form
                 with its width equal to the minimum number of bits needed to represent
                 the number in the designated form. This behavior is deprecated and will
                 later raise an error.

                 .. deprecated:: 1.12.0
   :type width: int, optional

   :returns: **bin** -- Binary representation of `num` or two's complement of `num`.
   :rtype: str

   .. seealso::

      :obj:`base_repr`
          Return a string representation of a number in the given base system.

      :obj:`bin`
          Python's built-in binary representation generator of an integer.

   .. rubric:: Notes

   `binary_repr` is equivalent to using `base_repr` with base 2, but about 25x
   faster.

   .. rubric:: References

   .. [1] Wikipedia, "Two's complement",
       https://en.wikipedia.org/wiki/Two's_complement

   .. rubric:: Examples

   >>> np.binary_repr(3)
   '11'
   >>> np.binary_repr(-3)
   '-11'
   >>> np.binary_repr(3, width=4)
   '0011'

   The two's complement is returned when the input number is negative and
   width is specified:

   >>> np.binary_repr(-3, width=3)
   '101'
   >>> np.binary_repr(-3, width=5)
   '11101'


.. py:class:: bitType(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned long``.

       :Character code: ``'L'``
       :Canonical name: `numpy.uint`
       :Alias on this platform (Linux x86_64): `numpy.uint64`: 64-bit unsigned integer (``0`` to ``18_446_744_073_709_551_615``).
       :Alias on this platform (Linux x86_64): `numpy.uintp`: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      uint64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint64(127).bit_count()
              7




.. py:class:: bitType(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned long``.

       :Character code: ``'L'``
       :Canonical name: `numpy.uint`
       :Alias on this platform (Linux x86_64): `numpy.uint64`: 64-bit unsigned integer (``0`` to ``18_446_744_073_709_551_615``).
       :Alias on this platform (Linux x86_64): `numpy.uintp`: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      uint64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint64(127).bit_count()
              7




.. py:class:: bool_(value)

   Bases: :py:obj:`numpy.generic`


   Boolean type (True or False), stored as a byte.

       .. warning::

          The :class:`bool_` type is not a subclass of the :class:`int_` type
          (the :class:`bool_` is not even a number type). This is different
          than Python's default implementation of :class:`bool` as a
          sub-class of :class:`int`.

       :Character code: ``'?'``



.. py:class:: bool_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:class:: bool_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:function:: broadcast(segments: pdarray, values: Union[pdarray, Strings], size: Union[int, np.int64, np.uint64] = -1, permutation: Union[pdarray, None] = None)

   Broadcast a dense column vector to the rows of a sparse matrix or grouped array.

   :param segments: Offsets of the start of each row in the sparse matrix or grouped array.
                    Must be sorted in ascending order.
   :type segments: pdarray, int64
   :param values: The values to broadcast, one per row (or group)
   :type values: pdarray, Strings
   :param size: The total number of nonzeros in the matrix. If permutation is given, this
                argument is ignored and the size is inferred from the permutation array.
   :type size: int
   :param permutation: The permutation to go from the original ordering of nonzeros to the ordering
                       grouped by row. To broadcast values back to the original ordering, this
                       permutation will be inverted. If no permutation is supplied, it is assumed
                       that the original nonzeros were already grouped by row. In this case, the
                       size argument must be given.
   :type permutation: pdarray, int64

   :returns: The broadcast values, one per nonzero
   :rtype: pdarray, Strings

   :raises ValueError: - If segments and values are different sizes
       - If segments are empty
       - If number of nonzeros (either user-specified or inferred from permutation)
         is less than one

   .. rubric:: Examples

   >>>
   # Define a sparse matrix with 3 rows and 7 nonzeros
   >>> row_starts = ak.array([0, 2, 5])
   >>> nnz = 7
   # Broadcast the row number to each nonzero element
   >>> row_number = ak.arange(3)
   >>> ak.broadcast(row_starts, row_number, nnz)
   array([0 0 1 1 1 2 2])
   # If the original nonzeros were in reverse order...
   >>> permutation = ak.arange(6, -1, -1)
   >>> ak.broadcast(row_starts, row_number, permutation=permutation)
   array([2 2 1 1 1 0 0])


.. py:function:: broadcast(segments: pdarray, values: Union[pdarray, Strings], size: Union[int, np.int64, np.uint64] = -1, permutation: Union[pdarray, None] = None)

   Broadcast a dense column vector to the rows of a sparse matrix or grouped array.

   :param segments: Offsets of the start of each row in the sparse matrix or grouped array.
                    Must be sorted in ascending order.
   :type segments: pdarray, int64
   :param values: The values to broadcast, one per row (or group)
   :type values: pdarray, Strings
   :param size: The total number of nonzeros in the matrix. If permutation is given, this
                argument is ignored and the size is inferred from the permutation array.
   :type size: int
   :param permutation: The permutation to go from the original ordering of nonzeros to the ordering
                       grouped by row. To broadcast values back to the original ordering, this
                       permutation will be inverted. If no permutation is supplied, it is assumed
                       that the original nonzeros were already grouped by row. In this case, the
                       size argument must be given.
   :type permutation: pdarray, int64

   :returns: The broadcast values, one per nonzero
   :rtype: pdarray, Strings

   :raises ValueError: - If segments and values are different sizes
       - If segments are empty
       - If number of nonzeros (either user-specified or inferred from permutation)
         is less than one

   .. rubric:: Examples

   >>>
   # Define a sparse matrix with 3 rows and 7 nonzeros
   >>> row_starts = ak.array([0, 2, 5])
   >>> nnz = 7
   # Broadcast the row number to each nonzero element
   >>> row_number = ak.arange(3)
   >>> ak.broadcast(row_starts, row_number, nnz)
   array([0 0 1 1 1 2 2])
   # If the original nonzeros were in reverse order...
   >>> permutation = ak.arange(6, -1, -1)
   >>> ak.broadcast(row_starts, row_number, permutation=permutation)
   array([2 2 1 1 1 0 0])


.. py:function:: broadcast(segments: pdarray, values: Union[pdarray, Strings], size: Union[int, np.int64, np.uint64] = -1, permutation: Union[pdarray, None] = None)

   Broadcast a dense column vector to the rows of a sparse matrix or grouped array.

   :param segments: Offsets of the start of each row in the sparse matrix or grouped array.
                    Must be sorted in ascending order.
   :type segments: pdarray, int64
   :param values: The values to broadcast, one per row (or group)
   :type values: pdarray, Strings
   :param size: The total number of nonzeros in the matrix. If permutation is given, this
                argument is ignored and the size is inferred from the permutation array.
   :type size: int
   :param permutation: The permutation to go from the original ordering of nonzeros to the ordering
                       grouped by row. To broadcast values back to the original ordering, this
                       permutation will be inverted. If no permutation is supplied, it is assumed
                       that the original nonzeros were already grouped by row. In this case, the
                       size argument must be given.
   :type permutation: pdarray, int64

   :returns: The broadcast values, one per nonzero
   :rtype: pdarray, Strings

   :raises ValueError: - If segments and values are different sizes
       - If segments are empty
       - If number of nonzeros (either user-specified or inferred from permutation)
         is less than one

   .. rubric:: Examples

   >>>
   # Define a sparse matrix with 3 rows and 7 nonzeros
   >>> row_starts = ak.array([0, 2, 5])
   >>> nnz = 7
   # Broadcast the row number to each nonzero element
   >>> row_number = ak.arange(3)
   >>> ak.broadcast(row_starts, row_number, nnz)
   array([0 0 1 1 1 2 2])
   # If the original nonzeros were in reverse order...
   >>> permutation = ak.arange(6, -1, -1)
   >>> ak.broadcast(row_starts, row_number, permutation=permutation)
   array([2 2 1 1 1 0 0])


.. py:function:: broadcast(segments: pdarray, values: Union[pdarray, Strings], size: Union[int, np.int64, np.uint64] = -1, permutation: Union[pdarray, None] = None)

   Broadcast a dense column vector to the rows of a sparse matrix or grouped array.

   :param segments: Offsets of the start of each row in the sparse matrix or grouped array.
                    Must be sorted in ascending order.
   :type segments: pdarray, int64
   :param values: The values to broadcast, one per row (or group)
   :type values: pdarray, Strings
   :param size: The total number of nonzeros in the matrix. If permutation is given, this
                argument is ignored and the size is inferred from the permutation array.
   :type size: int
   :param permutation: The permutation to go from the original ordering of nonzeros to the ordering
                       grouped by row. To broadcast values back to the original ordering, this
                       permutation will be inverted. If no permutation is supplied, it is assumed
                       that the original nonzeros were already grouped by row. In this case, the
                       size argument must be given.
   :type permutation: pdarray, int64

   :returns: The broadcast values, one per nonzero
   :rtype: pdarray, Strings

   :raises ValueError: - If segments and values are different sizes
       - If segments are empty
       - If number of nonzeros (either user-specified or inferred from permutation)
         is less than one

   .. rubric:: Examples

   >>>
   # Define a sparse matrix with 3 rows and 7 nonzeros
   >>> row_starts = ak.array([0, 2, 5])
   >>> nnz = 7
   # Broadcast the row number to each nonzero element
   >>> row_number = ak.arange(3)
   >>> ak.broadcast(row_starts, row_number, nnz)
   array([0 0 1 1 1 2 2])
   # If the original nonzeros were in reverse order...
   >>> permutation = ak.arange(6, -1, -1)
   >>> ak.broadcast(row_starts, row_number, permutation=permutation)
   array([2 2 1 1 1 0 0])


.. py:function:: broadcast_dims(sa: Sequence[int], sb: Sequence[int]) -> Tuple[int, Ellipsis]

   Algorithm to determine shape of broadcasted PD array given two array shapes

   see: https://data-apis.org/array-api/latest/API_specification/broadcasting.html#algorithm


.. py:function:: broadcast_to_shape(pda: pdarray, shape: Tuple[int, Ellipsis]) -> pdarray

   expand an array's rank to the specified shape using broadcasting


.. py:class:: byte(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with C ``char``.

       :Character code: ``'b'``
       :Canonical name: `numpy.byte`
       :Alias on this platform (Linux x86_64): `numpy.int8`: 8-bit signed integer (``-128`` to ``127``).



   .. py:method:: bit_count(*args, **kwargs)

      int8.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int8(127).bit_count()
              7
              >>> np.int8(-127).bit_count()
              7




.. py:class:: bytes_

   A byte string.

       When used in arrays, this type strips trailing null bytes.

       :Character code: ``'S'``
       :Alias: `numpy.string_`



   .. py:method:: T(*args, **kwargs)

      Scalar attribute identical to the corresponding array attribute.

          Please see `ndarray.T`.




   .. py:method:: all(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.all`.




   .. py:method:: any(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.any`.




   .. py:method:: argmax(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argmax`.




   .. py:method:: argmin(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argmin`.




   .. py:method:: argsort(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argsort`.




   .. py:method:: astype(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.astype`.




   .. py:method:: base(*args, **kwargs)

      Scalar attribute identical to the corresponding array attribute.

          Please see `ndarray.base`.




   .. py:method:: byteswap(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.byteswap`.




   .. py:method:: choose(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.choose`.




   .. py:method:: clip(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.clip`.




   .. py:method:: compress(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.compress`.




   .. py:method:: conj(*args, **kwargs)


   .. py:method:: conjugate(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.conjugate`.




   .. py:method:: copy(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.copy`.




   .. py:method:: cumprod(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.cumprod`.




   .. py:method:: cumsum(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.cumsum`.




   .. py:method:: data(*args, **kwargs)

      Pointer to start of data.




   .. py:method:: diagonal(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.diagonal`.




   .. py:method:: dtype(*args, **kwargs)

      Get array data-descriptor.




   .. py:method:: dump(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.dump`.




   .. py:method:: dumps(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.dumps`.




   .. py:method:: fill(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.fill`.




   .. py:method:: flags(*args, **kwargs)

      The integer value of flags.




   .. py:method:: flat(*args, **kwargs)

      A 1-D view of the scalar.




   .. py:method:: flatten(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.flatten`.




   .. py:method:: getfield(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.getfield`.




   .. py:method:: imag(*args, **kwargs)

      The imaginary part of the scalar.




   .. py:method:: item(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.item`.




   .. py:method:: itemset(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.itemset`.




   .. py:method:: itemsize(*args, **kwargs)

      The length of one element in bytes.




   .. py:method:: max(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.max`.




   .. py:method:: mean(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.mean`.




   .. py:method:: min(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.min`.




   .. py:method:: nbytes(*args, **kwargs)

      The length of the scalar in bytes.




   .. py:method:: ndim(*args, **kwargs)

      The number of array dimensions.




   .. py:method:: newbyteorder(*args, **kwargs)

      newbyteorder(new_order='S', /)

          Return a new `dtype` with a different byte order.

          Changes are also made in all fields and sub-arrays of the data type.

          The `new_order` code can be any from the following:

          * 'S' - swap dtype from current to opposite endian
          * {'<', 'little'} - little endian
          * {'>', 'big'} - big endian
          * {'=', 'native'} - native order
          * {'|', 'I'} - ignore (no change to byte order)

          Parameters
          ----------
          new_order : str, optional
              Byte order to force; a value from the byte order specifications
              above.  The default value ('S') results in swapping the current
              byte order.


          Returns
          -------
          new_dtype : dtype
              New `dtype` object with the given change to the byte order.




   .. py:method:: nonzero(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.nonzero`.




   .. py:method:: prod(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.prod`.




   .. py:method:: ptp(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.ptp`.




   .. py:method:: put(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.put`.




   .. py:method:: ravel(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.ravel`.




   .. py:method:: real(*args, **kwargs)

      The real part of the scalar.




   .. py:method:: repeat(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.repeat`.




   .. py:method:: reshape(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.reshape`.




   .. py:method:: resize(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.resize`.




   .. py:method:: round(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.round`.




   .. py:method:: searchsorted(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.searchsorted`.




   .. py:method:: setfield(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.setfield`.




   .. py:method:: setflags(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.setflags`.




   .. py:method:: shape(*args, **kwargs)

      Tuple of array dimensions.




   .. py:method:: size(*args, **kwargs)

      The number of elements in the gentype.




   .. py:method:: sort(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.sort`.




   .. py:method:: squeeze(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.squeeze`.




   .. py:method:: std(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.std`.




   .. py:method:: strides(*args, **kwargs)

      Tuple of bytes steps in each dimension.




   .. py:method:: sum(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.sum`.




   .. py:method:: swapaxes(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.swapaxes`.




   .. py:method:: take(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.take`.




   .. py:method:: tobytes(*args, **kwargs)


   .. py:method:: tofile(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tofile`.




   .. py:method:: tolist(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tolist`.




   .. py:method:: tostring(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tostring`.




   .. py:method:: trace(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.trace`.




   .. py:method:: transpose(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.transpose`.




   .. py:method:: var(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.var`.




   .. py:method:: view(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.view`.




.. py:function:: cast(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical], dt: Union[numpy.dtype, type, str, dtypes.dtypes.bigint], errors: _numeric.ErrorMode = ErrorMode.strict) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical, Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]]

   Cast an array to another dtype.

   :param pda: The array of values to cast
   :type pda: pdarray, Strings, or Categorical
   :param dt: The target dtype to cast values to
   :type dt: np.dtype, type, str, or bigint
   :param errors: Controls how errors are handled when casting strings to a numeric type
                  (ignored for casts from numeric types).
                      - strict: raise RuntimeError if *any* string cannot be converted
                      - ignore: never raise an error. Uninterpretable strings get
                          converted to NaN (float64), -2**63 (int64), zero (uint64 and
                          uint8), or False (bool)
                      - return_validity: in addition to returning the same output as
                        "ignore", also return a bool array indicating where the cast
                        was successful.
                  Default set to strict.
   :type errors: {strict, ignore, return_validity}, default=ErrorMode.strict

   :returns: * *pdarray or Strings* -- Array of values cast to desired dtype
             * **[validity** (*pdarray(bool)]*) -- If errors="return_validity" and input is Strings, a second array is
               returned with True where the cast succeeded and False where it failed.

   .. rubric:: Notes

   The cast is performed according to Chapel's casting rules and is NOT safe
   from overflows or underflows. The user must ensure that the target dtype
   has the precision and capacity to hold the desired result.

   .. rubric:: Examples

   >>> ak.cast(ak.linspace(1.0,5.0,5), dt=ak.int64)
   array([1 2 3 4 5])

   >>> ak.cast(ak.arange(0,5), dt=ak.float64).dtype
   dtype('float64')

   >>> ak.cast(ak.arange(0,5), dt=ak.bool_)
   array([False True True True True])

   >>> ak.cast(ak.linspace(0,4,5), dt=ak.bool_)
   array([False True True True True])


.. py:function:: cast(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical], dt: Union[numpy.dtype, type, str, dtypes.dtypes.bigint], errors: _numeric.ErrorMode = ErrorMode.strict) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical, Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]]

   Cast an array to another dtype.

   :param pda: The array of values to cast
   :type pda: pdarray, Strings, or Categorical
   :param dt: The target dtype to cast values to
   :type dt: np.dtype, type, str, or bigint
   :param errors: Controls how errors are handled when casting strings to a numeric type
                  (ignored for casts from numeric types).
                      - strict: raise RuntimeError if *any* string cannot be converted
                      - ignore: never raise an error. Uninterpretable strings get
                          converted to NaN (float64), -2**63 (int64), zero (uint64 and
                          uint8), or False (bool)
                      - return_validity: in addition to returning the same output as
                        "ignore", also return a bool array indicating where the cast
                        was successful.
                  Default set to strict.
   :type errors: {strict, ignore, return_validity}, default=ErrorMode.strict

   :returns: * *pdarray or Strings* -- Array of values cast to desired dtype
             * **[validity** (*pdarray(bool)]*) -- If errors="return_validity" and input is Strings, a second array is
               returned with True where the cast succeeded and False where it failed.

   .. rubric:: Notes

   The cast is performed according to Chapel's casting rules and is NOT safe
   from overflows or underflows. The user must ensure that the target dtype
   has the precision and capacity to hold the desired result.

   .. rubric:: Examples

   >>> ak.cast(ak.linspace(1.0,5.0,5), dt=ak.int64)
   array([1 2 3 4 5])

   >>> ak.cast(ak.arange(0,5), dt=ak.float64).dtype
   dtype('float64')

   >>> ak.cast(ak.arange(0,5), dt=ak.bool_)
   array([False True True True True])

   >>> ak.cast(ak.linspace(0,4,5), dt=ak.bool_)
   array([False True True True True])


.. py:class:: cdouble(value)

   Bases: :py:obj:`numpy.complexfloating`


   Complex number type composed of two double-precision floating-point
       numbers, compatible with Python `complex`.

       :Character code: ``'D'``
       :Canonical name: `numpy.cdouble`
       :Alias: `numpy.cfloat`
       :Alias: `numpy.complex_`
       :Alias on this platform (Linux x86_64): `numpy.complex128`: Complex number type composed of 2 64-bit-precision floating-point numbers.



.. py:function:: ceil(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise ceiling of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing ceiling values of the input array elements
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.ceil(ak.linspace(1.1,5.5,5))
   array([2.00000000000000000 3.00000000000000000 4.00000000000000000
   5.00000000000000000 6.00000000000000000])


.. py:class:: cfloat(value)

   Bases: :py:obj:`numpy.complexfloating`


   Complex number type composed of two double-precision floating-point
       numbers, compatible with Python `complex`.

       :Character code: ``'D'``
       :Canonical name: `numpy.cdouble`
       :Alias: `numpy.cfloat`
       :Alias: `numpy.complex_`
       :Alias on this platform (Linux x86_64): `numpy.complex128`: Complex number type composed of 2 64-bit-precision floating-point numbers.



.. py:class:: character(value)

   Bases: :py:obj:`numpy.flexible`


   Abstract base class of all character string scalar types.



.. py:function:: chisquare(f_obs, f_exp=None, ddof=0)

   Computes the chi square statistic and p-value.

   :param f_obs: The observed frequency.
   :type f_obs: pdarray
   :param f_exp: The expected frequency.
   :type f_exp: pdarray, default = None
   :param ddof: The delta degrees of freedom.
   :type ddof: int

   :rtype: arkouda.akstats.Power_divergenceResult

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.connect()
   >>> from arkouda.stats import chisquare
   >>> chisquare(ak.array([10, 20, 30, 10]), ak.array([10, 30, 20, 10]))
   Power_divergenceResult(statistic=8.333333333333334, pvalue=0.03960235520756414)

   .. seealso:: :obj:`scipy.stats.chisquare`, :obj:`arkouda.akstats.power_divergence`

   .. rubric:: References

   [1] Chi-squared test, https://en.wikipedia.org/wiki/Chi-squared_test

   [2] "scipy.stats.chisquare",
   https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html


.. py:function:: clear() -> None

   Send a clear message to clear all unregistered data from the server symbol table

   :rtype: None

   :raises RuntimeError: Raised if there is a server-side error in executing clear request


.. py:function:: clip(pda: arkouda.pdarrayclass.pdarray, lo: Union[float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, arkouda.pdarrayclass.pdarray], hi: Union[float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, arkouda.pdarrayclass.pdarray]) -> arkouda.pdarrayclass.pdarray

   Clip (limit) the values in an array to a given range [lo,hi]

   Given an array a, values outside the range are clipped to the
   range edges, such that all elements lie in the range.

   There is no check to enforce that lo < hi.  If lo > hi, the corresponding
   value of the array will be set to hi.

   If lo or hi (or both) are pdarrays, the check is by pairwise elements.
   See examples.

   :param pda: the array of values to clip
   :type pda: pdarray
   :param lo: the lower value of the clipping range
   :type lo: numeric_scalars or pdarray
   :param hi: the higher value of the clipping range
              If lo or hi (or both) are pdarrays, the check is by pairwise elements.
              See examples.
   :type hi: numeric_scalars or pdarray

   :returns:

             A pdarray matching pda, except that element x remains x if lo <= x <= hi,
                                                     or becomes lo if x < lo,
                                                     or becomes hi if x > hi.
   :rtype: arkouda.pdarrayclass.pdarray

   .. rubric:: Examples

   >>> a = ak.array([1,2,3,4,5,6,7,8,9,10])
   >>> ak.clip(a,3,8)
   array([3 3 3 4 5 6 7 8 8 8])
   >>> ak.clip(a,3,8.0)
   array([3.00000000000000000 3.00000000000000000 3.00000000000000000 4.00000000000000000
          5.00000000000000000 6.00000000000000000 7.00000000000000000 8.00000000000000000
          8.00000000000000000 8.00000000000000000])
   >>> ak.clip(a,None,7)
   array([1 2 3 4 5 6 7 7 7 7])
   >>> ak.clip(a,5,None)
   array([5 5 5 5 5 6 7 8 9 10])
   >>> ak.clip(a,None,None)
   ValueError: Either min or max must be supplied.
   >>> ak.clip(a,ak.array([2,2,3,3,8,8,5,5,6,6]),8)
   array([2 2 3 4 8 8 7 8 8 8])
   >>> ak.clip(a,4,ak.array([10,9,8,7,6,5,5,5,5,5]))
   array([4 4 4 4 5 5 5 5 5 5])

   .. rubric:: Notes

   Either lo or hi may be None, but not both.
   If lo > hi, all x = hi.
   If all inputs are int64, output is int64, but if any input is float64, output is float64.

   :raises ValueError: Raised if both lo and hi are None


.. py:class:: clongdouble(value)

   Bases: :py:obj:`numpy.complexfloating`


   Complex number type composed of two extended-precision floating-point
       numbers.

       :Character code: ``'G'``
       :Alias: `numpy.clongfloat`
       :Alias: `numpy.longcomplex`
       :Alias on this platform (Linux x86_64): `numpy.complex256`: Complex number type composed of 2 128-bit extended-precision floating-point numbers.



.. py:class:: clongfloat(value)

   Bases: :py:obj:`numpy.complexfloating`


   Complex number type composed of two extended-precision floating-point
       numbers.

       :Character code: ``'G'``
       :Alias: `numpy.clongfloat`
       :Alias: `numpy.longcomplex`
       :Alias on this platform (Linux x86_64): `numpy.complex256`: Complex number type composed of 2 128-bit extended-precision floating-point numbers.



.. py:function:: clz(pda: pdarray) -> pdarray

   Count leading zeros for each integer in an array.

   :param pda: Input array (must be integral).
   :type pda: pdarray, int64, uint64, bigint

   :returns: **lz** -- The number of leading zeros of each element.
   :rtype: pdarray

   :raises TypeError: If input array is not int64, uint64, or bigint

   .. rubric:: Examples

   >>> A = ak.arange(10)
   >>> ak.clz(A)
   array([64, 63, 62, 62, 61, 61, 61, 61, 60, 60])


.. py:function:: coargsort(arrays: Sequence[Union[arkouda.strings.Strings, arkouda.pdarrayclass.pdarray, arkouda.categorical.Categorical]], algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD) -> arkouda.pdarrayclass.pdarray

   Return the permutation that groups the rows (left-to-right), if the
   input arrays are treated as columns. The permutation sorts numeric
   columns, but not strings/Categoricals -- strings/Categoricals are grouped, but not ordered.

   :param arrays: The columns (int64, uint64, float64, Strings, or Categorical) to sort by row
   :type arrays: Sequence of Strings, pdarray, or Categorical
   :param algorithm: The algorithm to be used for sorting the arrays.
   :type algorithm: SortingAlgorithm, default=SortingAlgorithm.RadixSortLSD

   :returns: The indices that permute the rows to grouped order
   :rtype: pdarray of int64

   :raises ValueError: Raised if the pdarrays are not of the same size or if the parameter
       is not an Iterable containing pdarrays, Strings, or Categoricals

   .. seealso:: :obj:`argsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and resilient
   to non-uniformity in data but communication intensive. Starts with the
   last array and moves forward. This sort operates directly on numeric types,
   but for Strings, it operates on a hash. Thus, while grouping of equivalent
   strings is guaranteed, lexicographic ordering of the groups is not. For Categoricals,
   coargsort sorts based on Categorical.codes which guarantees grouping of equivalent categories
   but not lexicographic ordering of those groups.

   .. rubric:: Examples

   >>> a = ak.array([0, 1, 0, 1])
   >>> b = ak.array([1, 1, 0, 0])
   >>> perm = ak.coargsort([a, b])
   >>> perm
   array([2 0 3 1])
   >>> a[perm]
   array([0 0 1 1])
   >>> b[perm]
   array([0 1 0 1])


.. py:function:: coargsort(arrays: Sequence[Union[arkouda.strings.Strings, arkouda.pdarrayclass.pdarray, arkouda.categorical.Categorical]], algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD) -> arkouda.pdarrayclass.pdarray

   Return the permutation that groups the rows (left-to-right), if the
   input arrays are treated as columns. The permutation sorts numeric
   columns, but not strings/Categoricals -- strings/Categoricals are grouped, but not ordered.

   :param arrays: The columns (int64, uint64, float64, Strings, or Categorical) to sort by row
   :type arrays: Sequence of Strings, pdarray, or Categorical
   :param algorithm: The algorithm to be used for sorting the arrays.
   :type algorithm: SortingAlgorithm, default=SortingAlgorithm.RadixSortLSD

   :returns: The indices that permute the rows to grouped order
   :rtype: pdarray of int64

   :raises ValueError: Raised if the pdarrays are not of the same size or if the parameter
       is not an Iterable containing pdarrays, Strings, or Categoricals

   .. seealso:: :obj:`argsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and resilient
   to non-uniformity in data but communication intensive. Starts with the
   last array and moves forward. This sort operates directly on numeric types,
   but for Strings, it operates on a hash. Thus, while grouping of equivalent
   strings is guaranteed, lexicographic ordering of the groups is not. For Categoricals,
   coargsort sorts based on Categorical.codes which guarantees grouping of equivalent categories
   but not lexicographic ordering of those groups.

   .. rubric:: Examples

   >>> a = ak.array([0, 1, 0, 1])
   >>> b = ak.array([1, 1, 0, 0])
   >>> perm = ak.coargsort([a, b])
   >>> perm
   array([2 0 3 1])
   >>> a[perm]
   array([0 0 1 1])
   >>> b[perm]
   array([0 1 0 1])


.. py:function:: coargsort(arrays: Sequence[Union[arkouda.strings.Strings, arkouda.pdarrayclass.pdarray, arkouda.categorical.Categorical]], algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD) -> arkouda.pdarrayclass.pdarray

   Return the permutation that groups the rows (left-to-right), if the
   input arrays are treated as columns. The permutation sorts numeric
   columns, but not strings/Categoricals -- strings/Categoricals are grouped, but not ordered.

   :param arrays: The columns (int64, uint64, float64, Strings, or Categorical) to sort by row
   :type arrays: Sequence of Strings, pdarray, or Categorical
   :param algorithm: The algorithm to be used for sorting the arrays.
   :type algorithm: SortingAlgorithm, default=SortingAlgorithm.RadixSortLSD

   :returns: The indices that permute the rows to grouped order
   :rtype: pdarray of int64

   :raises ValueError: Raised if the pdarrays are not of the same size or if the parameter
       is not an Iterable containing pdarrays, Strings, or Categoricals

   .. seealso:: :obj:`argsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and resilient
   to non-uniformity in data but communication intensive. Starts with the
   last array and moves forward. This sort operates directly on numeric types,
   but for Strings, it operates on a hash. Thus, while grouping of equivalent
   strings is guaranteed, lexicographic ordering of the groups is not. For Categoricals,
   coargsort sorts based on Categorical.codes which guarantees grouping of equivalent categories
   but not lexicographic ordering of those groups.

   .. rubric:: Examples

   >>> a = ak.array([0, 1, 0, 1])
   >>> b = ak.array([1, 1, 0, 0])
   >>> perm = ak.coargsort([a, b])
   >>> perm
   array([2 0 3 1])
   >>> a[perm]
   array([0 0 1 1])
   >>> b[perm]
   array([0 1 0 1])


.. py:class:: complex128(value)

   Bases: :py:obj:`numpy.complexfloating`


   Complex number type composed of two double-precision floating-point
       numbers, compatible with Python `complex`.

       :Character code: ``'D'``
       :Canonical name: `numpy.cdouble`
       :Alias: `numpy.cfloat`
       :Alias: `numpy.complex_`
       :Alias on this platform (Linux x86_64): `numpy.complex128`: Complex number type composed of 2 64-bit-precision floating-point numbers.



.. py:class:: complex64(value)

   Bases: :py:obj:`numpy.complexfloating`


   Complex number type composed of two single-precision floating-point
       numbers.

       :Character code: ``'F'``
       :Canonical name: `numpy.csingle`
       :Alias: `numpy.singlecomplex`
       :Alias on this platform (Linux x86_64): `numpy.complex64`: Complex number type composed of 2 32-bit-precision floating-point numbers.



.. py:function:: compute_join_size(a: arkouda.pdarrayclass.pdarray, b: arkouda.pdarrayclass.pdarray) -> Tuple[int, int]

   Compute the internal size of a hypothetical join between a and b. Returns
   both the number of elements and number of bytes required for the join.


.. py:function:: concatenate(arrays: Sequence[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical]], ordered: bool = True) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical, Sequence[arkouda.categorical.Categorical]]

   Concatenate a list or tuple of ``pdarray`` or ``Strings`` objects into
   one ``pdarray`` or ``Strings`` object, respectively.

   :param arrays: The arrays to concatenate. Must all have same dtype.
   :type arrays: Sequence[Union[pdarray,Strings,Categorical]]
   :param ordered: If True (default), the arrays will be appended in the
                   order given. If False, array data may be interleaved
                   in blocks, which can greatly improve performance but
                   results in non-deterministic ordering of elements.
   :type ordered: bool

   :returns: Single pdarray or Strings object containing all values, returned in
             the original order
   :rtype: Union[pdarray,Strings,Categorical]

   :raises ValueError: Raised if arrays is empty or if 1..n pdarrays have
       differing dtypes
   :raises TypeError: Raised if arrays is not a pdarrays or Strings python Sequence such as a
       list or tuple
   :raises RuntimeError: Raised if 1..n array elements are dtypes for which
       concatenate has not been implemented.

   .. rubric:: Examples

   >>> ak.concatenate([ak.array([1, 2, 3]), ak.array([4, 5, 6])])
   array([1, 2, 3, 4, 5, 6])

   >>> ak.concatenate([ak.array([True,False,True]),ak.array([False,True,True])])
   array([True, False, True, False, True, True])

   >>> ak.concatenate([ak.array(['one','two']),ak.array(['three','four','five'])])
   array(['one', 'two', 'three', 'four', 'five'])


.. py:function:: concatenate(arrays: Sequence[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical]], ordered: bool = True) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical, Sequence[arkouda.categorical.Categorical]]

   Concatenate a list or tuple of ``pdarray`` or ``Strings`` objects into
   one ``pdarray`` or ``Strings`` object, respectively.

   :param arrays: The arrays to concatenate. Must all have same dtype.
   :type arrays: Sequence[Union[pdarray,Strings,Categorical]]
   :param ordered: If True (default), the arrays will be appended in the
                   order given. If False, array data may be interleaved
                   in blocks, which can greatly improve performance but
                   results in non-deterministic ordering of elements.
   :type ordered: bool

   :returns: Single pdarray or Strings object containing all values, returned in
             the original order
   :rtype: Union[pdarray,Strings,Categorical]

   :raises ValueError: Raised if arrays is empty or if 1..n pdarrays have
       differing dtypes
   :raises TypeError: Raised if arrays is not a pdarrays or Strings python Sequence such as a
       list or tuple
   :raises RuntimeError: Raised if 1..n array elements are dtypes for which
       concatenate has not been implemented.

   .. rubric:: Examples

   >>> ak.concatenate([ak.array([1, 2, 3]), ak.array([4, 5, 6])])
   array([1, 2, 3, 4, 5, 6])

   >>> ak.concatenate([ak.array([True,False,True]),ak.array([False,True,True])])
   array([True, False, True, False, True, True])

   >>> ak.concatenate([ak.array(['one','two']),ak.array(['three','four','five'])])
   array(['one', 'two', 'three', 'four', 'five'])


.. py:function:: concatenate(arrays: Sequence[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical]], ordered: bool = True) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical, Sequence[arkouda.categorical.Categorical]]

   Concatenate a list or tuple of ``pdarray`` or ``Strings`` objects into
   one ``pdarray`` or ``Strings`` object, respectively.

   :param arrays: The arrays to concatenate. Must all have same dtype.
   :type arrays: Sequence[Union[pdarray,Strings,Categorical]]
   :param ordered: If True (default), the arrays will be appended in the
                   order given. If False, array data may be interleaved
                   in blocks, which can greatly improve performance but
                   results in non-deterministic ordering of elements.
   :type ordered: bool

   :returns: Single pdarray or Strings object containing all values, returned in
             the original order
   :rtype: Union[pdarray,Strings,Categorical]

   :raises ValueError: Raised if arrays is empty or if 1..n pdarrays have
       differing dtypes
   :raises TypeError: Raised if arrays is not a pdarrays or Strings python Sequence such as a
       list or tuple
   :raises RuntimeError: Raised if 1..n array elements are dtypes for which
       concatenate has not been implemented.

   .. rubric:: Examples

   >>> ak.concatenate([ak.array([1, 2, 3]), ak.array([4, 5, 6])])
   array([1, 2, 3, 4, 5, 6])

   >>> ak.concatenate([ak.array([True,False,True]),ak.array([False,True,True])])
   array([True, False, True, False, True, True])

   >>> ak.concatenate([ak.array(['one','two']),ak.array(['three','four','five'])])
   array(['one', 'two', 'three', 'four', 'five'])


.. py:function:: convert_if_categorical(values)

   Convert a Categorical array to Strings for display


.. py:function:: corr(x: pdarray, y: pdarray) -> numpy.float64

   Return the correlation between x and y

   :param x: One of the pdarrays used to calculate correlation
   :type x: pdarray
   :param y: One of the pdarrays used to calculate correlation
   :type y: pdarray

   :returns: The scalar correlation of the two pdarrays
   :rtype: np.float64

   :raises TypeError: Raised if x or y is not a pdarray instance
   :raises RuntimeError: Raised if there's a server-side error thrown

   .. seealso:: :obj:`std`, :obj:`cov`

   .. rubric:: Notes

   The correlation is calculated by
   cov(x, y) / (x.std(ddof=1) * y.std(ddof=1))


.. py:function:: cos(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise cosine of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the cosine will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing cosine for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: cosh(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise hyperbolic cosine of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the hyperbolic cosine will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing hyperbolic cosine for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: count_nonzero(pda: arkouda.pdarrayclass.pdarray) -> numpy.int64

   Compute the nonzero count of a given array. 1D case only, for now.

   :param pda: The input data, in pdarray form, numeric, bool, or str
   :type pda: pdarray

   :returns: The nonzero count of the entire pdarray
   :rtype: np.int64

   :raises TypeError: Raised if the parameter is not a pdarray with numeric, bool, or str datatype
   :raises ValueError: Raised if sum applied to the pdarray doesn't come back with a scalar

   .. rubric:: Examples

   >>> pda = ak.array([0,4,7,8,1,3,5,2,-1])
   >>> ak.count_nonzero(pda)
   8
   >>> pda = ak.array([False,True,False,True,False])
   >>> ak.count_nonzero(pda)
   2
   >>> pda = ak.array(["hello","","there"])
   >>> ak.count_nonzero(pda)
   2


.. py:function:: cov(x: pdarray, y: pdarray) -> numpy.float64

   Return the covariance of x and y

   :param x: One of the pdarrays used to calculate covariance
   :type x: pdarray
   :param y: One of the pdarrays used to calculate covariance
   :type y: pdarray

   :returns: The scalar covariance of the two pdarrays
   :rtype: np.float64

   :raises TypeError: Raised if x or y is not a pdarray instance
   :raises RuntimeError: Raised if there's a server-side error thrown

   .. seealso:: :obj:`mean`, :obj:`var`

   .. rubric:: Notes

   The covariance is calculated by
   ``cov = ((x - x.mean()) * (y - y.mean())).sum() / (x.size - 1)``.


.. py:function:: create_pdarray(repMsg: str, max_bits=None) -> pdarray

   Return a pdarray instance pointing to an array created by the arkouda server.
   The user should not call this function directly.

   :param repMsg: space-delimited string containing the pdarray name, datatype, size
                  dimension, shape,and itemsize
   :type repMsg: str

   :returns: A pdarray with the same attributes and data as the pdarray; on GPU
   :rtype: pdarray

   :raises ValueError: If there's an error in parsing the repMsg parameter into the six
       values needed to create the pdarray instance
   :raises RuntimeError: Raised if a server-side error is thrown in the process of creating
       the pdarray instance


.. py:function:: create_pdarray(repMsg: str, max_bits=None) -> pdarray

   Return a pdarray instance pointing to an array created by the arkouda server.
   The user should not call this function directly.

   :param repMsg: space-delimited string containing the pdarray name, datatype, size
                  dimension, shape,and itemsize
   :type repMsg: str

   :returns: A pdarray with the same attributes and data as the pdarray; on GPU
   :rtype: pdarray

   :raises ValueError: If there's an error in parsing the repMsg parameter into the six
       values needed to create the pdarray instance
   :raises RuntimeError: Raised if a server-side error is thrown in the process of creating
       the pdarray instance


.. py:function:: create_pdarray(repMsg: str, max_bits=None) -> pdarray

   Return a pdarray instance pointing to an array created by the arkouda server.
   The user should not call this function directly.

   :param repMsg: space-delimited string containing the pdarray name, datatype, size
                  dimension, shape,and itemsize
   :type repMsg: str

   :returns: A pdarray with the same attributes and data as the pdarray; on GPU
   :rtype: pdarray

   :raises ValueError: If there's an error in parsing the repMsg parameter into the six
       values needed to create the pdarray instance
   :raises RuntimeError: Raised if a server-side error is thrown in the process of creating
       the pdarray instance


.. py:function:: create_pdarray(repMsg: str, max_bits=None) -> pdarray

   Return a pdarray instance pointing to an array created by the arkouda server.
   The user should not call this function directly.

   :param repMsg: space-delimited string containing the pdarray name, datatype, size
                  dimension, shape,and itemsize
   :type repMsg: str

   :returns: A pdarray with the same attributes and data as the pdarray; on GPU
   :rtype: pdarray

   :raises ValueError: If there's an error in parsing the repMsg parameter into the six
       values needed to create the pdarray instance
   :raises RuntimeError: Raised if a server-side error is thrown in the process of creating
       the pdarray instance


.. py:function:: create_pdarray(repMsg: str, max_bits=None) -> pdarray

   Return a pdarray instance pointing to an array created by the arkouda server.
   The user should not call this function directly.

   :param repMsg: space-delimited string containing the pdarray name, datatype, size
                  dimension, shape,and itemsize
   :type repMsg: str

   :returns: A pdarray with the same attributes and data as the pdarray; on GPU
   :rtype: pdarray

   :raises ValueError: If there's an error in parsing the repMsg parameter into the six
       values needed to create the pdarray instance
   :raises RuntimeError: Raised if a server-side error is thrown in the process of creating
       the pdarray instance


.. py:function:: create_sparray(repMsg: str, max_bits=None) -> sparray

   Return a sparray instance pointing to an array created by the arkouda server.
   The user should not call this function directly.

   :param repMsg: space-delimited string containing the sparray name, datatype, size
                  dimension, shape,and itemsize
   :type repMsg: str

   :returns: A sparray with the same attributes as on the server
   :rtype: sparray

   :raises ValueError: If there's an error in parsing the repMsg parameter into the six
       values needed to create the pdarray instance
   :raises RuntimeError: Raised if a server-side error is thrown in the process of creating
       the pdarray instance


.. py:function:: create_sparse_matrix(size: int, rows: arkouda.pdarrayclass.pdarray, cols: arkouda.pdarrayclass.pdarray, vals: arkouda.pdarrayclass.pdarray, layout: str) -> arkouda.sparrayclass.sparray

   Create a sparse matrix from three pdarrays representing the row indices,
   column indices, and values of the non-zero elements of the matrix.

   :param rows: The row indices of the non-zero elements
   :type rows: pdarray
   :param cols: The column indices of the non-zero elements
   :type cols: pdarray
   :param vals: The values of the non-zero elements
   :type vals: pdarray

   :returns: A sparse matrix with the specified row and column indices and values
   :rtype: sparray


.. py:class:: csingle(value)

   Bases: :py:obj:`numpy.complexfloating`


   Complex number type composed of two single-precision floating-point
       numbers.

       :Character code: ``'F'``
       :Canonical name: `numpy.csingle`
       :Alias: `numpy.singlecomplex`
       :Alias on this platform (Linux x86_64): `numpy.complex64`: Complex number type composed of 2 32-bit-precision floating-point numbers.



.. py:function:: ctz(pda: pdarray) -> pdarray

   Count trailing zeros for each integer in an array.

   :param pda: Input array (must be integral).
   :type pda: pdarray, int64, uint64, bigint

   :returns: **lz** -- The number of trailing zeros of each element.
   :rtype: pdarray

   .. rubric:: Notes

   ctz(0) is defined to be zero.

   :raises TypeError: If input array is not int64, uint64, or bigint

   .. rubric:: Examples

   >>> A = ak.arange(10)
   >>> ak.ctz(A)
   array([0, 0, 1, 0, 2, 0, 1, 0, 3, 0])


.. py:function:: cumprod(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the cumulative product over the array.

   The product is inclusive, such that the ``i`` th element of the
   result is the product of elements up to and including ``i``.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing cumulative products for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.cumprod(ak.arange(1,5))
   array([1 2 6 24])

   >>> ak.cumprod(ak.uniform(5,1.0,5.0))
   array([1.5728783400481925 7.0472855509390593 33.78523998586553
          134.05309592737584 450.21589865655358])


.. py:function:: cumsum(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the cumulative sum over the array.

   The sum is inclusive, such that the ``i`` th element of the
   result is the sum of elements up to and including ``i``.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing cumulative sums for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.cumsum(ak.arange(1,5))
   array([1 3 6 10])

   >>> ak.cumsum(ak.uniform(5,1.0,5.0))
   array([3.1598310770203937 5.4110385860243131 9.1622479306453748
          12.710615785506533 13.945880905466208])

   >>> ak.cumsum(ak.randint(0, 1, 5, dtype=ak.bool_))
   array([0 1 1 2 3])


.. py:function:: cumsum(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the cumulative sum over the array.

   The sum is inclusive, such that the ``i`` th element of the
   result is the sum of elements up to and including ``i``.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing cumulative sums for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.cumsum(ak.arange(1,5))
   array([1 3 6 10])

   >>> ak.cumsum(ak.uniform(5,1.0,5.0))
   array([3.1598310770203937 5.4110385860243131 9.1622479306453748
          12.710615785506533 13.945880905466208])

   >>> ak.cumsum(ak.randint(0, 1, 5, dtype=ak.bool_))
   array([0 1 1 2 3])


.. py:function:: cumsum(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the cumulative sum over the array.

   The sum is inclusive, such that the ``i`` th element of the
   result is the sum of elements up to and including ``i``.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing cumulative sums for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.cumsum(ak.arange(1,5))
   array([1 3 6 10])

   >>> ak.cumsum(ak.uniform(5,1.0,5.0))
   array([3.1598310770203937 5.4110385860243131 9.1622479306453748
          12.710615785506533 13.945880905466208])

   >>> ak.cumsum(ak.randint(0, 1, 5, dtype=ak.bool_))
   array([0 1 1 2 3])


.. py:function:: date_operators(cls)

.. py:function:: date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, inclusive='both', **kwargs)

   Creates a fixed frequency Datetime range. Alias for
   ``ak.Datetime(pd.date_range(args))``. Subject to size limit
   imposed by client.maxTransferBytes.

   :param start: Left bound for generating dates.
   :type start: str or datetime-like, optional
   :param end: Right bound for generating dates.
   :type end: str or datetime-like, optional
   :param periods: Number of periods to generate.
   :type periods: int, optional
   :param freq: Frequency strings can have multiples, e.g. '5H'. See
                timeseries.offset_aliases for a list of
                frequency aliases.
   :type freq: str or DateOffset, default 'D'
   :param tz: Time zone name for returning localized DatetimeIndex, for example
              'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is
              timezone-naive.
   :type tz: str or tzinfo, optional
   :param normalize: Normalize start/end dates to midnight before generating date range.
   :type normalize: bool, default False
   :param name: Name of the resulting DatetimeIndex.
   :type name: str, default None
   :param closed: Make the interval closed with respect to the given frequency to
                  the 'left', 'right', or both sides (None, the default).
                  *Deprecated*
   :type closed: {None, 'left', 'right'}, optional
   :param inclusive: Include boundaries. Whether to set each bound as closed or open.
   :type inclusive: {"both", "neither", "left", "right"}, default "both"
   :param \*\*kwargs: For compatibility. Has no effect on the result.

   :returns: **rng**
   :rtype: DatetimeIndex

   .. rubric:: Notes

   Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
   exactly three must be specified. If ``freq`` is omitted, the resulting
   ``DatetimeIndex`` will have ``periods`` linearly spaced elements between
   ``start`` and ``end`` (closed on both sides).

   To learn more about the frequency strings, please see `this link
   <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.


.. py:function:: date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, inclusive='both', **kwargs)

   Creates a fixed frequency Datetime range. Alias for
   ``ak.Datetime(pd.date_range(args))``. Subject to size limit
   imposed by client.maxTransferBytes.

   :param start: Left bound for generating dates.
   :type start: str or datetime-like, optional
   :param end: Right bound for generating dates.
   :type end: str or datetime-like, optional
   :param periods: Number of periods to generate.
   :type periods: int, optional
   :param freq: Frequency strings can have multiples, e.g. '5H'. See
                timeseries.offset_aliases for a list of
                frequency aliases.
   :type freq: str or DateOffset, default 'D'
   :param tz: Time zone name for returning localized DatetimeIndex, for example
              'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is
              timezone-naive.
   :type tz: str or tzinfo, optional
   :param normalize: Normalize start/end dates to midnight before generating date range.
   :type normalize: bool, default False
   :param name: Name of the resulting DatetimeIndex.
   :type name: str, default None
   :param closed: Make the interval closed with respect to the given frequency to
                  the 'left', 'right', or both sides (None, the default).
                  *Deprecated*
   :type closed: {None, 'left', 'right'}, optional
   :param inclusive: Include boundaries. Whether to set each bound as closed or open.
   :type inclusive: {"both", "neither", "left", "right"}, default "both"
   :param \*\*kwargs: For compatibility. Has no effect on the result.

   :returns: **rng**
   :rtype: DatetimeIndex

   .. rubric:: Notes

   Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
   exactly three must be specified. If ``freq`` is omitted, the resulting
   ``DatetimeIndex`` will have ``periods`` linearly spaced elements between
   ``start`` and ``end`` (closed on both sides).

   To learn more about the frequency strings, please see `this link
   <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.


.. py:class:: datetime64(value)

   Bases: :py:obj:`numpy.generic`


   If created from a 64-bit integer, it represents an offset from
       ``1970-01-01T00:00:00``.
       If created from string, the string can be in ISO 8601 date
       or datetime format.

       >>> np.datetime64(10, 'Y')
       numpy.datetime64('1980')
       >>> np.datetime64('1980', 'Y')
       numpy.datetime64('1980')
       >>> np.datetime64(10, 'D')
       numpy.datetime64('1970-01-11')

       See :ref:`arrays.datetime` for more information.

       :Character code: ``'M'``



.. py:function:: deg2rad(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Converts angles element-wise from degrees to radians.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True, the
                 corresponding value will be converted from degrees to radians. Elsewhere, it will retain its
                 original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing an angle converted to radians, from degrees, for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: delete(arr: arkouda.pdarrayclass.pdarray, obj: Union[arkouda.pdarrayclass.pdarray, slice, int], axis: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Return a copy of 'arr' with elements along the specified axis removed.

   :param arr: The array to remove elements from
   :type arr: pdarray
   :param obj: The indices to remove from 'arr'. If obj is a pdarray, it must
               have an integer dtype.
   :type obj: Union[pdarray, slice, int]
   :param axis: The axis along which to remove elements. If None, the array will
                be flattened before removing elements. Defaults to None.
   :type axis: Optional[int], optional

   :returns: A copy of 'arr' with elements removed
   :rtype: pdarray


.. py:function:: deprecate(*args, **kwargs)

   Issues a DeprecationWarning, adds warning to `old_name`'s
   docstring, rebinds ``old_name.__name__`` and returns the new
   function object.

   This function may also be used as a decorator.

   :param func: The function to be deprecated.
   :type func: function
   :param old_name: The name of the function to be deprecated. Default is None, in
                    which case the name of `func` is used.
   :type old_name: str, optional
   :param new_name: The new name for the function. Default is None, in which case the
                    deprecation message is that `old_name` is deprecated. If given, the
                    deprecation message is that `old_name` is deprecated and `new_name`
                    should be used instead.
   :type new_name: str, optional
   :param message: Additional explanation of the deprecation.  Displayed in the
                   docstring after the warning.
   :type message: str, optional

   :returns: **old_func** -- The deprecated function.
   :rtype: function

   .. rubric:: Examples

   Note that ``olduint`` returns a value after printing Deprecation
   Warning:

   >>> olduint = np.deprecate(np.uint)
   DeprecationWarning: `uint64` is deprecated! # may vary
   >>> olduint(6)
   6


.. py:function:: deprecate_with_doc(msg)

   Deprecates a function and includes the deprecation in its docstring.

   This function is used as a decorator. It returns an object that can be
   used to issue a DeprecationWarning, by passing the to-be decorated
   function as argument, this adds warning to the to-be decorated function's
   docstring and returns the new function object.

   .. seealso::

      :obj:`deprecate`
          Decorate a function such that it issues a `DeprecationWarning`

   :param msg: Additional explanation of the deprecation. Displayed in the
               docstring after the warning.
   :type msg: str

   :returns: **obj**
   :rtype: object


.. py:function:: disableVerbose(logLevel: LogLevel = LogLevel.INFO) -> None

   Disables verbose logging (DEBUG log level) for all ArkoudaLoggers, setting
   the log level for each to the logLevel parameter

   :param logLevel: The new log level, defaultts to LogLevel.INFO
   :type logLevel: LogLevel

   :raises TypeError: Raised if logLevel is not a LogLevel enum


.. py:function:: disp(mesg, device=None, linefeed=True)

   Display a message on a device.

   :param mesg: Message to display.
   :type mesg: str
   :param device: Device to write message. If None, defaults to ``sys.stdout`` which is
                  very similar to ``print``. `device` needs to have ``write()`` and
                  ``flush()`` methods.
   :type device: object
   :param linefeed: Option whether to print a line feed or not. Defaults to True.
   :type linefeed: bool, optional

   :raises AttributeError: If `device` does not have a ``write()`` or ``flush()`` method.

   .. rubric:: Examples

   Besides ``sys.stdout``, a file-like object can also be used as it has
   both required methods:

   >>> from io import StringIO
   >>> buf = StringIO()
   >>> np.disp(u'"Display" in a file', device=buf)
   >>> buf.getvalue()
   '"Display" in a file\n'


.. py:function:: divmod(x: Union[arkouda.numpy.dtypes.numeric_scalars, pdarray], y: Union[arkouda.numpy.dtypes.numeric_scalars, pdarray], where: Union[arkouda.numpy.dtypes.bool_scalars, pdarray] = True) -> Tuple[pdarray, pdarray]

   :param x: The dividend array, the values that will be the numerator of the floordivision and will be
             acted on by the bases for modular division.
   :type x: numeric_scalars(float_scalars, int_scalars) or pdarray
   :param y: The divisor array, the values that will be the denominator of the division and will be the
             bases for the modular division.
   :type y: numeric_scalars(float_scalars, int_scalars) or pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True, the
                 corresponding value will be divided using floor and modular division. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: Boolean or pdarray

   :returns: Returns a tuple that contains quotient and remainder of the division
   :rtype: (pdarray, pdarray)

   :raises TypeError: At least one entry must be a pdarray
   :raises ValueError: If both inputs are both pdarrays, their size must match
   :raises ZeroDivisionError: No entry in y is allowed to be 0, to prevent division by zero

   .. rubric:: Notes

   The div is calculated by x // y
   The mod is calculated by x % y

   .. rubric:: Examples

   >>> x = ak.arange(5, 10)
   >>> y = ak.array([2, 1, 4, 5, 8])
   >>> ak.divmod(x,y)
   (array([2 6 1 1 1]), array([1 0 3 3 1]))
   >>> ak.divmod(x,y, x % 2 == 0)
   (array([5 6 7 1 9]), array([5 0 7 3 9]))


.. py:function:: dot(pda1: Union[numpy.int64, numpy.float64, numpy.uint64, pdarray], pda2: Union[numpy.int64, numpy.float64, numpy.uint64, pdarray]) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

   Returns the sum of the elementwise product of two arrays of the same size (the dot product) or
   the product of a singleton element and an array.

   :param pda1:
   :type pda1: Union[numeric_scalars, pdarray]
   :param pda2:
   :type pda2: Union[numeric_scalars, pdarray]

   :returns: The sum of the elementwise product pda1 and pda2 or
             the product of a singleton element and an array.
   :rtype: Union[numeric_scalars, pdarray]

   :raises ValueError: Raised if the size of pda1 is not the same as pda2

   .. rubric:: Examples

   >>> x = ak.array([2, 3])
   >>> y = ak.array([4, 5])
   >>> ak.dot(x,y)
   23
   >>> ak.dot(x,2)
   array([4 6])


.. py:class:: double(value)

   Bases: :py:obj:`numpy.floating`


   Double-precision floating-point number type, compatible with Python `float`
       and C ``double``.

       :Character code: ``'d'``
       :Canonical name: `numpy.double`
       :Alias: `numpy.float_`
       :Alias on this platform (Linux x86_64): `numpy.float64`: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      double.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.double(10.0).as_integer_ratio()
              (10, 1)
              >>> np.double(0.0).as_integer_ratio()
              (0, 1)
              >>> np.double(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: fromhex(string, /)

      Create a floating-point number from a hexadecimal string.

      >>> float.fromhex('0x1.ffffp10')
      2047.984375
      >>> float.fromhex('-0x1p-1074')
      -5e-324




   .. py:method:: hex(/)

      Return a hexadecimal representation of a floating-point number.

      >>> (-0.1).hex()
      '-0x1.999999999999ap-4'
      >>> 3.14159.hex()
      '0x1.921f9f01b866ep+1'




   .. py:method:: is_integer(*args, **kwargs)

      double.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.double(-2.0).is_integer()
              True
              >>> np.double(3.2).is_integer()
              False




.. py:function:: dtype(x)

.. py:data:: e
   :type:  float

.. py:function:: enableVerbose() -> None

   Enables verbose logging (DEBUG log level) for all ArkoudaLoggers


.. py:data:: euler_gamma
   :type:  float

.. py:function:: exp(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise exponential of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing exponential values of the input
             array elements
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.exp(ak.arange(1,5))
   array([2.7182818284590451 7.3890560989306504 20.085536923187668 54.598150033144236])

   >>> ak.exp(ak.uniform(5,1.0,5.0))
   array([11.84010843172504 46.454368507659211 5.5571769623557188
          33.494295836924771 13.478894913238722])


.. py:function:: expm1(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise exponential of the array minus one.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing e raised to each of the inputs,
             then subtracting one.
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.expm1(ak.arange(1,5))
   array([1.7182818284590451 6.3890560989306504 19.085536923187668 53.598150033144236])

   >>> ak.expm1(ak.uniform(5,1.0,5.0))
   array([10.84010843172504 45.454368507659211 4.5571769623557188
          32.494295836924771 12.478894913238722])


.. py:function:: export(read_path: str, dataset_name: str = 'ak_data', write_file: Optional[str] = None, return_obj: bool = True, index: bool = False)

   Export data from Arkouda file (Parquet/HDF5) to Pandas object or file formatted to be
   readable by Pandas

   :param read_path: path to file where arkouda data is stored.
   :type read_path: str
   :param dataset_name: name to store dataset under
   :type dataset_name: str
   :param index: Default False. When True, maintain the indexes loaded from the pandas file
   :type index: bool
   :param write_file: path to file to write pandas formatted data to. Only write the file if this is set
   :type write_file: str, optional
   :param return_obj: Default True. When True return the Pandas DataFrame object, otherwise return None
   :type return_obj: bool, optional

   :raises RuntimeError: - Unsupported file type

   :returns: When `return_obj=True`
   :rtype: pd.DataFrame

   .. seealso:: :obj:`pandas.DataFrame.to_parquet`, :obj:`pandas.DataFrame.to_hdf`, :obj:`pandas.DataFrame.read_parquet`, :obj:`pandas.DataFrame.read_hdf`, :obj:`ak.import_data`

   .. rubric:: Notes

   - If Arkouda file is exported for pandas, the format will not change. This mean parquet files
     will remain parquet and hdf5 will remain hdf5.
   - Export can only be performed from hdf5 or parquet files written by Arkouda. The result will be
     the same file type, but formatted to be read by Pandas.


.. py:function:: eye(rows: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64], cols: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64], diag: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64] = 0, dt: type = numpy.int64) -> arkouda.pdarrayclass.pdarray

   Return a pdarray with zeros everywhere except along a diagonal, which is all ones.
   The matrix need not be square.

   :param rows:
   :type rows: int_scalars
   :param cols:
   :type cols: int_scalars
   :param diag: | if diag = 0, zeros start at element [0,0] and proceed along diagonal
                | if diag > 0, zeros start at element [0,diag] and proceed along diagonal
                | if diag < 0, zeros start at element [diag,0] and proceed along diagonal
                | etc. Default set to 0.
   :type diag: int_scalars, default=0
   :param dt: The data type of the elements in the matrix being returned. Default set to ak_int64
   :type dt: type, default=ak_int64

   :returns: an array of zeros with ones along the specified diagonal
   :rtype: pdarray

   .. rubric:: Examples

   >>> ak.eye(rows=4,cols=4,diag=0,dt=ak.int64)
   array([array([1 0 0 0]) array([0 1 0 0]) array([0 0 1 0]) array([0 0 0 1])])
   >>> ak.eye(rows=3,cols=3,diag=1,dt=ak.float64)
   array([array([0.00000000000000000 1.00000000000000000 0.00000000000000000])
   array([0.00000000000000000 0.00000000000000000 1.00000000000000000])
   array([0.00000000000000000 0.00000000000000000 0.00000000000000000])])
   >>> ak.eye(rows=4,cols=4,diag=-1,dt=ak.bool_)
   array([array([False False False False]) array([True False False False])
   array([False True False False]) array([False False True False])])

   .. rubric:: Notes

   if rows = cols and diag = 0, the result is an identity matrix
   Server returns an error if rank of pda < 2


.. py:function:: find(query, space, all_occurrences=False, remove_missing=False)

   Return indices of query items in a search list of items.

   :param query: The items to search for. If multiple arrays, each "row" is an item.
   :type query: (sequence of) array-like
   :param space: The set of items in which to search. Must have same shape/dtype as query.
   :type space: (sequence of) array-like
   :param all_occurrences: When duplicate terms are present in search space, if all_occurrences is True,
                           return all occurrences found as a SegArray, otherwise return only the first
                           occurrences as a pdarray. Defaults to only finding the first occurrence.
                           Finding all occurrences is not yet supported on sequences of arrays
   :type all_occurrences: bool
   :param remove_missing: If all_occurrences is True, remove_missing is automatically enabled.
                          If False, return -1 for any items in query not found in space. If True,
                          remove these and only return indices of items that are found.
   :type remove_missing: bool

   :returns: **indices** -- For each item in query, its index in space. If all_occurrences is False,
             the return will be a pdarray of the first index where each value in the
             query appears in the space. If all_occurrences is True, the return will be
             a SegArray containing every index where each value in the query appears in
             the space. If all_occurrences is True, remove_missing is automatically enabled.
             If remove_missing is True, exclude missing values, otherwise return -1.
   :rtype: pdarray or SegArray

   .. rubric:: Examples

   >>> select_from = ak.arange(10)
   >>> arr1 = select_from[ak.randint(0, select_from.size, 20, seed=10)]
   >>> arr2 = select_from[ak.randint(0, select_from.size, 20, seed=11)]

   Remove some values to ensure we have some values
   which don't appear in the search space

   >>> arr2 = arr2[arr2 != 9]
   >>> arr2 = arr2[arr2 != 3]

   Find with defaults (all_occurrences and remove_missing both False)

   >>> ak.find(arr1, arr2)
   array([-1 -1 -1 0 1 -1 -1 -1 2 -1 5 -1 8 -1 5 -1 -1 11 5 0])

   Set remove_missing to True, only difference from default
   is missing values are excluded

   >>> ak.find(arr1, arr2, remove_missing=True)
   array([0 1 2 5 8 5 11 5 0])

   Set both remove_missing and all_occurrences to True, missing values
   will be empty segments

   >>> ak.find(arr1, arr2, remove_missing=True, all_occurrences=True).to_list()
   [[],
    [],
    [],
    [0, 4],
    [1, 3, 10],
    [],
    [],
    [],
    [2, 6, 12, 13],
    [],
    [5, 7],
    [],
    [8, 9, 14],
    [],
    [5, 7],
    [],
    [],
    [11, 15],
    [5, 7],
    [0, 4]]


.. py:class:: finfo

   finfo(dtype)

   Machine limits for floating point types.

   .. attribute:: bits

      The number of bits occupied by the type.

      :type: int

   .. attribute:: dtype

      Returns the dtype for which `finfo` returns information. For complex
      input, the returned dtype is the associated ``float*`` dtype for its
      real and complex components.

      :type: dtype

   .. attribute:: eps

      The difference between 1.0 and the next smallest representable float
      larger than 1.0. For example, for 64-bit binary floats in the IEEE-754
      standard, ``eps = 2**-52``, approximately 2.22e-16.

      :type: float

   .. attribute:: epsneg

      The difference between 1.0 and the next smallest representable float
      less than 1.0. For example, for 64-bit binary floats in the IEEE-754
      standard, ``epsneg = 2**-53``, approximately 1.11e-16.

      :type: float

   .. attribute:: iexp

      The number of bits in the exponent portion of the floating point
      representation.

      :type: int

   .. attribute:: machep

      The exponent that yields `eps`.

      :type: int

   .. attribute:: max

      The largest representable number.

      :type: floating point number of the appropriate type

   .. attribute:: maxexp

      The smallest positive power of the base (2) that causes overflow.

      :type: int

   .. attribute:: min

      The smallest representable number, typically ``-max``.

      :type: floating point number of the appropriate type

   .. attribute:: minexp

      The most negative power of the base (2) consistent with there
      being no leading 0's in the mantissa.

      :type: int

   .. attribute:: negep

      The exponent that yields `epsneg`.

      :type: int

   .. attribute:: nexp

      The number of bits in the exponent including its sign and bias.

      :type: int

   .. attribute:: nmant

      The number of bits in the mantissa.

      :type: int

   .. attribute:: precision

      The approximate number of decimal digits to which this kind of
      float is precise.

      :type: int

   .. attribute:: resolution

      The approximate decimal resolution of this type, i.e.,
      ``10**-precision``.

      :type: floating point number of the appropriate type

   .. attribute:: tiny

      An alias for `smallest_normal`, kept for backwards compatibility.

      :type: float

   .. attribute:: smallest_normal

      The smallest positive floating point number with 1 as leading bit in
      the mantissa following IEEE-754 (see Notes).

      :type: float

   .. attribute:: smallest_subnormal

      The smallest positive floating point number with 0 as leading bit in
      the mantissa following IEEE-754.

      :type: float

   :param dtype: Kind of floating point or complex floating point
                 data-type about which to get information.
   :type dtype: float, dtype, or instance

   .. seealso::

      :obj:`iinfo`
          The equivalent for integer data types.

      :obj:`spacing`
          The distance between a value and the nearest adjacent number

      :obj:`nextafter`
          The next floating point value after x1 towards x2

   .. rubric:: Notes

   For developers of NumPy: do not instantiate this at the module level.
   The initial calculation of these parameters is expensive and negatively
   impacts import times.  These objects are cached, so calling ``finfo()``
   repeatedly inside your functions is not a problem.

   Note that ``smallest_normal`` is not actually the smallest positive
   representable value in a NumPy floating point type. As in the IEEE-754
   standard [1]_, NumPy floating point types make use of subnormal numbers to
   fill the gap between 0 and ``smallest_normal``. However, subnormal numbers
   may have significantly reduced precision [2]_.

   This function can also be used for complex data types as well. If used,
   the output will be the same as the corresponding real float type
   (e.g. numpy.finfo(numpy.csingle) is the same as numpy.finfo(numpy.single)).
   However, the output is true for the real and imaginary components.

   .. rubric:: References

   .. [1] IEEE Standard for Floating-Point Arithmetic, IEEE Std 754-2008,
          pp.1-70, 2008, http://www.doi.org/10.1109/IEEESTD.2008.4610935
   .. [2] Wikipedia, "Denormal Numbers",
          https://en.wikipedia.org/wiki/Denormal_number

   .. rubric:: Examples

   >>> np.finfo(np.float64).dtype
   dtype('float64')
   >>> np.finfo(np.complex64).dtype
   dtype('float32')


   .. py:property:: smallest_normal

      Return the value for the smallest normal.

      :returns: **smallest_normal** -- Value for the smallest normal.
      :rtype: float

      :Warns: **UserWarning** -- If the calculated value for the smallest normal is requested for
              double-double.


   .. py:property:: tiny

      Return the value for tiny, alias of smallest_normal.

      :returns: **tiny** -- Value for the smallest normal, alias of smallest_normal.
      :rtype: float

      :Warns: **UserWarning** -- If the calculated value for the smallest normal is requested for
              double-double.


.. py:class:: flexible(value)

   Bases: :py:obj:`numpy.generic`


   Abstract base class of all scalar types without predefined length.
       The actual size of these types depends on the specific `np.dtype`
       instantiation.



.. py:function:: flip(x: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical], /, *, axis: Union[int, Tuple[int, Ellipsis], NoneType] = None) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical]

   Reverse an array's values along a particular axis or axes.

   :param x: Reverse the order of elements in an array along the given axis.

             The shape of the array is preserved, but the elements are reordered.
   :type x: pdarray, Strings, or Categorical
   :param axis: The axis or axes along which to flip the array. If None, flip the array along all axes.
   :type axis: int or Tuple[int, ...], optional

   :returns: An array with the entries of axis reversed.
   :rtype: pdarray, Strings, or Categorical

   .. note:: This differs from numpy as it actually reverses the data, rather than presenting a view.


.. py:class:: float16(value)

   Bases: :py:obj:`numpy.floating`


   Half-precision floating-point number type.

       :Character code: ``'e'``
       :Canonical name: `numpy.half`
       :Alias on this platform (Linux x86_64): `numpy.float16`: 16-bit-precision floating-point number type: sign bit, 5 bits exponent, 10 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      half.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.half(10.0).as_integer_ratio()
              (10, 1)
              >>> np.half(0.0).as_integer_ratio()
              (0, 1)
              >>> np.half(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: is_integer(*args, **kwargs)

      half.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.half(-2.0).is_integer()
              True
              >>> np.half(3.2).is_integer()
              False




.. py:class:: float32(value)

   Bases: :py:obj:`numpy.floating`


   Single-precision floating-point number type, compatible with C ``float``.

       :Character code: ``'f'``
       :Canonical name: `numpy.single`
       :Alias on this platform (Linux x86_64): `numpy.float32`: 32-bit-precision floating-point number type: sign bit, 8 bits exponent, 23 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      single.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.single(10.0).as_integer_ratio()
              (10, 1)
              >>> np.single(0.0).as_integer_ratio()
              (0, 1)
              >>> np.single(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: is_integer(*args, **kwargs)

      single.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.single(-2.0).is_integer()
              True
              >>> np.single(3.2).is_integer()
              False




.. py:class:: float64(value)

   Bases: :py:obj:`numpy.floating`


   Double-precision floating-point number type, compatible with Python `float`
       and C ``double``.

       :Character code: ``'d'``
       :Canonical name: `numpy.double`
       :Alias: `numpy.float_`
       :Alias on this platform (Linux x86_64): `numpy.float64`: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      double.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.double(10.0).as_integer_ratio()
              (10, 1)
              >>> np.double(0.0).as_integer_ratio()
              (0, 1)
              >>> np.double(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: fromhex(string, /)

      Create a floating-point number from a hexadecimal string.

      >>> float.fromhex('0x1.ffffp10')
      2047.984375
      >>> float.fromhex('-0x1p-1074')
      -5e-324




   .. py:method:: hex(/)

      Return a hexadecimal representation of a floating-point number.

      >>> (-0.1).hex()
      '-0x1.999999999999ap-4'
      >>> 3.14159.hex()
      '0x1.921f9f01b866ep+1'




   .. py:method:: is_integer(*args, **kwargs)

      double.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.double(-2.0).is_integer()
              True
              >>> np.double(3.2).is_integer()
              False




.. py:class:: float_(value)

   Bases: :py:obj:`numpy.floating`


   Double-precision floating-point number type, compatible with Python `float`
       and C ``double``.

       :Character code: ``'d'``
       :Canonical name: `numpy.double`
       :Alias: `numpy.float_`
       :Alias on this platform (Linux x86_64): `numpy.float64`: 64-bit precision floating-point number type: sign bit, 11 bits exponent, 52 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      double.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.double(10.0).as_integer_ratio()
              (10, 1)
              >>> np.double(0.0).as_integer_ratio()
              (0, 1)
              >>> np.double(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: fromhex(string, /)

      Create a floating-point number from a hexadecimal string.

      >>> float.fromhex('0x1.ffffp10')
      2047.984375
      >>> float.fromhex('-0x1p-1074')
      -5e-324




   .. py:method:: hex(/)

      Return a hexadecimal representation of a floating-point number.

      >>> (-0.1).hex()
      '-0x1.999999999999ap-4'
      >>> 3.14159.hex()
      '0x1.921f9f01b866ep+1'




   .. py:method:: is_integer(*args, **kwargs)

      double.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.double(-2.0).is_integer()
              True
              >>> np.double(3.2).is_integer()
              False




.. py:class:: float_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:class:: floating(value)

   Bases: :py:obj:`numpy.inexact`


   Abstract base class of all floating-point scalar types.



.. py:function:: floor(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise floor of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing floor values of the input array elements
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.floor(ak.linspace(1.1,5.5,5))
   array([1.00000000000000000 2.00000000000000000 3.00000000000000000
   4.00000000000000000 5.00000000000000000])


.. py:function:: fmod(dividend: Union[pdarray, arkouda.numpy.dtypes.numeric_scalars], divisor: Union[pdarray, arkouda.numpy.dtypes.numeric_scalars]) -> pdarray

   Returns the element-wise remainder of division.

   It is equivalent to np.fmod, the remainder has the same sign as the dividend.

   :param dividend: The array being acted on by the bases for the modular division.
   :type dividend: numeric scalars or pdarray
   :param divisor: The array that will be the bases for the modular division.
   :type divisor: numeric scalars or pdarray

   :returns: Returns an array that contains the element-wise remainder of division.
   :rtype: pdarray

   :raises TypeError: Raised if neither dividend nor divisor is a pdarray (at least one must be)
       or if any scalar or pdarray element is not one of int, uint, float, bigint


.. py:function:: format_float_positional(x, precision=None, unique=True, fractional=True, trim='k', sign=False, pad_left=None, pad_right=None, min_digits=None)

   Format a floating-point scalar as a decimal string in positional notation.

   Provides control over rounding, trimming and padding. Uses and assumes
   IEEE unbiased rounding. Uses the "Dragon4" algorithm.

   :param x: Value to format.
   :type x: python float or numpy floating scalar
   :param precision: Maximum number of digits to print. May be None if `unique` is
                     `True`, but must be an integer if unique is `False`.
   :type precision: non-negative integer or None, optional
   :param unique: If `True`, use a digit-generation strategy which gives the shortest
                  representation which uniquely identifies the floating-point number from
                  other values of the same type, by judicious rounding. If `precision`
                  is given fewer digits than necessary can be printed, or if `min_digits`
                  is given more can be printed, in which cases the last digit is rounded
                  with unbiased rounding.
                  If `False`, digits are generated as if printing an infinite-precision
                  value and stopping after `precision` digits, rounding the remaining
                  value with unbiased rounding
   :type unique: boolean, optional
   :param fractional: If `True`, the cutoffs of `precision` and `min_digits` refer to the
                      total number of digits after the decimal point, including leading
                      zeros.
                      If `False`, `precision` and `min_digits` refer to the total number of
                      significant digits, before or after the decimal point, ignoring leading
                      zeros.
   :type fractional: boolean, optional
   :param trim: Controls post-processing trimming of trailing digits, as follows:

                * 'k' : keep trailing zeros, keep decimal point (no trimming)
                * '.' : trim all trailing zeros, leave decimal point
                * '0' : trim all but the zero before the decimal point. Insert the
                  zero if it is missing.
                * '-' : trim trailing zeros and any trailing decimal point
   :type trim: one of 'k', '.', '0', '-', optional
   :param sign: Whether to show the sign for positive values.
   :type sign: boolean, optional
   :param pad_left: Pad the left side of the string with whitespace until at least that
                    many characters are to the left of the decimal point.
   :type pad_left: non-negative integer, optional
   :param pad_right: Pad the right side of the string with whitespace until at least that
                     many characters are to the right of the decimal point.
   :type pad_right: non-negative integer, optional
   :param min_digits: Minimum number of digits to print. Only has an effect if `unique=True`
                      in which case additional digits past those necessary to uniquely
                      identify the value may be printed, rounding the last additional digit.

                      -- versionadded:: 1.21.0
   :type min_digits: non-negative integer or None, optional

   :returns: **rep** -- The string representation of the floating point value
   :rtype: string

   .. seealso:: :obj:`format_float_scientific`

   .. rubric:: Examples

   >>> np.format_float_positional(np.float32(np.pi))
   '3.1415927'
   >>> np.format_float_positional(np.float16(np.pi))
   '3.14'
   >>> np.format_float_positional(np.float16(0.3))
   '0.3'
   >>> np.format_float_positional(np.float16(0.3), unique=False, precision=10)
   '0.3000488281'


.. py:function:: format_float_scientific(x, precision=None, unique=True, trim='k', sign=False, pad_left=None, exp_digits=None, min_digits=None)

   Format a floating-point scalar as a decimal string in scientific notation.

   Provides control over rounding, trimming and padding. Uses and assumes
   IEEE unbiased rounding. Uses the "Dragon4" algorithm.

   :param x: Value to format.
   :type x: python float or numpy floating scalar
   :param precision: Maximum number of digits to print. May be None if `unique` is
                     `True`, but must be an integer if unique is `False`.
   :type precision: non-negative integer or None, optional
   :param unique: If `True`, use a digit-generation strategy which gives the shortest
                  representation which uniquely identifies the floating-point number from
                  other values of the same type, by judicious rounding. If `precision`
                  is given fewer digits than necessary can be printed. If `min_digits`
                  is given more can be printed, in which cases the last digit is rounded
                  with unbiased rounding.
                  If `False`, digits are generated as if printing an infinite-precision
                  value and stopping after `precision` digits, rounding the remaining
                  value with unbiased rounding
   :type unique: boolean, optional
   :param trim: Controls post-processing trimming of trailing digits, as follows:

                * 'k' : keep trailing zeros, keep decimal point (no trimming)
                * '.' : trim all trailing zeros, leave decimal point
                * '0' : trim all but the zero before the decimal point. Insert the
                  zero if it is missing.
                * '-' : trim trailing zeros and any trailing decimal point
   :type trim: one of 'k', '.', '0', '-', optional
   :param sign: Whether to show the sign for positive values.
   :type sign: boolean, optional
   :param pad_left: Pad the left side of the string with whitespace until at least that
                    many characters are to the left of the decimal point.
   :type pad_left: non-negative integer, optional
   :param exp_digits: Pad the exponent with zeros until it contains at least this many digits.
                      If omitted, the exponent will be at least 2 digits.
   :type exp_digits: non-negative integer, optional
   :param min_digits: Minimum number of digits to print. This only has an effect for
                      `unique=True`. In that case more digits than necessary to uniquely
                      identify the value may be printed and rounded unbiased.

                      -- versionadded:: 1.21.0
   :type min_digits: non-negative integer or None, optional

   :returns: **rep** -- The string representation of the floating point value
   :rtype: string

   .. seealso:: :obj:`format_float_positional`

   .. rubric:: Examples

   >>> np.format_float_scientific(np.float32(np.pi))
   '3.1415927e+00'
   >>> s = np.float32(1.23e24)
   >>> np.format_float_scientific(s, unique=False, precision=15)
   '1.230000071797338e+24'
   >>> np.format_float_scientific(s, exp_digits=4)
   '1.23e+0024'


.. py:class:: format_parser

   Class to convert formats, names, titles description to a dtype.

   After constructing the format_parser object, the dtype attribute is
   the converted data-type:
   ``dtype = format_parser(formats, names, titles).dtype``

   .. attribute:: dtype

      The converted data-type.

      :type: dtype

   :param formats: The format description, either specified as a string with
                   comma-separated format descriptions in the form ``'f8, i4, a5'``, or
                   a list of format description strings  in the form
                   ``['f8', 'i4', 'a5']``.
   :type formats: str or list of str
   :param names: The field names, either specified as a comma-separated string in the
                 form ``'col1, col2, col3'``, or as a list or tuple of strings in the
                 form ``['col1', 'col2', 'col3']``.
                 An empty list can be used, in that case default field names
                 ('f0', 'f1', ...) are used.
   :type names: str or list/tuple of str
   :param titles: Sequence of title strings. An empty list can be used to leave titles
                  out.
   :type titles: sequence
   :param aligned: If True, align the fields by padding as the C-compiler would.
                   Default is False.
   :type aligned: bool, optional
   :param byteorder: If specified, all the fields will be changed to the
                     provided byte-order.  Otherwise, the default byte-order is
                     used. For all available string specifiers, see `dtype.newbyteorder`.
   :type byteorder: str, optional

   .. seealso:: :obj:`dtype`, :obj:`typename`, :obj:`sctype2char`

   .. rubric:: Examples

   >>> np.format_parser(['<f8', '<i4', '<a5'], ['col1', 'col2', 'col3'],
   ...                  ['T1', 'T2', 'T3']).dtype
   dtype([(('T1', 'col1'), '<f8'), (('T2', 'col2'), '<i4'), (('T3', 'col3'), 'S5')])

   `names` and/or `titles` can be empty lists. If `titles` is an empty list,
   titles will simply not appear. If `names` is empty, default field names
   will be used.

   >>> np.format_parser(['f8', 'i4', 'a5'], ['col1', 'col2', 'col3'],
   ...                  []).dtype
   dtype([('col1', '<f8'), ('col2', '<i4'), ('col3', '<S5')])
   >>> np.format_parser(['<f8', '<i4', '<a5'], [], []).dtype
   dtype([('f0', '<f8'), ('f1', '<i4'), ('f2', 'S5')])


.. py:function:: from_series(series: pandas.Series, dtype: Optional[Union[type, str]] = None) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Converts a Pandas Series to an Arkouda pdarray or Strings object. If
   dtype is None, the dtype is inferred from the Pandas Series. Otherwise,
   the dtype parameter is set if the dtype of the Pandas Series is to be
   overridden or is  unknown (for example, in situations where the Series
   dtype is object).

   :param series: The Pandas Series with a dtype of bool, float64, int64, or string
   :type series: Pandas Series
   :param dtype: The valid dtype types are np.bool, np.float64, np.int64, and np.str
   :type dtype: Optional[type]

   :rtype: Union[pdarray,Strings]

   :raises TypeError: Raised if series is not a Pandas Series object
   :raises ValueError: Raised if the Series dtype is not bool, float64, int64, string, datetime, or timedelta

   .. rubric:: Examples

   >>> np.random.seed(1701)
   >>> ak.from_series(pd.Series(np.random.randint(0,10,5)))
   array([4 3 3 5 0])

   >>> ak.from_series(pd.Series(['1', '2', '3', '4', '5']),dtype=np.int64)
   array([1 2 3 4 5])

   >>> np.random.seed(1701)
   >>> ak.from_series(pd.Series(np.random.uniform(low=0.0,high=1.0,size=3)))
   array([0.089433234324597599 0.1153776854774361 0.51874393620990389])

   >>> ak.from_series(pd.Series(['0.57600036956445599', '0.41619265571741659',
                      '0.6615356693784662']), dtype=np.float64)
   array([0.57600036956445599 0.41619265571741659 0.6615356693784662])

   >>> np.random.seed(1864)
   >>> ak.from_series(pd.Series(np.random.choice([True, False],size=5)))
   array([True True True False False])

   >>> ak.from_series(pd.Series(['True', 'False', 'False', 'True', 'True']), dtype=bool)
   array([True True True True True])

   >>> ak.from_series(pd.Series(['a', 'b', 'c', 'd', 'e'], dtype="string"))
   array(['a', 'b', 'c', 'd', 'e'])

   >>> ak.from_series(pd.Series(pd.to_datetime(['1/1/2018', np.datetime64('2018-01-01')])))
   array([1514764800000000000 1514764800000000000])

   .. rubric:: Notes

   The supported datatypes are bool, float64, int64, string, and datetime64[ns]. The
   data type is either inferred from the the Series or is set via the dtype parameter.

   Series of datetime or timedelta are converted to Arkouda arrays of dtype int64 (nanoseconds)

   A Pandas Series containing strings has a dtype of object. Arkouda assumes the Series
   contains strings and sets the dtype to str


.. py:function:: from_series(series: pandas.Series, dtype: Optional[Union[type, str]] = None) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Converts a Pandas Series to an Arkouda pdarray or Strings object. If
   dtype is None, the dtype is inferred from the Pandas Series. Otherwise,
   the dtype parameter is set if the dtype of the Pandas Series is to be
   overridden or is  unknown (for example, in situations where the Series
   dtype is object).

   :param series: The Pandas Series with a dtype of bool, float64, int64, or string
   :type series: Pandas Series
   :param dtype: The valid dtype types are np.bool, np.float64, np.int64, and np.str
   :type dtype: Optional[type]

   :rtype: Union[pdarray,Strings]

   :raises TypeError: Raised if series is not a Pandas Series object
   :raises ValueError: Raised if the Series dtype is not bool, float64, int64, string, datetime, or timedelta

   .. rubric:: Examples

   >>> np.random.seed(1701)
   >>> ak.from_series(pd.Series(np.random.randint(0,10,5)))
   array([4 3 3 5 0])

   >>> ak.from_series(pd.Series(['1', '2', '3', '4', '5']),dtype=np.int64)
   array([1 2 3 4 5])

   >>> np.random.seed(1701)
   >>> ak.from_series(pd.Series(np.random.uniform(low=0.0,high=1.0,size=3)))
   array([0.089433234324597599 0.1153776854774361 0.51874393620990389])

   >>> ak.from_series(pd.Series(['0.57600036956445599', '0.41619265571741659',
                      '0.6615356693784662']), dtype=np.float64)
   array([0.57600036956445599 0.41619265571741659 0.6615356693784662])

   >>> np.random.seed(1864)
   >>> ak.from_series(pd.Series(np.random.choice([True, False],size=5)))
   array([True True True False False])

   >>> ak.from_series(pd.Series(['True', 'False', 'False', 'True', 'True']), dtype=bool)
   array([True True True True True])

   >>> ak.from_series(pd.Series(['a', 'b', 'c', 'd', 'e'], dtype="string"))
   array(['a', 'b', 'c', 'd', 'e'])

   >>> ak.from_series(pd.Series(pd.to_datetime(['1/1/2018', np.datetime64('2018-01-01')])))
   array([1514764800000000000 1514764800000000000])

   .. rubric:: Notes

   The supported datatypes are bool, float64, int64, string, and datetime64[ns]. The
   data type is either inferred from the the Series or is set via the dtype parameter.

   Series of datetime or timedelta are converted to Arkouda arrays of dtype int64 (nanoseconds)

   A Pandas Series containing strings has a dtype of object. Arkouda assumes the Series
   contains strings and sets the dtype to str


.. py:function:: full(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], fill_value: Union[arkouda.numpy.dtypes.numeric_scalars, str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Create a pdarray filled with fill_value.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param fill_value: Value with which the array will be filled
   :type fill_value: int_scalars or str
   :param dtype: Resulting array type, default float64
   :type dtype: all_scalars
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: array of the requested size and dtype filled with fill_value
   :rtype: pdarray or Strings

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`zeros`, :obj:`ones`

   .. rubric:: Examples

   >>> ak.full(5, 7, dtype=ak.int64)
   array([7 7 7 7 7])

   >>> ak.full(5, 9, dtype=ak.float64)
   array([9.00000000000000000 9.00000000000000000 9.00000000000000000
          9.00000000000000000 9.00000000000000000])

   >>> ak.full(5, 5, dtype=ak.bool_)
   array([True True True True True])


.. py:function:: full(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], fill_value: Union[arkouda.numpy.dtypes.numeric_scalars, str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Create a pdarray filled with fill_value.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param fill_value: Value with which the array will be filled
   :type fill_value: int_scalars or str
   :param dtype: Resulting array type, default float64
   :type dtype: all_scalars
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: array of the requested size and dtype filled with fill_value
   :rtype: pdarray or Strings

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`zeros`, :obj:`ones`

   .. rubric:: Examples

   >>> ak.full(5, 7, dtype=ak.int64)
   array([7 7 7 7 7])

   >>> ak.full(5, 9, dtype=ak.float64)
   array([9.00000000000000000 9.00000000000000000 9.00000000000000000
          9.00000000000000000 9.00000000000000000])

   >>> ak.full(5, 5, dtype=ak.bool_)
   array([True True True True True])


.. py:function:: full_like(pda: arkouda.pdarrayclass.pdarray, fill_value: arkouda.numpy.dtypes.numeric_scalars) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Create a pdarray filled with fill_value of the same size and dtype as an existing
   pdarray.

   :param pda: Array to use for size and dtype
   :type pda: pdarray
   :param fill_value: Value with which the array will be filled
   :type fill_value: int_scalars

   :returns: Equivalent to ak.full(pda.size, fill_value, pda.dtype)
   :rtype: pdarray

   :raises TypeError: Raised if the pda parameter is not a pdarray.

   .. seealso:: :obj:`ones_like`, :obj:`zeros_like`

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.
   Accordingly, the supported dtypes match are defined by the ak.full method.

   .. rubric:: Examples

   >>> ak.full_like(ak.full(5,7,dtype=ak.int64),6)
   array([6 6 6 6 6])

   >>> ak.full_like(ak.full(7,9,dtype=ak.float64),10)
   array([10.00000000000000000 10.00000000000000000 10.00000000000000000
          10.00000000000000000 10.00000000000000000 10.00000000000000000 10.00000000000000000])

   >>> ak.full_like(ak.full(5,True,dtype=ak.bool_),False)
   array([False False False False False])


.. py:function:: gen_ranges(starts, ends, stride=1, return_lengths=False)

   Generate a segmented array of variable-length, contiguous ranges between pairs of
   start- and end-points.

   :param starts: The start value of each range
   :type starts: pdarray, int64
   :param ends: The end value (exclusive) of each range
   :type ends: pdarray, int64
   :param stride: Difference between successive elements of each range
   :type stride: int
   :param return_lengths: Whether or not to return the lengths of each segment. Default False.
   :type return_lengths: bool, optional

   :returns: * **segments** (*pdarray, int64*) -- The starting index of each range in the resulting array
             * **ranges** (*pdarray, int64*) -- The actual ranges, flattened into a single array
             * **lengths** (*pdarray, int64*) -- The lengths of each segment. Only returned if return_lengths=True.


.. py:function:: gen_ranges(starts, ends, stride=1, return_lengths=False)

   Generate a segmented array of variable-length, contiguous ranges between pairs of
   start- and end-points.

   :param starts: The start value of each range
   :type starts: pdarray, int64
   :param ends: The end value (exclusive) of each range
   :type ends: pdarray, int64
   :param stride: Difference between successive elements of each range
   :type stride: int
   :param return_lengths: Whether or not to return the lengths of each segment. Default False.
   :type return_lengths: bool, optional

   :returns: * **segments** (*pdarray, int64*) -- The starting index of each range in the resulting array
             * **ranges** (*pdarray, int64*) -- The actual ranges, flattened into a single array
             * **lengths** (*pdarray, int64*) -- The lengths of each segment. Only returned if return_lengths=True.


.. py:function:: generic_concat(items, ordered=True)

.. py:function:: getArkoudaLogger(name: str, handlers: Optional[List[logging.Handler]] = None, logFormat: Optional[str] = ArkoudaLogger.DEFAULT_LOG_FORMAT, logLevel: Optional[LogLevel] = None) -> ArkoudaLogger

   A convenience method for instantiating an ArkoudaLogger that retrieves the
   logging level from the ARKOUDA_LOG_LEVEL env variable

   :param name: The name of the ArkoudaLogger
   :type name: str
   :param handlers: A list of logging.Handler objects, if None, a list consisting of
                    one StreamHandler named 'console-handler' is generated and configured
   :type handlers: List[Handler]
   :param logFormat: The format for log messages, defaults to the following format:
                     '[%(name)s] Line %(lineno)d %(levelname)s: %(message)s'
   :type logFormat: str

   :rtype: ArkoudaLogger

   :raises TypeError: Raised if either name or logFormat is not a str object or if handlers
       is not a list of str objects

   .. rubric:: Notes

   Important note: if a list of 1..n logging.Handler objects is passed in, and
   dynamic changes to 1..n handlers is desired, set a name for each Handler
   object as follows: handler.name = <desired name>, which will enable retrieval
   and updates for the specified handler.


.. py:function:: get_byteorder(dt: np.dtype) -> str

   Get a concrete byteorder (turns '=' into '<' or '>')



.. py:function:: get_callback(x)

.. py:function:: get_columns(filenames: Union[str, List[str]], col_delim: str = ',', allow_errors: bool = False) -> List[str]

   Get a list of column names from CSV file(s).


.. py:function:: get_datasets(filenames: Union[str, List[str]], allow_errors: bool = False, column_delim: str = ',', read_nested: bool = True) -> List[str]

   Get the names of the datasets in the provide files

   :param filenames: Name of the file/s from which to return datasets
   :type filenames: str or List[str]
   :param allow_errors: Default: False
                        Whether or not to allow errors while accessing datasets
   :type allow_errors: bool
   :param column_delim: Column delimiter to be used if dataset is CSV. Otherwise, unused.
   :type column_delim: str
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Only used for Parquet Files.
   :type read_nested: bool

   :rtype: List[str] of names of the datasets

   :raises RuntimeError: - If no datasets are returned

   .. rubric:: Notes

   - This function currently supports HDF5 and Parquet formats.
   - Future updates to Parquet will deprecate this functionality on that format,
   but similar support will be added for Parquet at that time.
   - If a list of files is provided, only the datasets in the first file will be returned

   .. seealso:: :obj:`ls`


.. py:function:: get_filetype(filenames: Union[str, List[str]]) -> str

   Get the type of a file accessible to the server. Supported
   file types and possible return strings are 'HDF5' and 'Parquet'.

   :param filenames: A file or list of files visible to the arkouda server
   :type filenames: Union[str, List[str]]

   :returns: Type of the file returned as a string, either 'HDF5', 'Parquet' or 'CSV
   :rtype: str

   :raises ValueError: Raised if filename is empty or contains only whitespace

   .. rubric:: Notes

   - When list provided, it is assumed that all files are the same type
   - CSV Files without the Arkouda Header are not supported

   .. seealso:: :obj:`read_parquet`, :obj:`read_hdf`


.. py:function:: get_null_indices(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None) -> Union[arkouda.pdarrayclass.pdarray, Mapping[str, arkouda.pdarrayclass.pdarray]]

   Get null indices of a string column in a Parquet file.

   :param filenames: Either a list of filenames or shell expression
   :type filenames: list or str
   :param datasets: (List of) name(s) of dataset(s) to read. Each dataset must be a string
                    column. There is no default value for this function, the datasets to be
                    read must be specified.
   :type datasets: list or str or None

   :returns: Dictionary of {datasetName: pdarray}
   :rtype: returns a dictionary of Arkouda pdarrays

   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. seealso:: :obj:`get_datasets`, :obj:`ls`


.. py:function:: get_server_byteorder() -> str

   Get the server's byteorder



.. py:class:: half(value)

   Bases: :py:obj:`numpy.floating`


   Half-precision floating-point number type.

       :Character code: ``'e'``
       :Canonical name: `numpy.half`
       :Alias on this platform (Linux x86_64): `numpy.float16`: 16-bit-precision floating-point number type: sign bit, 5 bits exponent, 10 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      half.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.half(10.0).as_integer_ratio()
              (10, 1)
              >>> np.half(0.0).as_integer_ratio()
              (0, 1)
              >>> np.half(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: is_integer(*args, **kwargs)

      half.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.half(-2.0).is_integer()
              True
              >>> np.half(3.2).is_integer()
              False




.. py:function:: hash(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~SegArray, ~Categorical, List[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~SegArray, ~Categorical]]], full: bool = True) -> Union[Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray], arkouda.pdarrayclass.pdarray]

   Return an element-wise hash of the array or list of arrays.

   :param pda:
   :type pda: pdarray, Strings, SegArray, or Categorical     or List of pdarray, Strings, SegArray, or Categorical
   :param full: This is only used when a single pdarray is passed into hash
                By default, a 128-bit hash is computed and returned as
                two int64 arrays. If full=False, then a 64-bit hash
                is computed and returned as a single int64 array.
   :type full: bool, default=True

   :returns: If full=True or a list of pdarrays is passed,
             a 2-tuple of pdarrays containing the high
             and low 64 bits of each hash, respectively.
             If full=False and a single pdarray is passed,
             a single pdarray containing a 64-bit hash
   :rtype: hashes

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Notes

   In the case of a single pdarray being passed, this function
   uses the SIPhash algorithm, which can output either a 64-bit
   or 128-bit hash. However, the 64-bit hash runs a significant
   risk of collisions when applied to more than a few million
   unique values. Unless the number of unique values is known to
   be small, the 128-bit hash is strongly recommended.

   Note that this hash should not be used for security, or for
   any cryptographic application. Not only is SIPhash not
   intended for such uses, but this implementation employs a
   fixed key for the hash, which makes it possible for an
   adversary with control over input to engineer collisions.

   In the case of a list of pdrrays, Strings, Categoricals, or Segarrays
   being passed, a non-linear function must be applied to each
   array since hashes of subsequent arrays cannot be simply XORed
   because equivalent values will cancel each other out, hence we
   do a rotation by the ordinal of the array.


.. py:function:: hist_all(ak_df: arkouda.dataframe.DataFrame, cols: list = [])

   Create a grid plot histogramming all numeric columns in ak dataframe

   :param ak_df: Full Arkouda DataFrame containing data to be visualized
   :type ak_df: ak.DataFrame
   :param cols: (Optional) A specified list of columns to be plotted
   :type cols: list

   .. rubric:: Notes

   This function displays the plot.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from arkouda.plotting import hist_all
   >>> ak_df = ak.DataFrame({"a": ak.array(np.random.randn(100)),
                             "b": ak.array(np.random.randn(100)),
                             "c": ak.array(np.random.randn(100)),
                             "d": ak.array(np.random.randn(100))
                             })
   >>> hist_all(ak_df)


.. py:function:: histogram(pda: arkouda.pdarrayclass.pdarray, bins: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64] = 10) -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

   Compute a histogram of evenly spaced bins over the range of an array.

   :param pda: The values to histogram
   :type pda: pdarray
   :param bins: The number of equal-size bins to use (default: 10)
   :type bins: int_scalars, default=10

   :returns: The number of values present in each bin and the bin edges
   :rtype: (pdarray, Union[pdarray, int64 or float64])

   :raises TypeError: Raised if the parameter is not a pdarray or if bins is
       not an int.
   :raises ValueError: Raised if bins < 1
   :raises NotImplementedError: Raised if pdarray dtype is bool or uint8

   .. seealso:: :obj:`value_counts`, :obj:`histogram2d`

   .. rubric:: Notes

   The bins are evenly spaced in the interval [pda.min(), pda.max()].

   .. rubric:: Examples

   >>> import matplotlib.pyplot as plt
   >>> A = ak.arange(0, 10, 1)
   >>> nbins = 3
   >>> h, b = ak.histogram(A, bins=nbins)
   >>> h
   array([3 3 4])
   >>> b
   array([0.00000000000000000 3.00000000000000000 6.00000000000000000 9.00000000000000000])
   # To plot, export the left edges and the histogram to NumPy
   >>> b_np = b.to_ndarray()
   >>> import numpy as np
   >>> b_widths = np.diff(b_np)
   >>> plt.bar(b_np[:-1], h.to_ndarray(), width=b_widths, align='edge', edgecolor='black')
   <BarContainer object of 3 artists>
   >>> plt.show()


.. py:function:: histogram(pda: arkouda.pdarrayclass.pdarray, bins: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64] = 10) -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

   Compute a histogram of evenly spaced bins over the range of an array.

   :param pda: The values to histogram
   :type pda: pdarray
   :param bins: The number of equal-size bins to use (default: 10)
   :type bins: int_scalars, default=10

   :returns: The number of values present in each bin and the bin edges
   :rtype: (pdarray, Union[pdarray, int64 or float64])

   :raises TypeError: Raised if the parameter is not a pdarray or if bins is
       not an int.
   :raises ValueError: Raised if bins < 1
   :raises NotImplementedError: Raised if pdarray dtype is bool or uint8

   .. seealso:: :obj:`value_counts`, :obj:`histogram2d`

   .. rubric:: Notes

   The bins are evenly spaced in the interval [pda.min(), pda.max()].

   .. rubric:: Examples

   >>> import matplotlib.pyplot as plt
   >>> A = ak.arange(0, 10, 1)
   >>> nbins = 3
   >>> h, b = ak.histogram(A, bins=nbins)
   >>> h
   array([3 3 4])
   >>> b
   array([0.00000000000000000 3.00000000000000000 6.00000000000000000 9.00000000000000000])
   # To plot, export the left edges and the histogram to NumPy
   >>> b_np = b.to_ndarray()
   >>> import numpy as np
   >>> b_widths = np.diff(b_np)
   >>> plt.bar(b_np[:-1], h.to_ndarray(), width=b_widths, align='edge', edgecolor='black')
   <BarContainer object of 3 artists>
   >>> plt.show()


.. py:function:: histogram2d(x: arkouda.pdarrayclass.pdarray, y: arkouda.pdarrayclass.pdarray, bins: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, Sequence[Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64]]] = 10) -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

   Compute the bi-dimensional histogram of two data samples with evenly spaced bins

   :param x: A pdarray containing the x coordinates of the points to be histogrammed.
   :type x: pdarray
   :param y: A pdarray containing the y coordinates of the points to be histogrammed.
   :type y: pdarray
   :param bins: The number of equal-size bins to use.
                If int, the number of bins for the two dimensions (nx=ny=bins).
                If [int, int], the number of bins in each dimension (nx, ny = bins).
                Defaults to 10
   :type bins: int_scalars or [int, int], default=10

   :returns: * **hist** (*pdarray*) -- shape(nx, ny)
               The bi-dimensional histogram of samples x and y.
               Values in x are histogrammed along the first dimension and
               values in y are histogrammed along the second dimension.
             * **x_edges** (*pdarray*) -- The bin edges along the first dimension.
             * **y_edges** (*pdarray*) -- The bin edges along the second dimension.

   :raises TypeError: Raised if x or y parameters are not pdarrays or if bins is
       not an int or (int, int).
   :raises ValueError: Raised if bins < 1
   :raises NotImplementedError: Raised if pdarray dtype is bool or uint8

   .. seealso:: :obj:`histogram`

   .. rubric:: Notes

   The x bins are evenly spaced in the interval [x.min(), x.max()]
   and y bins are evenly spaced in the interval [y.min(), y.max()].

   .. rubric:: Examples

   >>> x = ak.arange(0, 10, 1)
   >>> y = ak.arange(9, -1, -1)
   >>> nbins = 3
   >>> h, x_edges, y_edges = ak.histogram2d(x, y, bins=nbins)
   >>> h
   array([array([0.00000000000000000 0.00000000000000000 3.00000000000000000])
          array([0.00000000000000000 2.00000000000000000 1.00000000000000000])
          array([3.00000000000000000 1.00000000000000000 0.00000000000000000])])
   >>> x_edges
   array([0.00000000000000000 3.00000000000000000 6.00000000000000000 9.00000000000000000])
   >>> y_edges
   array([0.00000000000000000 3.00000000000000000 6.00000000000000000 9.00000000000000000])


.. py:function:: histogramdd(sample: Sequence[arkouda.pdarrayclass.pdarray], bins: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, Sequence[Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64]]] = 10) -> Tuple[arkouda.pdarrayclass.pdarray, Sequence[arkouda.pdarrayclass.pdarray]]

   Compute the multidimensional histogram of data in sample with evenly spaced bins.

   :param sample: A sequence of pdarrays containing the coordinates of the points to be histogrammed.
   :type sample: Sequence of pdarray
   :param bins: The number of equal-size bins to use.
                If int, the number of bins for all dimensions (nx=ny=...=bins).
                If [int, int, ...], the number of bins in each dimension (nx, ny, ... = bins).
                Defaults to 10
   :type bins: int_scalars or Sequence of int_scalars, default=10

   :returns: * **hist** (*pdarray*) -- shape(nx, ny, ..., nd)
               The multidimensional histogram of pdarrays in sample.
               Values in first pdarray are histogrammed along the first dimension.
               Values in second pdarray are histogrammed along the second dimension and so on.
             * **edges** (*List[pdarray]*) -- A list of pdarrays containing the bin edges for each dimension.

   :raises ValueError: Raised if bins < 1
   :raises NotImplementedError: Raised if pdarray dtype is bool or uint8

   .. seealso:: :obj:`histogram`

   .. rubric:: Notes

   The bins for each dimension, m, are evenly spaced in the interval [m.min(), m.max()]

   .. rubric:: Examples

   >>> x = ak.arange(0, 10, 1)
   >>> y = ak.arange(9, -1, -1)
   >>> z = ak.where(x % 2 == 0, x, y)
   >>> h, edges = ak.histogramdd((x, y,z), bins=(2,2,5))
   >>> h
   array([array([array([0 0 0 0 0])
          array([1 1 1 1 1])])
          array([array([1 1 1 1 1])
          array([0 0 0 0 0])])])
   >>> edges
   [array([0.00000000000000000 4.5 9.00000000000000000]),
   array([0.00000000000000000 4.5 9.00000000000000000]),
   array([0.00000000000000000 1.6000000000000001 3.2000000000000002
   4.8000000000000007 6.4000000000000004 8.00000000000000000])]


.. py:class:: iinfo

   iinfo(type)

   Machine limits for integer types.

   .. attribute:: bits

      The number of bits occupied by the type.

      :type: int

   .. attribute:: dtype

      Returns the dtype for which `iinfo` returns information.

      :type: dtype

   .. attribute:: min

      The smallest integer expressible by the type.

      :type: int

   .. attribute:: max

      The largest integer expressible by the type.

      :type: int

   :param int_type: The kind of integer data type to get information about.
   :type int_type: integer type, dtype, or instance

   .. seealso::

      :obj:`finfo`
          The equivalent for floating point data types.

   .. rubric:: Examples

   With types:

   >>> ii16 = np.iinfo(np.int16)
   >>> ii16.min
   -32768
   >>> ii16.max
   32767
   >>> ii32 = np.iinfo(np.int32)
   >>> ii32.min
   -2147483648
   >>> ii32.max
   2147483647

   With instances:

   >>> ii32 = np.iinfo(np.int32(10))
   >>> ii32.min
   -2147483648
   >>> ii32.max
   2147483647


   .. py:property:: max

      Maximum value of given dtype.


   .. py:property:: min

      Minimum value of given dtype.


.. py:function:: import_data(read_path: str, write_file: Optional[str] = None, return_obj: bool = True, index: bool = False)

   Import data from a file saved by Pandas (HDF5/Parquet) to Arkouda object and/or
   a file formatted to be read by Arkouda.

   :param read_path: path to file where pandas data is stored. This can be glob expression for parquet formats.
   :type read_path: str
   :param write_file: path to file to write arkouda formatted data to. Only write file if provided
   :type write_file: str, optional
   :param return_obj: Default True. When True return the Arkouda DataFrame object, otherwise return None
   :type return_obj: bool, optional
   :param index: Default False. When True, maintain the indexes loaded from the pandas file
   :type index: bool, optional

   :raises RuntimeWarning: - Export attempted on Parquet file. Arkouda formatted Parquet files are readable by pandas.
   :raises RuntimeError: - Unsupported file type

   :returns: When `return_obj=True`
   :rtype: pd.DataFrame

   .. seealso:: :obj:`pandas.DataFrame.to_parquet`, :obj:`pandas.DataFrame.to_hdf`, :obj:`pandas.DataFrame.read_parquet`, :obj:`pandas.DataFrame.read_hdf`, :obj:`ak.export`

   .. rubric:: Notes

   - Import can only be performed from hdf5 or parquet files written by pandas.


.. py:function:: in1d(pda1: arkouda.groupbyclass.groupable, pda2: arkouda.groupbyclass.groupable, assume_unique: bool = False, symmetric: bool = False, invert: bool = False) -> arkouda.groupbyclass.groupable

   Test whether each element of a 1-D array is also present in a second array.

   Returns a boolean array the same length as `pda1` that is True
   where an element of `pda1` is in `pda2` and False otherwise.

   Support multi-level -- test membership of rows of a in the set of rows of b.

   :param a: Rows are elements for which to test membership in b
   :type a: list of pdarrays, pdarray, Strings, or Categorical
   :param b: Rows are elements of the set in which to test membership
   :type b: list of pdarrays, pdarray, Strings, or Categorical
   :param assume_unique: If true, assume rows of a and b are each unique and sorted.
                         By default, sort and unique them explicitly.
   :type assume_unique: bool
   :param symmetric: Return in1d(pda1, pda2), in1d(pda2, pda1) when pda1 and 2 are single items.
   :type symmetric: bool
   :param invert: If True, the values in the returned array are inverted (that is,
                  False where an element of `pda1` is in `pda2` and True otherwise).
                  Default is False. ``ak.in1d(a, b, invert=True)`` is equivalent
                  to (but is faster than) ``~ak.in1d(a, b)``.
   :type invert: bool, optional

   :returns: * True for each row in a that is contained in b
             * *Return Type*
             * *------------* -- pdarray, bool

   .. rubric:: Notes

   Only works for pdarrays of int64 dtype, float64, Strings, or Categorical


.. py:function:: in1d(pda1: arkouda.groupbyclass.groupable, pda2: arkouda.groupbyclass.groupable, assume_unique: bool = False, symmetric: bool = False, invert: bool = False) -> arkouda.groupbyclass.groupable

   Test whether each element of a 1-D array is also present in a second array.

   Returns a boolean array the same length as `pda1` that is True
   where an element of `pda1` is in `pda2` and False otherwise.

   Support multi-level -- test membership of rows of a in the set of rows of b.

   :param a: Rows are elements for which to test membership in b
   :type a: list of pdarrays, pdarray, Strings, or Categorical
   :param b: Rows are elements of the set in which to test membership
   :type b: list of pdarrays, pdarray, Strings, or Categorical
   :param assume_unique: If true, assume rows of a and b are each unique and sorted.
                         By default, sort and unique them explicitly.
   :type assume_unique: bool
   :param symmetric: Return in1d(pda1, pda2), in1d(pda2, pda1) when pda1 and 2 are single items.
   :type symmetric: bool
   :param invert: If True, the values in the returned array are inverted (that is,
                  False where an element of `pda1` is in `pda2` and True otherwise).
                  Default is False. ``ak.in1d(a, b, invert=True)`` is equivalent
                  to (but is faster than) ``~ak.in1d(a, b)``.
   :type invert: bool, optional

   :returns: * True for each row in a that is contained in b
             * *Return Type*
             * *------------* -- pdarray, bool

   .. rubric:: Notes

   Only works for pdarrays of int64 dtype, float64, Strings, or Categorical


.. py:function:: in1d(pda1: arkouda.groupbyclass.groupable, pda2: arkouda.groupbyclass.groupable, assume_unique: bool = False, symmetric: bool = False, invert: bool = False) -> arkouda.groupbyclass.groupable

   Test whether each element of a 1-D array is also present in a second array.

   Returns a boolean array the same length as `pda1` that is True
   where an element of `pda1` is in `pda2` and False otherwise.

   Support multi-level -- test membership of rows of a in the set of rows of b.

   :param a: Rows are elements for which to test membership in b
   :type a: list of pdarrays, pdarray, Strings, or Categorical
   :param b: Rows are elements of the set in which to test membership
   :type b: list of pdarrays, pdarray, Strings, or Categorical
   :param assume_unique: If true, assume rows of a and b are each unique and sorted.
                         By default, sort and unique them explicitly.
   :type assume_unique: bool
   :param symmetric: Return in1d(pda1, pda2), in1d(pda2, pda1) when pda1 and 2 are single items.
   :type symmetric: bool
   :param invert: If True, the values in the returned array are inverted (that is,
                  False where an element of `pda1` is in `pda2` and True otherwise).
                  Default is False. ``ak.in1d(a, b, invert=True)`` is equivalent
                  to (but is faster than) ``~ak.in1d(a, b)``.
   :type invert: bool, optional

   :returns: * True for each row in a that is contained in b
             * *Return Type*
             * *------------* -- pdarray, bool

   .. rubric:: Notes

   Only works for pdarrays of int64 dtype, float64, Strings, or Categorical


.. py:function:: in1d_intervals(vals, intervals, symmetric=False)

   Test each value for membership in *any* of a set of half-open (pythonic)
   intervals.

   :param vals: Values to test for membership in intervals
   :type vals: pdarray(int, float)
   :param intervals: Non-overlapping, half-open intervals, as a tuple of
                     (lower_bounds_inclusive, upper_bounds_exclusive)
   :type intervals: 2-tuple of pdarrays
   :param symmetric: If True, also return boolean pdarray indicating which intervals
                     contained one or more query values.
   :type symmetric: bool

   :returns: * *pdarray(bool)* -- Array of same length as <vals>, True if corresponding value is
               included in any of the ranges defined by (low[i], high[i]) inclusive.
             * *pdarray(bool) (if symmetric=True)* -- Array of same length as number of intervals, True if corresponding
               interval contains any of the values in <vals>.

   .. rubric:: Notes

   First return array is equivalent to the following:
       ((vals >= intervals[0][0]) & (vals < intervals[1][0])) |
       ((vals >= intervals[0][1]) & (vals < intervals[1][1])) |
       ...
       ((vals >= intervals[0][-1]) & (vals < intervals[1][-1]))
   But much faster when testing many ranges.

   Second (optional) return array is equivalent to:
       ((intervals[0] <= vals[0]) & (intervals[1] > vals[0])) |
       ((intervals[0] <= vals[1]) & (intervals[1] > vals[1])) |
       ...
       ((intervals[0] <= vals[-1]) & (intervals[1] > vals[-1]))
   But much faster when vals is non-trivial size.


.. py:function:: indexof1d(query: arkouda.groupbyclass.groupable, space: arkouda.groupbyclass.groupable) -> arkouda.pdarrayclass.pdarray

   Return indices of query items in a search list of items. Items not found will be excluded.
   When duplicate terms are present in search space return indices of all occurrences.

   :param query: The items to search for. If multiple arrays, each "row" is an item.
   :type query: (sequence of) pdarray or Strings or Categorical
   :param space: The set of items in which to search. Must have same shape/dtype as query.
   :type space: (sequence of) pdarray or Strings or Categorical

   :returns: **indices** -- For each item in query, its index in space.
   :rtype: pdarray, int64

   .. rubric:: Notes

   This is an alias of
   `ak.find(query, space, all_occurrences=True, remove_missing=True).values`

   .. rubric:: Examples

   >>> select_from = ak.arange(10)
   >>> arr1 = select_from[ak.randint(0, select_from.size, 20, seed=10)]
   >>> arr2 = select_from[ak.randint(0, select_from.size, 20, seed=11)]
   # remove some values to ensure we have some values
   # which don't appear in the search space
   >>> arr2 = arr2[arr2 != 9]
   >>> arr2 = arr2[arr2 != 3]

   >>> ak.indexof1d(arr1, arr2)
   array([0 4 1 3 10 2 6 12 13 5 7 8 9 14 5 7 11 15 5 7 0 4])

   :raises TypeError: Raised if either `keys` or `arr` is not a pdarray, Strings, or
       Categorical object
   :raises RuntimeError: Raised if the dtype of either array is not supported


.. py:class:: inexact(value)

   Bases: :py:obj:`numpy.number`


   Abstract base class of all numeric scalar types with a (potentially)
       inexact representation of the values in its range, such as
       floating-point numbers.



.. py:data:: inf
   :type:  float

.. py:function:: information(names: Union[List[str], str] = RegisteredSymbols) -> str

   Returns JSON formatted string containing information about the objects in names

   :param names: names is either the name of an object or list of names of objects to retrieve info
                 if names is ak.AllSymbols, retrieves info for all symbols in the symbol table
                 if names is ak.RegisteredSymbols, retrieves info for all symbols in the registry
   :type names: Union[List[str], str]

   :returns: JSON formatted string containing a list of information for each object in names
   :rtype: str

   :raises RuntimeError: Raised if a server-side error is thrown in the process of
       retrieving information about the objects in names


.. py:data:: infty
   :type:  float

.. py:class:: int16(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with C ``short``.

       :Character code: ``'h'``
       :Canonical name: `numpy.short`
       :Alias on this platform (Linux x86_64): `numpy.int16`: 16-bit signed integer (``-32_768`` to ``32_767``).



   .. py:method:: bit_count(*args, **kwargs)

      int16.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int16(127).bit_count()
              7
              >>> np.int16(-127).bit_count()
              7




.. py:class:: int32(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with C ``int``.

       :Character code: ``'i'``
       :Canonical name: `numpy.intc`
       :Alias on this platform (Linux x86_64): `numpy.int32`: 32-bit signed integer (``-2_147_483_648`` to ``2_147_483_647``).



   .. py:method:: bit_count(*args, **kwargs)

      int32.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int32(127).bit_count()
              7
              >>> np.int32(-127).bit_count()
              7




.. py:class:: int64(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with Python `int` and C ``long``.

       :Character code: ``'l'``
       :Canonical name: `numpy.int_`
       :Alias on this platform (Linux x86_64): `numpy.int64`: 64-bit signed integer (``-9_223_372_036_854_775_808`` to ``9_223_372_036_854_775_807``).
       :Alias on this platform (Linux x86_64): `numpy.intp`: Signed integer large enough to fit pointer, compatible with C ``intptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      int64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int64(127).bit_count()
              7
              >>> np.int64(-127).bit_count()
              7




.. py:class:: int64(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with Python `int` and C ``long``.

       :Character code: ``'l'``
       :Canonical name: `numpy.int_`
       :Alias on this platform (Linux x86_64): `numpy.int64`: 64-bit signed integer (``-9_223_372_036_854_775_808`` to ``9_223_372_036_854_775_807``).
       :Alias on this platform (Linux x86_64): `numpy.intp`: Signed integer large enough to fit pointer, compatible with C ``intptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      int64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int64(127).bit_count()
              7
              >>> np.int64(-127).bit_count()
              7




.. py:class:: int8(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with C ``char``.

       :Character code: ``'b'``
       :Canonical name: `numpy.byte`
       :Alias on this platform (Linux x86_64): `numpy.int8`: 8-bit signed integer (``-128`` to ``127``).



   .. py:method:: bit_count(*args, **kwargs)

      int8.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int8(127).bit_count()
              7
              >>> np.int8(-127).bit_count()
              7




.. py:class:: intTypes

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: intTypes

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: intTypes

   frozenset() -> empty frozenset object
   frozenset(iterable) -> frozenset object

   Build an immutable unordered collection of unique elements.



   .. py:method:: copy(*args, **kwargs)

      Return a shallow copy of a set.




   .. py:method:: difference(*args, **kwargs)

      Return the difference of two or more sets as a new set.

      (i.e. all elements that are in this set but not the others.)




   .. py:method:: intersection(*args, **kwargs)

      Return the intersection of two sets as a new set.

      (i.e. all elements that are in both sets.)




   .. py:method:: isdisjoint(*args, **kwargs)

      Return True if two sets have a null intersection.




   .. py:method:: issubset(*args, **kwargs)

      Report whether another set contains this set.




   .. py:method:: issuperset(*args, **kwargs)

      Report whether this set contains another set.




   .. py:method:: symmetric_difference(*args, **kwargs)

      Return the symmetric difference of two sets as a new set.

      (i.e. all elements that are in exactly one of the sets.)




   .. py:method:: union(*args, **kwargs)

      Return the union of sets as a new set.

      (i.e. all elements that are in either set.)




.. py:class:: int_(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with Python `int` and C ``long``.

       :Character code: ``'l'``
       :Canonical name: `numpy.int_`
       :Alias on this platform (Linux x86_64): `numpy.int64`: 64-bit signed integer (``-9_223_372_036_854_775_808`` to ``9_223_372_036_854_775_807``).
       :Alias on this platform (Linux x86_64): `numpy.intp`: Signed integer large enough to fit pointer, compatible with C ``intptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      int64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int64(127).bit_count()
              7
              >>> np.int64(-127).bit_count()
              7




.. py:class:: int_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:class:: int_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:class:: int_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:class:: intc(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with C ``int``.

       :Character code: ``'i'``
       :Canonical name: `numpy.intc`
       :Alias on this platform (Linux x86_64): `numpy.int32`: 32-bit signed integer (``-2_147_483_648`` to ``2_147_483_647``).



   .. py:method:: bit_count(*args, **kwargs)

      int32.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int32(127).bit_count()
              7
              >>> np.int32(-127).bit_count()
              7




.. py:class:: integer(value)

   Bases: :py:obj:`numpy.number`


   Abstract base class of all integer scalar types.



   .. py:method:: denominator(*args, **kwargs)

      denominator of value (1)




   .. py:method:: is_integer(*args, **kwargs)

      integer.is_integer() -> bool

          Return ``True`` if the number is finite with integral value.

          .. versionadded:: 1.22

          Examples
          --------
          >>> np.int64(-2).is_integer()
          True
          >>> np.uint32(5).is_integer()
          True




   .. py:method:: numerator(*args, **kwargs)

      numerator of value (the value itself)




.. py:function:: intersect(a, b, positions=True, unique=False)

   Find the intersection of two arkouda arrays.

   This function can be especially useful when `positions=True` so
   that the caller gets the indices of values present in both arrays.

   :param a: An array of strings.
   :type a: Strings or pdarray
   :param b: An array of strings.
   :type b: Strings or pdarray
   :param positions: Return tuple of boolean pdarrays that indicate positions in `a` and `b`
                     of the intersection values.
   :type positions: bool, default=True
   :param unique: If the number of distinct values in `a` (and `b`) is equal to the size of
                  `a` (and `b`), there is a more efficient method to compute the intersection.
   :type unique: bool, default=False

   :returns: The indices of `a` and `b` where any element occurs at least once in both
             arrays.
   :rtype: (arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray) or arkouda.pdarrayclass.pdarray

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.connect()
   >>> a = ak.arange(10)
   >>> print(a)
   [0 1 2 3 4 5 6 7 8 9]

   >>> b = 2 * ak.arange(10)
   >>> print(b)
   [0 2 4 6 8 10 12 14 16 18]

   >>> intersect(a,b, positions=True)
   (array([True False True False True False True False True False]),
   array([True True True True True False False False False False]))

   >>> intersect(a,b, positions=False)
   array([0 2 4 6 8])


.. py:function:: intersect1d(pda1: arkouda.groupbyclass.groupable, pda2: arkouda.groupbyclass.groupable, assume_unique: bool = False) -> Union[arkouda.pdarrayclass.pdarray, arkouda.groupbyclass.groupable]

   Find the intersection of two arrays.

   Return the sorted, unique values that are in both of the input arrays.

   :param pda1: Input array/Sequence of groupable objects
   :type pda1: pdarray/Sequence[pdarray, Strings, Categorical]
   :param pda2: Input array/sequence of groupable objects
   :type pda2: pdarray/List
   :param assume_unique: If True, the input arrays are both assumed to be unique, which
                         can speed up the calculation.  Default is False.
   :type assume_unique: bool

   :returns: Sorted 1D array/List of sorted pdarrays of common and unique elements.
   :rtype: pdarray/groupable

   :raises TypeError: Raised if either pda1 or pda2 is not a pdarray
   :raises RuntimeError: Raised if the dtype of either pdarray is not supported

   .. seealso:: :obj:`arkouda.groupbyclass.unique`, :obj:`union1d`

   .. rubric:: Notes

   ak.intersect1d is not supported for bool or float64 pdarrays

   .. rubric:: Examples

   >>>
   # 1D Example
   >>> ak.intersect1d([1, 3, 4, 3], [3, 1, 2, 1])
   array([1, 3])
   # Multi-Array Example
   >>> a = ak.arange(5)
   >>> b = ak.array([1, 5, 3, 4, 2])
   >>> c = ak.array([1, 4, 3, 2, 5])
   >>> d = ak.array([1, 2, 3, 5, 4])
   >>> multia = [a, a, a]
   >>> multib = [b, c, d]
   >>> ak.intersect1d(multia, multib)
   [array([1, 3]), array([1, 3]), array([1, 3])]


.. py:function:: interval_lookup(keys, values, arguments, fillvalue=-1, tiebreak=None, hierarchical=False)

   Apply a function defined over intervals to an array of arguments.

   :param keys: Tuple of closed intervals expressed as (lower_bounds_inclusive, upper_bounds_inclusive).
                Must have same dtype(s) as vals.
   :type keys: 2-tuple of (sequences of) pdarrays
   :param values: Function value to return for each entry in keys.
   :type values: pdarray
   :param arguments: Values to search for in intervals. If multiple arrays, each "row" is an item.
   :type arguments: (sequences of) pdarray
   :param fillvalue: Default value to return when argument is not in any interval.
   :type fillvalue: scalar
   :param tiebreak: When an argument is present in more than one key interval, the interval with the
                    lowest tiebreak value will be chosen. If no tiebreak is given, the
                    first valid key interval will be chosen.
   :type tiebreak: (optional) pdarray, numeric

   :returns: Value of function corresponding to the keys interval
             containing each argument, or fillvalue if argument not
             in any interval.
   :rtype: pdarray


.. py:class:: intp(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with Python `int` and C ``long``.

       :Character code: ``'l'``
       :Canonical name: `numpy.int_`
       :Alias on this platform (Linux x86_64): `numpy.int64`: 64-bit signed integer (``-9_223_372_036_854_775_808`` to ``9_223_372_036_854_775_807``).
       :Alias on this platform (Linux x86_64): `numpy.intp`: Signed integer large enough to fit pointer, compatible with C ``intptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      int64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int64(127).bit_count()
              7
              >>> np.int64(-127).bit_count()
              7




.. py:function:: intx(a, b)

   Find all the rows that are in both dataframes.
   Columns should be in identical order.

   Note: does not work for columns of floating point values, but does work for
   Strings, pdarrays of int64 type, and Categorical *should* work.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.connect()
   >>> a = ak.DataFrame({'a':ak.arange(5),'b': 2* ak.arange(5)})
   >>> display(a)

   +----+-----+-----+
   |    |   a |   b |
   +====+=====+=====+
   |  0 |   0 |   0 |
   +----+-----+-----+
   |  1 |   1 |   2 |
   +----+-----+-----+
   |  2 |   2 |   4 |
   +----+-----+-----+
   |  3 |   3 |   6 |
   +----+-----+-----+
   |  4 |   4 |   8 |
   +----+-----+-----+

   >>> b = ak.DataFrame({'a':ak.arange(5),'b':ak.array([0,3,4,7,8])})
   >>> display(b)

   +----+-----+-----+
   |    |   a |   b |
   +====+=====+=====+
   |  0 |   0 |   0 |
   +----+-----+-----+
   |  1 |   1 |   3 |
   +----+-----+-----+
   |  2 |   2 |   4 |
   +----+-----+-----+
   |  3 |   3 |   7 |
   +----+-----+-----+
   |  4 |   4 |   8 |
   +----+-----+-----+

   >>> intx(a,b)
   >>> intersect_df = a[intx(a,b)]
   >>> display(intersect_df)

   +----+-----+-----+
   |    |   a |   b |
   +====+=====+=====+
   |  0 |   0 |   0 |
   +----+-----+-----+
   |  1 |   2 |   4 |
   +----+-----+-----+
   |  2 |   4 |   8 |
   +----+-----+-----+


.. py:function:: invert_permutation(perm)

   Find the inverse of a permutation array.

   :param perm: The permutation array.
   :type perm: pdarray

   :returns: The inverse of the permutation array.
   :rtype: arkouda.pdarrayclass.pdarray

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.connect()
   >>> from arkouda.index import Index
   >>> i = Index(ak.array([1,2,0,5,4]))
   >>> perm = i.argsort()
   >>> print(perm)
   [2 0 1 4 3]
   >>> invert_permutation(perm)
   array([1 2 0 4 3])


.. py:function:: ip_address(values)

   Convert values to an Arkouda array of IP addresses.

   :param values: The integer IP addresses or IPv4 object.
   :type values: list-like, integer pdarray, or IPv4

   :returns: The same IP addresses as an Arkouda array
   :rtype: IPv4

   .. rubric:: Notes

   This helper is intended to help future proof changes made to
   accomodate IPv6 and to prevent errors if a user inadvertently
   casts a IPv4 instead of a int64 pdarray. It can also be used
   for importing Python lists of IP addresses into Arkouda.


.. py:function:: isSupportedBool(num)

.. py:function:: isSupportedDType(scalar)

.. py:function:: isSupportedFloat(num)

.. py:function:: isSupportedInt(num)

.. py:function:: isSupportedInt(num)

.. py:function:: isSupportedInt(num)

.. py:function:: isSupportedInt(num)

.. py:function:: isSupportedNumber(num)

.. py:function:: is_cosorted(arrays)

   Return True iff the arrays are cosorted, i.e., if the arrays were columns in a table
   then the rows are sorted.

   :param arrays: Arrays to check for cosortedness
   :type arrays: list-like of pdarrays

   :returns: True iff arrays are cosorted.
   :rtype: bool

   :raises ValueError: Raised if arrays are not the same length
   :raises TypeError: Raised if arrays is not a list-like of pdarrays


.. py:function:: is_ipv4(ip: Union[arkouda.pdarrayclass.pdarray, IPv4], ip2: Optional[arkouda.pdarrayclass.pdarray] = None) -> arkouda.pdarrayclass.pdarray

   Indicate which values are ipv4 when passed data containing IPv4 and IPv6 values.

   :param ip:
   :type ip: pdarray (int64) or ak.IPv4
   :param IPv4 value. High Bits of IPv6 if IPv6 is passed in.:
   :param ip2:
   :type ip2: pdarray (int64), Optional
   :param Low Bits of IPv6. This is added for support when dealing with data that contains IPv6 as well.:

   :rtype: pdarray of bools indicating which indexes are IPv4.

   .. seealso:: :obj:`ak.is_ipv6`


.. py:function:: is_ipv6(ip: Union[arkouda.pdarrayclass.pdarray, IPv4], ip2: Optional[arkouda.pdarrayclass.pdarray] = None) -> arkouda.pdarrayclass.pdarray

   Indicate which values are ipv6 when passed data containing IPv4 and IPv6 values.

   :param ip:
   :type ip: pdarray (int64) or ak.IPv4
   :param High Bits of IPv6.:
   :param ip2:
   :type ip2: pdarray (int64), Optional
   :param Low Bits of IPv6:

   :rtype: pdarray of bools indicating which indexes are IPv6.

   .. seealso:: :obj:`ak.is_ipv4`


.. py:function:: is_registered(name: str, as_component: bool = False) -> bool

   Determine if the name provided is associated with a registered Object

   :param name: The name to check for in the registry
   :type name: str
   :param as_component: Default: False
                        When True, the name will be checked to determine if it is registered as a component of
                        a registered object
   :type as_component: bool

   :rtype: bool


.. py:function:: isfinite(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise isfinite check applied to the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing boolean values indicating whether the
             input array elements are finite
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray
   :raises RuntimeError: if the underlying pdarray is not float-based

   .. rubric:: Examples

   >>> ak.isfinite(ak.array([1.0, 2.0, ak.inf]))
   array([True True False])


.. py:function:: isinf(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise isinf check applied to the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing boolean values indicating whether the
             input array elements are infinite (positive or negative)
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray
   :raises RuntimeError: if the underlying pdarray is not float-based

   .. rubric:: Examples

   >>> ak.isinf(ak.array([1.0, 2.0, ak.inf]))
   array([False False True])


.. py:function:: isnan(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise isnan check applied to the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing boolean values indicating whether the
             input array elements are NaN
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray
   :raises RuntimeError: if the underlying pdarray is not float-based

   .. rubric:: Examples

   >>> ak.isnan(ak.array([1.0, 2.0, np.log(-1)]))
   array([False False True])


.. py:function:: isnan(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise isnan check applied to the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing boolean values indicating whether the
             input array elements are NaN
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray
   :raises RuntimeError: if the underlying pdarray is not float-based

   .. rubric:: Examples

   >>> ak.isnan(ak.array([1.0, 2.0, np.log(-1)]))
   array([False False True])


.. py:function:: isscalar(element)

   Returns True if the type of `element` is a scalar type.

   :param element: Input argument, can be of any type and shape.
   :type element: any

   :returns: **val** -- True if `element` is a scalar type, False if it is not.
   :rtype: bool

   .. seealso::

      :obj:`ndim`
          Get the number of dimensions of an array

   .. rubric:: Notes

   If you need a stricter way to identify a *numerical* scalar, use
   ``isinstance(x, numbers.Number)``, as that returns ``False`` for most
   non-numerical elements such as strings.

   In most cases ``np.ndim(x) == 0`` should be used instead of this function,
   as that will also return true for 0d arrays. This is how numpy overloads
   functions in the style of the ``dx`` arguments to `gradient` and the ``bins``
   argument to `histogram`. Some key differences:

   +--------------------------------------+---------------+-------------------+
   | x                                    |``isscalar(x)``|``np.ndim(x) == 0``|
   +======================================+===============+===================+
   | PEP 3141 numeric objects (including  | ``True``      | ``True``          |
   | builtins)                            |               |                   |
   +--------------------------------------+---------------+-------------------+
   | builtin string and buffer objects    | ``True``      | ``True``          |
   +--------------------------------------+---------------+-------------------+
   | other builtin objects, like          | ``False``     | ``True``          |
   | `pathlib.Path`, `Exception`,         |               |                   |
   | the result of `re.compile`           |               |                   |
   +--------------------------------------+---------------+-------------------+
   | third-party objects like             | ``False``     | ``True``          |
   | `matplotlib.figure.Figure`           |               |                   |
   +--------------------------------------+---------------+-------------------+
   | zero-dimensional numpy arrays        | ``False``     | ``True``          |
   +--------------------------------------+---------------+-------------------+
   | other numpy arrays                   | ``False``     | ``False``         |
   +--------------------------------------+---------------+-------------------+
   | `list`, `tuple`, and other sequence  | ``False``     | ``False``         |
   | objects                              |               |                   |
   +--------------------------------------+---------------+-------------------+

   .. rubric:: Examples

   >>> np.isscalar(3.1)
   True
   >>> np.isscalar(np.array(3.1))
   False
   >>> np.isscalar([3.1])
   False
   >>> np.isscalar(False)
   True
   >>> np.isscalar('numpy')
   True

   NumPy supports PEP 3141 numbers:

   >>> from fractions import Fraction
   >>> np.isscalar(Fraction(5, 17))
   True
   >>> from numbers import Number
   >>> np.isscalar(Number())
   True


.. py:function:: issctype(rep)

   Determines whether the given object represents a scalar data-type.

   :param rep: If `rep` is an instance of a scalar dtype, True is returned. If not,
               False is returned.
   :type rep: any

   :returns: **out** -- Boolean result of check whether `rep` is a scalar dtype.
   :rtype: bool

   .. seealso:: :obj:`issubsctype`, :obj:`issubdtype`, :obj:`obj2sctype`, :obj:`sctype2char`

   .. rubric:: Examples

   >>> np.issctype(np.int32)
   True
   >>> np.issctype(list)
   False
   >>> np.issctype(1.1)
   False

   Strings are also a scalar type:

   >>> np.issctype(np.dtype('str'))
   True


.. py:function:: issubclass_(arg1, arg2)

   Determine if a class is a subclass of a second class.

   `issubclass_` is equivalent to the Python built-in ``issubclass``,
   except that it returns False instead of raising a TypeError if one
   of the arguments is not a class.

   :param arg1: Input class. True is returned if `arg1` is a subclass of `arg2`.
   :type arg1: class
   :param arg2: Input class. If a tuple of classes, True is returned if `arg1` is a
                subclass of any of the tuple elements.
   :type arg2: class or tuple of classes.

   :returns: **out** -- Whether `arg1` is a subclass of `arg2` or not.
   :rtype: bool

   .. seealso:: :obj:`issubsctype`, :obj:`issubdtype`, :obj:`issctype`

   .. rubric:: Examples

   >>> np.issubclass_(np.int32, int)
   False
   >>> np.issubclass_(np.int32, float)
   False
   >>> np.issubclass_(np.float64, float)
   True


.. py:function:: issubdtype(arg1, arg2)

   Returns True if first argument is a typecode lower/equal in type hierarchy.

   This is like the builtin :func:`issubclass`, but for `dtype`\ s.

   :param arg1: `dtype` or object coercible to one
   :type arg1: dtype_like
   :param arg2: `dtype` or object coercible to one
   :type arg2: dtype_like

   :returns: **out**
   :rtype: bool

   .. seealso::

      :ref:`arrays.scalars`
          Overview of the numpy type hierarchy.

      :obj:`issubsctype`, :obj:`issubclass_`

   .. rubric:: Examples

   `issubdtype` can be used to check the type of arrays:

   >>> ints = np.array([1, 2, 3], dtype=np.int32)
   >>> np.issubdtype(ints.dtype, np.integer)
   True
   >>> np.issubdtype(ints.dtype, np.floating)
   False

   >>> floats = np.array([1, 2, 3], dtype=np.float32)
   >>> np.issubdtype(floats.dtype, np.integer)
   False
   >>> np.issubdtype(floats.dtype, np.floating)
   True

   Similar types of different sizes are not subdtypes of each other:

   >>> np.issubdtype(np.float64, np.float32)
   False
   >>> np.issubdtype(np.float32, np.float64)
   False

   but both are subtypes of `floating`:

   >>> np.issubdtype(np.float64, np.floating)
   True
   >>> np.issubdtype(np.float32, np.floating)
   True

   For convenience, dtype-like objects are allowed too:

   >>> np.issubdtype('S1', np.string_)
   True
   >>> np.issubdtype('i4', np.signedinteger)
   True


.. py:function:: join_on_eq_with_dt(a1: arkouda.pdarrayclass.pdarray, a2: arkouda.pdarrayclass.pdarray, t1: arkouda.pdarrayclass.pdarray, t2: arkouda.pdarrayclass.pdarray, dt: Union[int, numpy.int64], pred: str, result_limit: Union[int, numpy.int64] = 1000) -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

   Performs an inner-join on equality between two integer arrays where
   the time-window predicate is also true

   :param a1: pdarray to be joined
   :type a1: pdarray, int64
   :param a2: pdarray to be joined
   :type a2: pdarray, int64
   :param t1: timestamps in millis corresponding to the a1 pdarray
   :type t1: pdarray
   :param t2: timestamps in millis corresponding to the a2 pdarray
   :type t2: pdarray
   :param dt: time delta
   :type dt: Union[int,np.int64]
   :param pred: time window predicate
   :type pred: str
   :param result_limit: size limit for returned result
   :type result_limit: Union[int,np.int64]

   :returns: * **result_array_one** (*pdarray, int64*) -- a1 indices where a1 == a2
             * **result_array_one** (*pdarray, int64*) -- a2 indices where a2 == a1

   :raises TypeError: Raised if a1, a2, t1, or t2 is not a pdarray, or if dt or
       result_limit is not an int
   :raises ValueError: if a1, a2, t1, or t2 dtype is not int64, pred is not
       'true_dt', 'abs_dt', or 'pos_dt', or result_limit is < 0


.. py:function:: left_align(left, right)

   Map two arrays of sparse identifiers to the 0-up index set implied by the left array,
   discarding values from right that do not appear in left.


.. py:function:: linspace(start: arkouda.numpy.dtypes.numeric_scalars, stop: arkouda.numpy.dtypes.numeric_scalars, length: arkouda.numpy.dtypes.int_scalars) -> arkouda.pdarrayclass.pdarray

   Create a pdarray of linearly-spaced floats in a closed interval.

   :param start: Start of interval (inclusive)
   :type start: numeric_scalars
   :param stop: End of interval (inclusive)
   :type stop: numeric_scalars
   :param length: Number of points
   :type length: int_scalars

   :returns: Array of evenly spaced float values along the interval
   :rtype: pdarray, float64

   :raises TypeError: Raised if start or stop is not a float or int or if length is not an int

   .. seealso:: :obj:`arange`

   .. rubric:: Notes

   If that start is greater than stop, the pdarray values are generated
   in descending order.

   .. rubric:: Examples

   >>> ak.linspace(0, 1, 5)
   array([0.00000000000000000 0.25 0.5 0.75 1.00000000000000000])

   >>> ak.linspace(start=1, stop=0, length=5)
   array([1.00000000000000000 0.75 0.5 0.25 0.00000000000000000])

   >>> ak.linspace(start=-5, stop=0, length=5)
   array([-5.00000000000000000 -3.75 -2.5 -1.25 0.00000000000000000])


.. py:function:: list_registry(detailed: bool = False)

   Return a list containing the names of all registered objects

   :param detailed: Default = False
                    Return details of registry objects. Currently includes object type for any objects
   :type detailed: bool

   :returns: Dict containing keys "Components" and "Objects".
   :rtype: dict

   :raises RuntimeError: Raised if there's a server-side error thrown


.. py:function:: list_symbol_table() -> List[str]

   Return a list containing the names of all objects in the symbol table

   :param None:

   :returns: List of all object names in the symbol table
   :rtype: list

   :raises RuntimeError: Raised if there's a server-side error thrown


.. py:function:: load(path_prefix: str, file_format: str = 'INFER', dataset: str = 'array', calc_string_offsets: bool = False, column_delim: str = ',') -> Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray, arkouda.categorical.Categorical, arkouda.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.timeclass.Datetime, arkouda.timeclass.Timedelta, arkouda.index.Index]]]

   Load a pdarray previously saved with ``pdarray.save()``.

   :param path_prefix: Filename prefix used to save the original pdarray
   :type path_prefix: str
   :param file_format: 'INFER', 'HDF5' or 'Parquet'. Defaults to 'INFER'. Used to indicate the file type being loaded.
                       If INFER, this will be detected during processing
   :type file_format: str
   :param dataset: Dataset name where the pdarray was saved, defaults to 'array'
   :type dataset: str
   :param calc_string_offsets: If True the server will ignore Segmented Strings 'offsets' array and derive
                               it from the null-byte terminators.  Defaults to False currently
   :type calc_string_offsets: bool
   :param column_delim: Column delimiter to be used if dataset is CSV. Otherwise, unused.
   :type column_delim: str

   :returns: Dictionary of {datsetName: Union[pdarray, Strings, SegArray, Categorical]}
             with the previously saved pdarrays, Strings, SegArrays, or Categoricals
   :rtype: Mapping[str, Union[pdarray, Strings, SegArray, Categorical]]

   :raises TypeError: Raised if either path_prefix or dataset is not a str
   :raises ValueError: Raised if invalid file_format or if the dataset is not present in all hdf5 files or if the
       path_prefix does not correspond to files accessible to Arkouda
   :raises RuntimeError: Raised if the hdf5 files are present but there is an error in opening
       one or more of them

   .. seealso:: :obj:`to_parquet`, :obj:`to_hdf`, :obj:`load_all`, :obj:`read`

   .. rubric:: Notes

   If you have a previously saved Parquet file that is raising a FileNotFound error, try loading it
   with a .parquet appended to the prefix_path.
   Parquet files were previously ALWAYS stored with a ``.parquet`` extension.

   ak.load does not support loading a single file.
   For loading single HDF5 files without the _LOCALE#### suffix please use ak.read().

   CSV files without the Arkouda Header are not supported.

   .. rubric:: Examples

   >>> # Loading from file without extension
   >>> obj = ak.load('path/prefix')
   Loads the array from numLocales files with the name ``cwd/path/name_prefix_LOCALE####``.
   The file type is inferred during processing.

   >>> # Loading with an extension (HDF5)
   >>> obj = ak.load('path/prefix.test')
   Loads the object from numLocales files with the name ``cwd/path/name_prefix_LOCALE####.test`` where
   #### is replaced by each locale numbers. Because filetype is inferred during processing,
   the extension is not required to be a specific format.


.. py:function:: load_all(path_prefix: str, file_format: str = 'INFER', column_delim: str = ',', read_nested=True) -> Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray, arkouda.categorical.Categorical]]

   Load multiple pdarrays, Strings, SegArrays, or Categoricals previously
   saved with ``save_all()``.

   :param path_prefix: Filename prefix used to save the original pdarray
   :type path_prefix: str
   :param file_format: 'INFER', 'HDF5', 'Parquet', or 'CSV'. Defaults to 'INFER'. Indicates the format being loaded.
                       When 'INFER' the processing will detect the format
                       Defaults to 'INFER'
   :type file_format: str
   :param column_delim: Column delimiter to be used if dataset is CSV. Otherwise, unused.
   :type column_delim: str
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Parquet files only
   :type read_nested: bool

   :returns: Dictionary of {datsetName: Union[pdarray, Strings, SegArray, Categorical]}
             with the previously saved pdarrays, Strings, SegArrays, or Categoricals
   :rtype: Mapping[str, Union[pdarray, Strings, SegArray, Categorical]]

   :raises TypeError:: Raised if path_prefix is not a str
   :raises ValueError: Raised if file_format/extension is encountered that is not hdf5 or parquet or
       if all datasets are not present in all hdf5/parquet files or if the
       path_prefix does not correspond to files accessible to Arkouda
   :raises RuntimeError: Raised if the hdf5 files are present but there is an error in opening
       one or more of them

   .. seealso:: :obj:`to_parquet`, :obj:`to_hdf`, :obj:`load`, :obj:`read`

   .. rubric:: Notes

   This function has been updated to determine the file extension based on the file format variable

   This function will be deprecated when glob flags are added to read_* methods

   CSV files without the Arkouda Header are not supported.


.. py:function:: load_checkpoint(name, path='.akdata')

   Load server's state. The server metadata must match the current
   configuration (e.g. same number of locales must be used).

   :param name: Name of the checkpoint. ``<path>/<name>`` must be a directory.
   :type name: str
   :param path: The directory to save the checkpoint.
   :type path: str

   :returns: The checkpoint name, which will be the same as the ``name`` argument.
   :rtype: str

   .. rubric:: Examples

   >>> arr = ak.zeros(10, int)
   >>> arr[2] = 2
   >>> arr[2]
   2
   >>> cp_name = ak.save_checkpoint()
   >>> arr[2] = 3
   >>> arr[2]
   3
   >>> ak.load_checkpoint(cp_name)
   >>> arr[2]
   2

   .. seealso:: :obj:`save_checkpoint`


.. py:function:: log(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise natural log of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing natural log values of the input
             array elements
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Notes

   Logarithms with other bases can be computed as follows:

   .. rubric:: Examples

   >>> A = ak.array([1, 10, 100])
   # Natural log
   >>> ak.log(A)
   array([0.00000000000000000 2.3025850929940459 4.6051701859880918])
   # Log base 10
   >>> ak.log(A) / np.log(10)
   array([0.00000000000000000 1.00000000000000000 2.00000000000000000])
   # Log base 2
   >>> ak.log(A) / np.log(2)
   array([0.00000000000000000 3.3219280948873626 6.6438561897747253])


.. py:function:: log10(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise base 10 log of the array.

   :param pda: array to compute on
   :type pda: pdarray

   :returns: pdarray containing base 10 log values of the input array elements
   :rtype: pdarray


.. py:function:: log1p(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise natural log of one plus the array.

   :param pda: array to compute on
   :type pda: pdarray

   :returns: pdarray containing natural log values of the input array elements,
             adding one before taking the log
   :rtype: pdarray

   .. rubric:: Examples

   >>> ak.log1p(ak.arange(1,5))
   array([0.69314718055994529 1.0986122886681098 1.3862943611198906 1.6094379124341003])


.. py:function:: log2(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise base 2 log of the array.

   :param pda: array to compute on
   :type pda: pdarray

   :returns: pdarray containing base 2 log values of the input array elements
   :rtype: pdarray


.. py:class:: longdouble(value)

   Bases: :py:obj:`numpy.floating`


   Extended-precision floating-point number type, compatible with C
       ``long double`` but not necessarily with IEEE 754 quadruple-precision.

       :Character code: ``'g'``
       :Alias: `numpy.longfloat`
       :Alias on this platform (Linux x86_64): `numpy.float128`: 128-bit extended-precision floating-point number type.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      longdouble.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.longdouble(10.0).as_integer_ratio()
              (10, 1)
              >>> np.longdouble(0.0).as_integer_ratio()
              (0, 1)
              >>> np.longdouble(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: is_integer(*args, **kwargs)

      longdouble.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.longdouble(-2.0).is_integer()
              True
              >>> np.longdouble(3.2).is_integer()
              False




.. py:class:: longfloat(value)

   Bases: :py:obj:`numpy.floating`


   Extended-precision floating-point number type, compatible with C
       ``long double`` but not necessarily with IEEE 754 quadruple-precision.

       :Character code: ``'g'``
       :Alias: `numpy.longfloat`
       :Alias on this platform (Linux x86_64): `numpy.float128`: 128-bit extended-precision floating-point number type.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      longdouble.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.longdouble(10.0).as_integer_ratio()
              (10, 1)
              >>> np.longdouble(0.0).as_integer_ratio()
              (0, 1)
              >>> np.longdouble(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: is_integer(*args, **kwargs)

      longdouble.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.longdouble(-2.0).is_integer()
              True
              >>> np.longdouble(3.2).is_integer()
              False




.. py:class:: longlong(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with C ``long long``.

       :Character code: ``'q'``



   .. py:method:: bit_count(*args, **kwargs)


.. py:function:: lookup(keys, values, arguments, fillvalue=-1)

   Apply the function defined by the mapping keys --> values to arguments.

   :param keys: The domain of the function. Entries must be unique (if a sequence of
                arrays is given, each row is treated as a tuple-valued entry).
   :type keys: (sequence of) array-like
   :param values: The range of the function. Must be same length as keys.
   :type values: pdarray
   :param arguments: The arguments on which to evaluate the function. Must have same dtype
                     (or tuple of dtypes, for a sequence) as keys.
   :type arguments: (sequence of) array-like
   :param fillvalue: The default value to return for arguments not in keys.
   :type fillvalue: scalar

   :returns: **evaluated** -- The result of evaluating the function over arguments.
   :rtype: pdarray

   .. rubric:: Notes

   While the values cannot be Strings (or other complex objects), the same
   result can be achieved by passing an arange as the values, then using
   the return as indices into the desired object.

   .. rubric:: Examples

   Lookup numbers by two-word name

   >>> keys1 = ak.array(['twenty' for _ in range(5)])
   >>> keys2 = ak.array(['one', 'two', 'three', 'four', 'five'])
   >>> values = ak.array([21, 22, 23, 24, 25])
   >>> args1 = ak.array(['twenty', 'thirty', 'twenty'])
   >>> args2 = ak.array(['four', 'two', 'two'])
   >>> ak.lookup([keys1, keys2], values, [args1, args2])
   array([24 -1 22])

   Other direction requires an intermediate index

   >>> revkeys = values
   >>> revindices = ak.arange(values.size)
   >>> revargs = ak.array([24, 21, 22])
   >>> idx = ak.lookup(revkeys, revindices, revargs)
   >>> keys1[idx], keys2[idx]
   (array(['twenty', 'twenty', 'twenty']),
   array(['four', 'one', 'two']))


.. py:function:: ls(filename: str, col_delim: str = ',', read_nested: bool = True) -> List[str]

   This function calls the h5ls utility on a HDF5 file visible to the
   arkouda server or calls a function that imitates the result of h5ls
   on a Parquet file.

   :param filename: The name of the file to pass to the server
   :type filename: str
   :param col_delim: The delimiter used to separate columns if the file is a csv
   :type col_delim: str
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Only used for Parquet files.
   :type read_nested: bool

   :returns: The string output of the datasets from the server
   :rtype: str

   :raises TypeError: Raised if filename is not a str
   :raises ValueError: Raised if filename is empty or contains only whitespace
   :raises RuntimeError: Raised if error occurs in executing ls on an HDF5 file
   :raises Notes: - This will need to be updated because Parquet will not technically support this when we update.
           Similar functionality will be added for Parquet in the future
       - For CSV files without headers, please use ls_csv

   .. seealso:: :obj:`ls_csv`


.. py:function:: ls_csv(filename: str, col_delim: str = ',') -> List[str]

   Used for identifying the datasets within a file when a CSV does not
   have a header.

   :param filename: The name of the file to pass to the server
   :type filename: str
   :param col_delim: The delimiter used to separate columns if the file is a csv
   :type col_delim: str

   :returns: The string output of the datasets from the server
   :rtype: str

   .. seealso:: :obj:`ls`


.. py:function:: matmul(pdaLeft: arkouda.pdarrayclass.pdarray, pdaRight: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Compute the product of two matrices.

   :param pdaLeft:
   :type pdaLeft: pdarray
   :param pdaRight:
   :type pdaRight: pdarray

   :returns: the matrix product pdaLeft x pdaRight
   :rtype: pdarray

   .. rubric:: Examples

   >>> a = ak.array([[1,2,3,4,5],[1,2,3,4,5]])
   >>> b = ak.array([[1,1],[2,2],[3,3],[4,4],[5,5]])
   >>> ak.matmul(a,b)
   array([array([55 55]) array([55 55])])

   >>> x = ak.array([[1,2,3],[1.1,2.1,3.1]])
   >>> y = ak.array([[1,1,1],[0,2,2],[0,0,3]])
   >>> ak.matmul(x,y)
   array([array([1.00000000000000000 5.00000000000000000 14.00000000000000000])
   array([1.1000000000000001 5.3000000000000007 14.600000000000001])])

   .. rubric:: Notes

   Server returns an error if shapes of pdaLeft and pdaRight
   are incompatible with matrix multiplication.


.. py:function:: maximum_sctype(t)

   Return the scalar type of highest precision of the same kind as the input.

   :param t: The input data type. This can be a `dtype` object or an object that
             is convertible to a `dtype`.
   :type t: dtype or dtype specifier

   :returns: **out** -- The highest precision data type of the same kind (`dtype.kind`) as `t`.
   :rtype: dtype

   .. seealso:: :obj:`obj2sctype`, :obj:`mintypecode`, :obj:`sctype2char`, :obj:`dtype`

   .. rubric:: Examples

   >>> np.maximum_sctype(int)
   <class 'numpy.int64'>
   >>> np.maximum_sctype(np.uint8)
   <class 'numpy.uint64'>
   >>> np.maximum_sctype(complex)
   <class 'numpy.complex256'> # may vary

   >>> np.maximum_sctype(str)
   <class 'numpy.str_'>

   >>> np.maximum_sctype('i2')
   <class 'numpy.int64'>
   >>> np.maximum_sctype('f4')
   <class 'numpy.float128'> # may vary


.. py:function:: maxk(pda: pdarray, k: arkouda.numpy.dtypes.int_scalars) -> pdarray

   Find the `k` maximum values of an array.

   Returns the largest `k` values of an array, sorted

   :param pda: Input array.
   :type pda: pdarray
   :param k: The desired count of maximum values to be returned by the output.
   :type k: int_scalars

   :returns: The maximum `k` values from pda, sorted
   :rtype: pdarray, int

   :raises TypeError: Raised if pda is not a pdarray or k is not an integer
   :raises ValueError: Raised if the pda is empty or k < 1

   .. rubric:: Notes

   This call is equivalent in value to:

       a[ak.argsort(a)[k:]]

   and generally outperforms this operation.

   This reduction will see a significant drop in performance as `k` grows
   beyond a certain value. This value is system dependent, but generally
   about a `k` of 5 million is where performance degredation has been observed.

   .. rubric:: Examples

   >>> A = ak.array([10,5,1,3,7,2,9,0])
   >>> ak.maxk(A, 3)
   array([7, 9, 10])
   >>> ak.maxk(A, 4)
   array([5, 7, 9, 10])


.. py:function:: mean(pda: pdarray) -> numpy.float64

   Return the mean of the array.

   :param pda: Values for which to calculate the mean
   :type pda: pdarray

   :returns: The mean calculated from the pda sum and size
   :rtype: np.float64

   :raises TypeError: Raised if pda is not a pdarray instance
   :raises RuntimeError: Raised if there's a server-side error thrown


.. py:function:: median(pda: arkouda.pdarrayclass.pdarray) -> numpy.float64

   Compute the median of a given array.  1d case only, for now.

   :param pda: The input data, in pdarray form, numeric type or boolean
   :type pda: pdarray

   :returns: | The median of the entire pdarray
             | The array is sorted, and then if the number of elements is odd,
                 the return value is the middle element.  If even, then the
                 mean of the two middle elements.
   :rtype: np.float64

   .. rubric:: Examples

   >>> pda = ak.array([0,4,7,8,1,3,5,2,-1])
   >>> ak.median(pda)
   3.0
   >>> pda = ak.array([0,1,3,3,1,2,3,4,2,3])
   >>> ak.median(pda)
   2.5


.. py:function:: merge(left: DataFrame, right: DataFrame, on: Optional[Union[str, List[str]]] = None, how: str = 'inner', left_suffix: str = '_x', right_suffix: str = '_y', convert_ints: bool = True, sort: bool = True) -> DataFrame

   Merge Arkouda DataFrames with a database-style join.
   The resulting dataframe contains rows from both DataFrames as specified by
   the merge condition (based on the "how" and "on" parameters).

   Based on pandas merge functionality.
   https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html

   :param left: The Left DataFrame to be joined.
   :type left: DataFrame
   :param right: The Right DataFrame to be joined.
   :type right: DataFrame
   :param on: The name or list of names of the DataFrame column(s) to join on.
              If on is None, this defaults to the intersection of the columns in both DataFrames.
   :type on: Optional[Union[str, List[str]]] = None
   :param how: The merge condition.
               Must be one of "inner", "left", "right", or "outer".
   :type how: str, default = "inner"
   :param left_suffix: A string indicating the suffix to add to columns from the left dataframe for overlapping
                       column names in both left and right. Defaults to "_x". Only used when how is "inner".
   :type left_suffix: str, default = "_x"
   :param right_suffix: A string indicating the suffix to add to columns from the right dataframe for overlapping
                        column names in both left and right. Defaults to "_y". Only used when how is "inner".
   :type right_suffix: str, default = "_y"
   :param convert_ints: If True, convert columns with missing int values (due to the join) to float64.
                        This is to match pandas.
                        If False, do not convert the column dtypes.
                        This has no effect when how = "inner".
   :type convert_ints: bool = True
   :param sort: If True, DataFrame is returned sorted by "on".
                Otherwise, the DataFrame is not sorted.
   :type sort: bool = True

   :returns: Joined Arkouda DataFrame.
   :rtype: arkouda.dataframe.DataFrame

   .. note:: Multiple column joins are only supported for integer columns.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.connect()
   >>> from arkouda import merge
   >>> left_df = ak.DataFrame({'col1': ak.arange(5), 'col2': -1 * ak.arange(5)})
   >>> display(left_df)

   +----+--------+--------+
   |    |   col1 |   col2 |
   +====+========+========+
   |  0 |      0 |      0 |
   +----+--------+--------+
   |  1 |      1 |     -1 |
   +----+--------+--------+
   |  2 |      2 |     -2 |
   +----+--------+--------+
   |  3 |      3 |     -3 |
   +----+--------+--------+
   |  4 |      4 |     -4 |
   +----+--------+--------+

   >>> right_df = ak.DataFrame({'col1': 2 * ak.arange(5), 'col2': 2 * ak.arange(5)})
   >>> display(right_df)

   +----+--------+--------+
   |    |   col1 |   col2 |
   +====+========+========+
   |  0 |      0 |      0 |
   +----+--------+--------+
   |  1 |      2 |      2 |
   +----+--------+--------+
   |  2 |      4 |      4 |
   +----+--------+--------+
   |  3 |      6 |      6 |
   +----+--------+--------+
   |  4 |      8 |      8 |
   +----+--------+--------+

   >>> merge(left_df, right_df, on = "col1")

   +----+--------+----------+----------+
   |    |   col1 |   col2_x |   col2_y |
   +====+========+==========+==========+
   |  0 |      0 |        0 |        0 |
   +----+--------+----------+----------+
   |  1 |      2 |       -2 |        2 |
   +----+--------+----------+----------+
   |  2 |      4 |       -4 |        4 |
   +----+--------+----------+----------+

   >>> merge(left_df, right_df, on = "col1", how = "left")

   +----+--------+----------+----------+
   |    |   col1 |   col2_y |   col2_x |
   +====+========+==========+==========+
   |  0 |      0 |        0 |        0 |
   +----+--------+----------+----------+
   |  1 |      1 |      nan |       -1 |
   +----+--------+----------+----------+
   |  2 |      2 |        2 |       -2 |
   +----+--------+----------+----------+
   |  3 |      3 |      nan |       -3 |
   +----+--------+----------+----------+
   |  4 |      4 |        4 |       -4 |
   +----+--------+----------+----------+

   >>> merge(left_df, right_df, on = "col1", how = "right")

   +----+--------+----------+----------+
   |    |   col1 |   col2_x |   col2_y |
   +====+========+==========+==========+
   |  0 |      0 |        0 |        0 |
   +----+--------+----------+----------+
   |  1 |      2 |       -2 |        2 |
   +----+--------+----------+----------+
   |  2 |      4 |       -4 |        4 |
   +----+--------+----------+----------+
   |  3 |      6 |      nan |        6 |
   +----+--------+----------+----------+
   |  4 |      8 |      nan |        8 |
   +----+--------+----------+----------+

   >>> merge(left_df, right_df, on = "col1", how = "outer")

   +----+--------+----------+----------+
   |    |   col1 |   col2_y |   col2_x |
   +====+========+==========+==========+
   |  0 |      0 |        0 |        0 |
   +----+--------+----------+----------+
   |  1 |      1 |      nan |       -1 |
   +----+--------+----------+----------+
   |  2 |      2 |        2 |       -2 |
   +----+--------+----------+----------+
   |  3 |      3 |      nan |       -3 |
   +----+--------+----------+----------+
   |  4 |      4 |        4 |       -4 |
   +----+--------+----------+----------+
   |  5 |      6 |        6 |      nan |
   +----+--------+----------+----------+
   |  6 |      8 |        8 |      nan |
   +----+--------+----------+----------+


.. py:function:: mink(pda: pdarray, k: arkouda.numpy.dtypes.int_scalars) -> pdarray

   Find the `k` minimum values of an array.

   Returns the smallest `k` values of an array, sorted

   :param pda: Input array.
   :type pda: pdarray
   :param k: The desired count of minimum values to be returned by the output.
   :type k: int_scalars

   :returns: The minimum `k` values from pda, sorted
   :rtype: pdarray

   :raises TypeError: Raised if pda is not a pdarray
   :raises ValueError: Raised if the pda is empty or k < 1

   .. rubric:: Notes

   This call is equivalent in value to:

       a[ak.argsort(a)[:k]]

   and generally outperforms this operation.

   This reduction will see a significant drop in performance as `k` grows
   beyond a certain value. This value is system dependent, but generally
   about a `k` of 5 million is where performance degredation has been observed.

   .. rubric:: Examples

   >>> A = ak.array([10,5,1,3,7,2,9,0])
   >>> ak.mink(A, 3)
   array([0, 1, 2])
   >>> ak.mink(A, 4)
   array([0, 1, 2, 3])


.. py:function:: mod(dividend, divisor) -> pdarray

   Returns the element-wise remainder of division.

   Computes the remainder complementary to the floor_divide function.
   It is equivalent to np.mod, the remainder has the same sign as the divisor.

   :param dividend: The array being acted on by the bases for the modular division.
   :param divisor: The array that will be the bases for the modular division.

   :returns: Returns an array that contains the element-wise remainder of division.
   :rtype: pdarray


.. py:data:: nan
   :type:  float

.. py:class:: number(value)

   Bases: :py:obj:`numpy.generic`


   Abstract base class of all numeric scalar types.



.. py:class:: numeric_and_bool_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:class:: numeric_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:class:: numpy_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:class:: object_(value)

   Bases: :py:obj:`numpy.generic`


   Any Python object.

       :Character code: ``'O'``



.. py:function:: ones(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with ones.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: Resulting array type, default ak.float64
   :type dtype: Union[float64, int64, bool]
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
                    Included for consistency, as ones are all zeros ending on a one, regardless
                    of max_bits
   :type max_bits: int

   :returns: Ones of the requested size or shape and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`zeros`, :obj:`ones_like`

   .. rubric:: Examples

   >>> ak.ones(5, dtype=ak.int64)
   array([1 1 1 1 1])

   >>> ak.ones(5, dtype=ak.float64)
   array([1.00000000000000000 1.00000000000000000 1.00000000000000000
          1.00000000000000000 1.00000000000000000])

   >>> ak.ones(5, dtype=ak.bool_)
   array([True True True True True])

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.


.. py:function:: ones(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with ones.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: Resulting array type, default ak.float64
   :type dtype: Union[float64, int64, bool]
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
                    Included for consistency, as ones are all zeros ending on a one, regardless
                    of max_bits
   :type max_bits: int

   :returns: Ones of the requested size or shape and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`zeros`, :obj:`ones_like`

   .. rubric:: Examples

   >>> ak.ones(5, dtype=ak.int64)
   array([1 1 1 1 1])

   >>> ak.ones(5, dtype=ak.float64)
   array([1.00000000000000000 1.00000000000000000 1.00000000000000000
          1.00000000000000000 1.00000000000000000])

   >>> ak.ones(5, dtype=ak.bool_)
   array([True True True True True])

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.


.. py:function:: ones(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with ones.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: Resulting array type, default ak.float64
   :type dtype: Union[float64, int64, bool]
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
                    Included for consistency, as ones are all zeros ending on a one, regardless
                    of max_bits
   :type max_bits: int

   :returns: Ones of the requested size or shape and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`zeros`, :obj:`ones_like`

   .. rubric:: Examples

   >>> ak.ones(5, dtype=ak.int64)
   array([1 1 1 1 1])

   >>> ak.ones(5, dtype=ak.float64)
   array([1.00000000000000000 1.00000000000000000 1.00000000000000000
          1.00000000000000000 1.00000000000000000])

   >>> ak.ones(5, dtype=ak.bool_)
   array([True True True True True])

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.


.. py:function:: ones(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with ones.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: Resulting array type, default ak.float64
   :type dtype: Union[float64, int64, bool]
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
                    Included for consistency, as ones are all zeros ending on a one, regardless
                    of max_bits
   :type max_bits: int

   :returns: Ones of the requested size or shape and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`zeros`, :obj:`ones_like`

   .. rubric:: Examples

   >>> ak.ones(5, dtype=ak.int64)
   array([1 1 1 1 1])

   >>> ak.ones(5, dtype=ak.float64)
   array([1.00000000000000000 1.00000000000000000 1.00000000000000000
          1.00000000000000000 1.00000000000000000])

   >>> ak.ones(5, dtype=ak.bool_)
   array([True True True True True])

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.


.. py:function:: ones_like(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Create a one-filled pdarray of the same size and dtype as an existing
   pdarray.

   :param pda: Array to use for size and dtype
   :type pda: pdarray

   :returns: Equivalent to ak.ones(pda.size, pda.dtype)
   :rtype: pdarray

   :raises TypeError: Raised if the pda parameter is not a pdarray.

   .. seealso:: :obj:`ones`, :obj:`zeros_like`

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.ones method.
   Accordingly, the supported dtypes match are defined by the ak.ones method.

   .. rubric:: Examples

   >>> ak.ones_like(ak.zeros(5,dtype=ak.int64))
   array([1 1 1 1 1])

   >>> ak.ones_like(ak.zeros(5,dtype=ak.float64))
   array([1.00000000000000000 1.00000000000000000 1.00000000000000000
          1.00000000000000000 1.00000000000000000])

   >>> ak.ones_like(ak.zeros(5,dtype=ak.bool_))
   array([True True True True True])


.. py:function:: parity(pda: pdarray) -> pdarray

   Find the bit parity (XOR of all bits) for each integer in an array.

   :param pda: Input array (must be integral).
   :type pda: pdarray, int64, uint64, bigint

   :returns: **parity** -- The parity of each element: 0 if even number of bits set, 1 if odd.
   :rtype: pdarray

   :raises TypeError: If input array is not int64, uint64, or bigint

   .. rubric:: Examples

   >>> A = ak.arange(10)
   >>> ak.parity(A)
   array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0])


.. py:class:: pdarray(name: str, mydtype: Union[numpy.dtype, str], size: arkouda.numpy.dtypes.int_scalars, ndim: arkouda.numpy.dtypes.int_scalars, shape: Sequence[int], itemsize: arkouda.numpy.dtypes.int_scalars, max_bits: Optional[int] = None)

   The basic arkouda array class. This class contains only the
   attributies of the array; the data resides on the arkouda
   server. When a server operation results in a new array, arkouda
   will create a pdarray instance that points to the array data on
   the server. As such, the user should not initialize pdarray
   instances directly.

   .. attribute:: name

      The server-side identifier for the array

      :type: str

   .. attribute:: dtype

      The element type of the array

      :type: dtype

   .. attribute:: size

      The number of elements in the array

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      A list or tuple containing the sizes of each dimension of the array

      :type: Sequence[int]

   .. attribute:: itemsize

      The size in bytes of each element

      :type: int_scalars


   .. py:attribute:: BinOps


   .. py:attribute:: OpEqOps


   .. py:method:: all(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff all elements of the array evaluate to True.



   .. py:method:: any(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff any element of the array evaluates to True.



   .. py:method:: argmax(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array max value.



   .. py:method:: argmaxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Finds the indices corresponding to the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the  maximum `k` values, sorted
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: argmin(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array min value



   .. py:method:: argmink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: astype(dtype) -> pdarray

      Cast values of pdarray to provided dtype

      :param dtype: Dtype to cast to
      :type dtype: np.dtype or str

      :returns: An arkouda pdarray with values converted to the specified data type
      :rtype: ak.pdarray

      .. rubric:: Notes

      This is essentially shorthand for ak.cast(x, '<dtype>') where x is a pdarray.



   .. py:method:: attach(user_defined_name: str) -> pdarray
      :staticmethod:


      class method to return a pdarray attached to the registered name in the arkouda
      server which was registered using register()

      :param user_defined_name: user defined name which array was registered under
      :type user_defined_name: str

      :returns: pdarray which is bound to the corresponding server side component which was registered
                with user_defined_name
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: bigint_to_uint_arrays() -> List[pdarray]

      Creates a list of uint pdarrays from a bigint pdarray.
      The first item in return will be the highest 64 bits of the
      bigint pdarray and the last item will be the lowest 64 bits.

      :returns: A list of uint pdarrays where:
                The first item in return will be the highest 64 bits of the
                bigint pdarray and the last item will be the lowest 64 bits.
      :rtype: List[pdarrays]

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`pdarraycreation.bigint_from_uint_arrays`

      .. rubric:: Examples

      >>> a = ak.arange(2**64, 2**64 + 5)
      >>> a
      array(["18446744073709551616" "18446744073709551617" "18446744073709551618"
      "18446744073709551619" "18446744073709551620"])

      >>> a.bigint_to_uint_arrays()
      [array([1 1 1 1 1]), array([0 1 2 3 4])]



   .. py:method:: clz() -> pdarray

      Count the number of leading zeros in each element. See `ak.clz`.



   .. py:method:: corr(y: pdarray) -> numpy.float64

      Compute the correlation between self and y using pearson correlation coefficient.

      :param y: Other pdarray used to calculate correlation
      :type y: pdarray

      :returns: The scalar correlation of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: cov(y: pdarray) -> numpy.float64

      Compute the covariance between self and y.

      :param y: Other pdarray used to calculate covariance
      :type y: pdarray

      :returns: The scalar covariance of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: ctz() -> pdarray

      Count the number of trailing zeros in each element. See `ak.ctz`.



   .. py:attribute:: dtype


   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether pdarrays are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the pdarrays are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array([1, 2, 3])
      >>> a_cpy = ak.array([1, 2, 3])
      >>> a.equals(a_cpy)
      True
      >>> a2 = ak.array([1, 2, 5)
      >>> a.equals(a2)
      False



   .. py:method:: fill(value: arkouda.numpy.dtypes.numeric_scalars) -> None

      Fill the array (in place) with a constant value.

      :param value:
      :type value: numeric_scalars

      :raises TypeError: Raised if value is not an int, int64, float, or float64



   .. py:method:: flatten()

      Return a copy of the array collapsed into one dimension.

      :rtype: A copy of the input array, flattened to one dimension.



   .. py:method:: format_other(other) -> str

      Attempt to cast scalar other to the element dtype of this pdarray,
      and print the resulting value to a string (e.g. for sending to a
      server command). The user should not call this function directly.

      :param other: The scalar to be cast to the pdarray.dtype
      :type other: object

      :rtype: string representation of np.dtype corresponding to the other parameter

      :raises TypeError: Raised if the other parameter cannot be converted to
          Numpy dtype



   .. py:property:: inferred_type
      :type: Union[str, None]


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry

      :param None:

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RuntimeError: Raised if there's a server-side error thrown

      .. note::

         This will return True if the object is registered itself or as a component
         of another object



   .. py:method:: is_sorted(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff the array is monotonically non-decreasing.

      :param None:

      :returns: Indicates if the array is monotonically non-decreasing
      :rtype: bool

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:attribute:: itemsize


   .. py:method:: max(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the maximum value of the array.



   .. py:property:: max_bits


   .. py:method:: maxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: mean() -> numpy.float64

      Return the mean of the array.



   .. py:method:: min(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the minimum value of the array.



   .. py:method:: mink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:attribute:: name


   .. py:property:: nbytes

      The size of the pdarray in bytes.

      :returns: The size of the pdarray in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: objType
      :value: 'pdarray'



   .. py:method:: opeq(other, op)


   .. py:method:: parity() -> pdarray

      Find the parity (XOR of all bits) in each element. See `ak.parity`.



   .. py:method:: popcount() -> pdarray

      Find the population (number of bits set) in each element. See `ak.popcount`.



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: prod(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the product of all elements in the array. Return value is
      always a np.float64 or np.int64.



   .. py:method:: register(user_defined_name: str) -> pdarray

      Register this pdarray with a user defined name in the arkouda server
      so it can be attached to later using pdarray.attach()
      This is an in-place operation, registering a pdarray more than once will
      update the name in the registry and remove the previously registered name.
      A name can only be registered to one pdarray at a time.

      :param user_defined_name: user defined name array is to be registered under
      :type user_defined_name: str

      :returns: The same pdarray which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different pdarrays with the same name.
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the pdarray with the user_defined_name
          If the user is attempting to register more than one pdarray with the same name,
          the former should be unregistered first to free up the registration name.

      .. seealso:: :obj:`attach`, :obj:`unregister`, :obj:`is_registered`, :obj:`list_registry`, :obj:`unregister_pdarray_by_name`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reshape(*shape)

      Gives a new shape to an array without changing its data.

      :param shape: The new shape should be compatible with the original shape.
      :type shape: int, tuple of ints, or pdarray

      :returns: a pdarray with the same data, reshaped to the new shape
      :rtype: pdarray



   .. py:method:: rotl(other) -> pdarray

      Rotate bits left by <other>.



   .. py:method:: rotr(other) -> pdarray

      Rotate bits right by <other>.



   .. py:method:: save(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the pdarray to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. HDF5 support single files, in which case the file name will
      only be that provided. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str {'HDF5', 'Parquet'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: Raised if there is an error in parsing the prefix path pointing to
          file write location or if the mode parameter is neither truncate
          nor append
      :raises TypeError: Raised if any one of the prefix_path, dataset, or mode parameters
          is not a string

      .. seealso:: :obj:`save_all`, :obj:`load`, :obj:`read`, :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Notes

      The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales``. If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      Previously all files saved in Parquet format were saved with a ``.parquet`` file extension.
      This will require you to use load as if you saved the file with the extension. Try this if
      an older file is not being found.
      Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.save('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.save('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving with an extension (Parquet)
      >>> a.save('path/prefix.parquet', dataset='array', file_format='Parquet')
      Saves the array in numLocales Parquet files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:property:: shape

      Return the shape of an array.

      :returns: The elements of the shape tuple give the lengths of the corresponding array dimensions.
      :rtype: tuple of int


   .. py:attribute:: size


   .. py:method:: slice_bits(low, high) -> pdarray

      Returns a pdarray containing only bits from low to high of self.

      This is zero indexed and inclusive on both ends, so slicing the bottom 64 bits is
      pda.slice_bits(0, 63)

      :param low: The lowest bit included in the slice (inclusive)
                  zero indexed, so the first bit is 0
      :type low: int
      :param high: The highest bit included in the slice (inclusive)
      :type high: int

      :returns: A new pdarray containing the bits of self from low to high
      :rtype: pdarray

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. rubric:: Examples

      >>> p = ak.array([2**65 + (2**64 - 1)])
      >>> bin(p[0])
      '0b101111111111111111111111111111111111111111111111111111111111111111'

      >>> bin(p.slice_bits(64, 65)[0])
      '0b10'



   .. py:method:: std(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the standard deviation. See ``arkouda.std`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: The scalar standard deviation of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: sum(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the sum of all elements in the array.



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'array', col_delim: str = ',', overwrite: bool = False)

              Write pdarray to CSV file(s). File will contain a single column with the pdarray data.
              All CSV Files written by Arkouda include a header denoting data types of the columns.

              Parameters
              -----------
              prefix_path: str
                  The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                  when they are written to disk.
              dataset: str
                  Column name to save the pdarray under. Defaults to "array".
              col_delim: str
                  Defaults to ",". Value to be used to separate columns within the file.
                  Please be sure that the value used DOES NOT appear in your dataset.
              overwrite: bool
                  Defaults to False. If True, any existing files matching your provided prefix_path will
                  be overwritten. If False, an error will be returned if existing files are found.

              Returns
              --------
              str reponse message

              Raises
              ------
              ValueError
                  Raised if all datasets are not present in all parquet files or if one or
                  more of the specified files do not exist
              RuntimeError
                  Raised if one or more of the specified files cannot be opened.
                  If `allow_errors` is true this may be raised if no values are returned
                  from the server.
              TypeError
                  Raised if we receive an unknown arkouda_type returned from the server

              Notes
              ------
              - CSV format is not currently supported by load/load_all operations
              - The column delimiter is expected to be the same for column names and data
              - Be sure that column delimiters are not found within your data.
              - All CSV files must delimit rows using newline (`
      `) at this time.




   .. py:method:: to_cuda()

      Convert the array to a Numba DeviceND array, transferring array data from the
      arkouda server to Python via ndarray. If the array exceeds a builtin size limit,
      a RuntimeError is raised.

      :returns: A Numba ndarray with the same attributes and data as the pdarray; on GPU
      :rtype: numba.DeviceNDArray

      :raises ImportError: Raised if CUDA is not available
      :raises ModuleNotFoundError: Raised if Numba is either not installed or not enabled
      :raises RuntimeError: Raised if there is a server-side error thrown in the course of retrieving
          the pdarray.

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_cuda()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_cuda())
      numpy.devicendarray



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the pdarray to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_hdf('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_hdf('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving to a single file
      >>> a.to_hdf('path/prefix.hdf5', dataset='array', file_type='single')
      Saves the array in to single hdf5 file on the root node.
      ``cwd/path/name_prefix.hdf5``



   .. py:method:: to_list() -> List

      Convert the array to a list, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A list with the same data as the pdarray
      :rtype: list

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_list()
      [0, 1, 2, 3, 4]

      >>> type(a.to_list())
      list



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A numpy ndarray with the same attributes and data as the pdarray
      :rtype: np.ndarray

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_ndarray()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      Save the pdarray to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_parquet('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_parqet('path/prefix.parquet', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a pdarray to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the pdarray is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister() -> None

      Unregister a pdarray in the arkouda server which was previously
      registered using register() and/or attahced to using attach()


      :rtype: None

      :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion until
      they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'array', repack: bool = True)

      Overwrite the dataset with the name provided with this pdarray. If
      the dataset does not exist it is added

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added



   .. py:method:: value_counts()

      Count the occurrences of the unique values of self.

      :returns: * **unique_values** (*pdarray*) -- The unique values, sorted in ascending order
                * **counts** (*pdarray, int64*) -- The number of times the corresponding unique value occurs

      .. rubric:: Examples

      >>> ak.array([2, 0, 2, 4, 0, 0]).value_counts()
      (array([0, 2, 4]), array([3, 2, 1]))



   .. py:method:: var(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the variance. See ``arkouda.var`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: The scalar variance of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises ValueError: Raised if the ddof >= pdarray size
      :raises RuntimeError: Raised if there's a server-side error thrown



.. py:class:: pdarray(name: str, mydtype: Union[numpy.dtype, str], size: arkouda.numpy.dtypes.int_scalars, ndim: arkouda.numpy.dtypes.int_scalars, shape: Sequence[int], itemsize: arkouda.numpy.dtypes.int_scalars, max_bits: Optional[int] = None)

   The basic arkouda array class. This class contains only the
   attributies of the array; the data resides on the arkouda
   server. When a server operation results in a new array, arkouda
   will create a pdarray instance that points to the array data on
   the server. As such, the user should not initialize pdarray
   instances directly.

   .. attribute:: name

      The server-side identifier for the array

      :type: str

   .. attribute:: dtype

      The element type of the array

      :type: dtype

   .. attribute:: size

      The number of elements in the array

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      A list or tuple containing the sizes of each dimension of the array

      :type: Sequence[int]

   .. attribute:: itemsize

      The size in bytes of each element

      :type: int_scalars


   .. py:attribute:: BinOps


   .. py:attribute:: OpEqOps


   .. py:method:: all(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff all elements of the array evaluate to True.



   .. py:method:: any(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff any element of the array evaluates to True.



   .. py:method:: argmax(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array max value.



   .. py:method:: argmaxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Finds the indices corresponding to the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the  maximum `k` values, sorted
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: argmin(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array min value



   .. py:method:: argmink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: astype(dtype) -> pdarray

      Cast values of pdarray to provided dtype

      :param dtype: Dtype to cast to
      :type dtype: np.dtype or str

      :returns: An arkouda pdarray with values converted to the specified data type
      :rtype: ak.pdarray

      .. rubric:: Notes

      This is essentially shorthand for ak.cast(x, '<dtype>') where x is a pdarray.



   .. py:method:: attach(user_defined_name: str) -> pdarray
      :staticmethod:


      class method to return a pdarray attached to the registered name in the arkouda
      server which was registered using register()

      :param user_defined_name: user defined name which array was registered under
      :type user_defined_name: str

      :returns: pdarray which is bound to the corresponding server side component which was registered
                with user_defined_name
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: bigint_to_uint_arrays() -> List[pdarray]

      Creates a list of uint pdarrays from a bigint pdarray.
      The first item in return will be the highest 64 bits of the
      bigint pdarray and the last item will be the lowest 64 bits.

      :returns: A list of uint pdarrays where:
                The first item in return will be the highest 64 bits of the
                bigint pdarray and the last item will be the lowest 64 bits.
      :rtype: List[pdarrays]

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`pdarraycreation.bigint_from_uint_arrays`

      .. rubric:: Examples

      >>> a = ak.arange(2**64, 2**64 + 5)
      >>> a
      array(["18446744073709551616" "18446744073709551617" "18446744073709551618"
      "18446744073709551619" "18446744073709551620"])

      >>> a.bigint_to_uint_arrays()
      [array([1 1 1 1 1]), array([0 1 2 3 4])]



   .. py:method:: clz() -> pdarray

      Count the number of leading zeros in each element. See `ak.clz`.



   .. py:method:: corr(y: pdarray) -> numpy.float64

      Compute the correlation between self and y using pearson correlation coefficient.

      :param y: Other pdarray used to calculate correlation
      :type y: pdarray

      :returns: The scalar correlation of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: cov(y: pdarray) -> numpy.float64

      Compute the covariance between self and y.

      :param y: Other pdarray used to calculate covariance
      :type y: pdarray

      :returns: The scalar covariance of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: ctz() -> pdarray

      Count the number of trailing zeros in each element. See `ak.ctz`.



   .. py:attribute:: dtype


   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether pdarrays are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the pdarrays are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array([1, 2, 3])
      >>> a_cpy = ak.array([1, 2, 3])
      >>> a.equals(a_cpy)
      True
      >>> a2 = ak.array([1, 2, 5)
      >>> a.equals(a2)
      False



   .. py:method:: fill(value: arkouda.numpy.dtypes.numeric_scalars) -> None

      Fill the array (in place) with a constant value.

      :param value:
      :type value: numeric_scalars

      :raises TypeError: Raised if value is not an int, int64, float, or float64



   .. py:method:: flatten()

      Return a copy of the array collapsed into one dimension.

      :rtype: A copy of the input array, flattened to one dimension.



   .. py:method:: format_other(other) -> str

      Attempt to cast scalar other to the element dtype of this pdarray,
      and print the resulting value to a string (e.g. for sending to a
      server command). The user should not call this function directly.

      :param other: The scalar to be cast to the pdarray.dtype
      :type other: object

      :rtype: string representation of np.dtype corresponding to the other parameter

      :raises TypeError: Raised if the other parameter cannot be converted to
          Numpy dtype



   .. py:property:: inferred_type
      :type: Union[str, None]


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry

      :param None:

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RuntimeError: Raised if there's a server-side error thrown

      .. note::

         This will return True if the object is registered itself or as a component
         of another object



   .. py:method:: is_sorted(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff the array is monotonically non-decreasing.

      :param None:

      :returns: Indicates if the array is monotonically non-decreasing
      :rtype: bool

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:attribute:: itemsize


   .. py:method:: max(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the maximum value of the array.



   .. py:property:: max_bits


   .. py:method:: maxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: mean() -> numpy.float64

      Return the mean of the array.



   .. py:method:: min(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the minimum value of the array.



   .. py:method:: mink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:attribute:: name


   .. py:property:: nbytes

      The size of the pdarray in bytes.

      :returns: The size of the pdarray in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: objType
      :value: 'pdarray'



   .. py:method:: opeq(other, op)


   .. py:method:: parity() -> pdarray

      Find the parity (XOR of all bits) in each element. See `ak.parity`.



   .. py:method:: popcount() -> pdarray

      Find the population (number of bits set) in each element. See `ak.popcount`.



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: prod(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the product of all elements in the array. Return value is
      always a np.float64 or np.int64.



   .. py:method:: register(user_defined_name: str) -> pdarray

      Register this pdarray with a user defined name in the arkouda server
      so it can be attached to later using pdarray.attach()
      This is an in-place operation, registering a pdarray more than once will
      update the name in the registry and remove the previously registered name.
      A name can only be registered to one pdarray at a time.

      :param user_defined_name: user defined name array is to be registered under
      :type user_defined_name: str

      :returns: The same pdarray which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different pdarrays with the same name.
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the pdarray with the user_defined_name
          If the user is attempting to register more than one pdarray with the same name,
          the former should be unregistered first to free up the registration name.

      .. seealso:: :obj:`attach`, :obj:`unregister`, :obj:`is_registered`, :obj:`list_registry`, :obj:`unregister_pdarray_by_name`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reshape(*shape)

      Gives a new shape to an array without changing its data.

      :param shape: The new shape should be compatible with the original shape.
      :type shape: int, tuple of ints, or pdarray

      :returns: a pdarray with the same data, reshaped to the new shape
      :rtype: pdarray



   .. py:method:: rotl(other) -> pdarray

      Rotate bits left by <other>.



   .. py:method:: rotr(other) -> pdarray

      Rotate bits right by <other>.



   .. py:method:: save(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the pdarray to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. HDF5 support single files, in which case the file name will
      only be that provided. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str {'HDF5', 'Parquet'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: Raised if there is an error in parsing the prefix path pointing to
          file write location or if the mode parameter is neither truncate
          nor append
      :raises TypeError: Raised if any one of the prefix_path, dataset, or mode parameters
          is not a string

      .. seealso:: :obj:`save_all`, :obj:`load`, :obj:`read`, :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Notes

      The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales``. If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      Previously all files saved in Parquet format were saved with a ``.parquet`` file extension.
      This will require you to use load as if you saved the file with the extension. Try this if
      an older file is not being found.
      Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.save('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.save('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving with an extension (Parquet)
      >>> a.save('path/prefix.parquet', dataset='array', file_format='Parquet')
      Saves the array in numLocales Parquet files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:property:: shape

      Return the shape of an array.

      :returns: The elements of the shape tuple give the lengths of the corresponding array dimensions.
      :rtype: tuple of int


   .. py:attribute:: size


   .. py:method:: slice_bits(low, high) -> pdarray

      Returns a pdarray containing only bits from low to high of self.

      This is zero indexed and inclusive on both ends, so slicing the bottom 64 bits is
      pda.slice_bits(0, 63)

      :param low: The lowest bit included in the slice (inclusive)
                  zero indexed, so the first bit is 0
      :type low: int
      :param high: The highest bit included in the slice (inclusive)
      :type high: int

      :returns: A new pdarray containing the bits of self from low to high
      :rtype: pdarray

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. rubric:: Examples

      >>> p = ak.array([2**65 + (2**64 - 1)])
      >>> bin(p[0])
      '0b101111111111111111111111111111111111111111111111111111111111111111'

      >>> bin(p.slice_bits(64, 65)[0])
      '0b10'



   .. py:method:: std(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the standard deviation. See ``arkouda.std`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: The scalar standard deviation of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: sum(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the sum of all elements in the array.



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'array', col_delim: str = ',', overwrite: bool = False)

              Write pdarray to CSV file(s). File will contain a single column with the pdarray data.
              All CSV Files written by Arkouda include a header denoting data types of the columns.

              Parameters
              -----------
              prefix_path: str
                  The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                  when they are written to disk.
              dataset: str
                  Column name to save the pdarray under. Defaults to "array".
              col_delim: str
                  Defaults to ",". Value to be used to separate columns within the file.
                  Please be sure that the value used DOES NOT appear in your dataset.
              overwrite: bool
                  Defaults to False. If True, any existing files matching your provided prefix_path will
                  be overwritten. If False, an error will be returned if existing files are found.

              Returns
              --------
              str reponse message

              Raises
              ------
              ValueError
                  Raised if all datasets are not present in all parquet files or if one or
                  more of the specified files do not exist
              RuntimeError
                  Raised if one or more of the specified files cannot be opened.
                  If `allow_errors` is true this may be raised if no values are returned
                  from the server.
              TypeError
                  Raised if we receive an unknown arkouda_type returned from the server

              Notes
              ------
              - CSV format is not currently supported by load/load_all operations
              - The column delimiter is expected to be the same for column names and data
              - Be sure that column delimiters are not found within your data.
              - All CSV files must delimit rows using newline (`
      `) at this time.




   .. py:method:: to_cuda()

      Convert the array to a Numba DeviceND array, transferring array data from the
      arkouda server to Python via ndarray. If the array exceeds a builtin size limit,
      a RuntimeError is raised.

      :returns: A Numba ndarray with the same attributes and data as the pdarray; on GPU
      :rtype: numba.DeviceNDArray

      :raises ImportError: Raised if CUDA is not available
      :raises ModuleNotFoundError: Raised if Numba is either not installed or not enabled
      :raises RuntimeError: Raised if there is a server-side error thrown in the course of retrieving
          the pdarray.

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_cuda()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_cuda())
      numpy.devicendarray



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the pdarray to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_hdf('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_hdf('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving to a single file
      >>> a.to_hdf('path/prefix.hdf5', dataset='array', file_type='single')
      Saves the array in to single hdf5 file on the root node.
      ``cwd/path/name_prefix.hdf5``



   .. py:method:: to_list() -> List

      Convert the array to a list, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A list with the same data as the pdarray
      :rtype: list

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_list()
      [0, 1, 2, 3, 4]

      >>> type(a.to_list())
      list



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A numpy ndarray with the same attributes and data as the pdarray
      :rtype: np.ndarray

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_ndarray()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      Save the pdarray to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_parquet('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_parqet('path/prefix.parquet', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a pdarray to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the pdarray is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister() -> None

      Unregister a pdarray in the arkouda server which was previously
      registered using register() and/or attahced to using attach()


      :rtype: None

      :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion until
      they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'array', repack: bool = True)

      Overwrite the dataset with the name provided with this pdarray. If
      the dataset does not exist it is added

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added



   .. py:method:: value_counts()

      Count the occurrences of the unique values of self.

      :returns: * **unique_values** (*pdarray*) -- The unique values, sorted in ascending order
                * **counts** (*pdarray, int64*) -- The number of times the corresponding unique value occurs

      .. rubric:: Examples

      >>> ak.array([2, 0, 2, 4, 0, 0]).value_counts()
      (array([0, 2, 4]), array([3, 2, 1]))



   .. py:method:: var(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the variance. See ``arkouda.var`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: The scalar variance of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises ValueError: Raised if the ddof >= pdarray size
      :raises RuntimeError: Raised if there's a server-side error thrown



.. py:class:: pdarray(name: str, mydtype: Union[numpy.dtype, str], size: arkouda.numpy.dtypes.int_scalars, ndim: arkouda.numpy.dtypes.int_scalars, shape: Sequence[int], itemsize: arkouda.numpy.dtypes.int_scalars, max_bits: Optional[int] = None)

   The basic arkouda array class. This class contains only the
   attributies of the array; the data resides on the arkouda
   server. When a server operation results in a new array, arkouda
   will create a pdarray instance that points to the array data on
   the server. As such, the user should not initialize pdarray
   instances directly.

   .. attribute:: name

      The server-side identifier for the array

      :type: str

   .. attribute:: dtype

      The element type of the array

      :type: dtype

   .. attribute:: size

      The number of elements in the array

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      A list or tuple containing the sizes of each dimension of the array

      :type: Sequence[int]

   .. attribute:: itemsize

      The size in bytes of each element

      :type: int_scalars


   .. py:attribute:: BinOps


   .. py:attribute:: OpEqOps


   .. py:method:: all(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff all elements of the array evaluate to True.



   .. py:method:: any(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff any element of the array evaluates to True.



   .. py:method:: argmax(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array max value.



   .. py:method:: argmaxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Finds the indices corresponding to the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the  maximum `k` values, sorted
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: argmin(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array min value



   .. py:method:: argmink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: astype(dtype) -> pdarray

      Cast values of pdarray to provided dtype

      :param dtype: Dtype to cast to
      :type dtype: np.dtype or str

      :returns: An arkouda pdarray with values converted to the specified data type
      :rtype: ak.pdarray

      .. rubric:: Notes

      This is essentially shorthand for ak.cast(x, '<dtype>') where x is a pdarray.



   .. py:method:: attach(user_defined_name: str) -> pdarray
      :staticmethod:


      class method to return a pdarray attached to the registered name in the arkouda
      server which was registered using register()

      :param user_defined_name: user defined name which array was registered under
      :type user_defined_name: str

      :returns: pdarray which is bound to the corresponding server side component which was registered
                with user_defined_name
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: bigint_to_uint_arrays() -> List[pdarray]

      Creates a list of uint pdarrays from a bigint pdarray.
      The first item in return will be the highest 64 bits of the
      bigint pdarray and the last item will be the lowest 64 bits.

      :returns: A list of uint pdarrays where:
                The first item in return will be the highest 64 bits of the
                bigint pdarray and the last item will be the lowest 64 bits.
      :rtype: List[pdarrays]

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`pdarraycreation.bigint_from_uint_arrays`

      .. rubric:: Examples

      >>> a = ak.arange(2**64, 2**64 + 5)
      >>> a
      array(["18446744073709551616" "18446744073709551617" "18446744073709551618"
      "18446744073709551619" "18446744073709551620"])

      >>> a.bigint_to_uint_arrays()
      [array([1 1 1 1 1]), array([0 1 2 3 4])]



   .. py:method:: clz() -> pdarray

      Count the number of leading zeros in each element. See `ak.clz`.



   .. py:method:: corr(y: pdarray) -> numpy.float64

      Compute the correlation between self and y using pearson correlation coefficient.

      :param y: Other pdarray used to calculate correlation
      :type y: pdarray

      :returns: The scalar correlation of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: cov(y: pdarray) -> numpy.float64

      Compute the covariance between self and y.

      :param y: Other pdarray used to calculate covariance
      :type y: pdarray

      :returns: The scalar covariance of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: ctz() -> pdarray

      Count the number of trailing zeros in each element. See `ak.ctz`.



   .. py:attribute:: dtype


   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether pdarrays are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the pdarrays are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array([1, 2, 3])
      >>> a_cpy = ak.array([1, 2, 3])
      >>> a.equals(a_cpy)
      True
      >>> a2 = ak.array([1, 2, 5)
      >>> a.equals(a2)
      False



   .. py:method:: fill(value: arkouda.numpy.dtypes.numeric_scalars) -> None

      Fill the array (in place) with a constant value.

      :param value:
      :type value: numeric_scalars

      :raises TypeError: Raised if value is not an int, int64, float, or float64



   .. py:method:: flatten()

      Return a copy of the array collapsed into one dimension.

      :rtype: A copy of the input array, flattened to one dimension.



   .. py:method:: format_other(other) -> str

      Attempt to cast scalar other to the element dtype of this pdarray,
      and print the resulting value to a string (e.g. for sending to a
      server command). The user should not call this function directly.

      :param other: The scalar to be cast to the pdarray.dtype
      :type other: object

      :rtype: string representation of np.dtype corresponding to the other parameter

      :raises TypeError: Raised if the other parameter cannot be converted to
          Numpy dtype



   .. py:property:: inferred_type
      :type: Union[str, None]


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry

      :param None:

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RuntimeError: Raised if there's a server-side error thrown

      .. note::

         This will return True if the object is registered itself or as a component
         of another object



   .. py:method:: is_sorted(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff the array is monotonically non-decreasing.

      :param None:

      :returns: Indicates if the array is monotonically non-decreasing
      :rtype: bool

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:attribute:: itemsize


   .. py:method:: max(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the maximum value of the array.



   .. py:property:: max_bits


   .. py:method:: maxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: mean() -> numpy.float64

      Return the mean of the array.



   .. py:method:: min(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the minimum value of the array.



   .. py:method:: mink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:attribute:: name


   .. py:property:: nbytes

      The size of the pdarray in bytes.

      :returns: The size of the pdarray in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: objType
      :value: 'pdarray'



   .. py:method:: opeq(other, op)


   .. py:method:: parity() -> pdarray

      Find the parity (XOR of all bits) in each element. See `ak.parity`.



   .. py:method:: popcount() -> pdarray

      Find the population (number of bits set) in each element. See `ak.popcount`.



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: prod(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the product of all elements in the array. Return value is
      always a np.float64 or np.int64.



   .. py:method:: register(user_defined_name: str) -> pdarray

      Register this pdarray with a user defined name in the arkouda server
      so it can be attached to later using pdarray.attach()
      This is an in-place operation, registering a pdarray more than once will
      update the name in the registry and remove the previously registered name.
      A name can only be registered to one pdarray at a time.

      :param user_defined_name: user defined name array is to be registered under
      :type user_defined_name: str

      :returns: The same pdarray which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different pdarrays with the same name.
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the pdarray with the user_defined_name
          If the user is attempting to register more than one pdarray with the same name,
          the former should be unregistered first to free up the registration name.

      .. seealso:: :obj:`attach`, :obj:`unregister`, :obj:`is_registered`, :obj:`list_registry`, :obj:`unregister_pdarray_by_name`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reshape(*shape)

      Gives a new shape to an array without changing its data.

      :param shape: The new shape should be compatible with the original shape.
      :type shape: int, tuple of ints, or pdarray

      :returns: a pdarray with the same data, reshaped to the new shape
      :rtype: pdarray



   .. py:method:: rotl(other) -> pdarray

      Rotate bits left by <other>.



   .. py:method:: rotr(other) -> pdarray

      Rotate bits right by <other>.



   .. py:method:: save(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the pdarray to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. HDF5 support single files, in which case the file name will
      only be that provided. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str {'HDF5', 'Parquet'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: Raised if there is an error in parsing the prefix path pointing to
          file write location or if the mode parameter is neither truncate
          nor append
      :raises TypeError: Raised if any one of the prefix_path, dataset, or mode parameters
          is not a string

      .. seealso:: :obj:`save_all`, :obj:`load`, :obj:`read`, :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Notes

      The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales``. If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      Previously all files saved in Parquet format were saved with a ``.parquet`` file extension.
      This will require you to use load as if you saved the file with the extension. Try this if
      an older file is not being found.
      Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.save('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.save('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving with an extension (Parquet)
      >>> a.save('path/prefix.parquet', dataset='array', file_format='Parquet')
      Saves the array in numLocales Parquet files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:property:: shape

      Return the shape of an array.

      :returns: The elements of the shape tuple give the lengths of the corresponding array dimensions.
      :rtype: tuple of int


   .. py:attribute:: size


   .. py:method:: slice_bits(low, high) -> pdarray

      Returns a pdarray containing only bits from low to high of self.

      This is zero indexed and inclusive on both ends, so slicing the bottom 64 bits is
      pda.slice_bits(0, 63)

      :param low: The lowest bit included in the slice (inclusive)
                  zero indexed, so the first bit is 0
      :type low: int
      :param high: The highest bit included in the slice (inclusive)
      :type high: int

      :returns: A new pdarray containing the bits of self from low to high
      :rtype: pdarray

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. rubric:: Examples

      >>> p = ak.array([2**65 + (2**64 - 1)])
      >>> bin(p[0])
      '0b101111111111111111111111111111111111111111111111111111111111111111'

      >>> bin(p.slice_bits(64, 65)[0])
      '0b10'



   .. py:method:: std(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the standard deviation. See ``arkouda.std`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: The scalar standard deviation of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: sum(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the sum of all elements in the array.



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'array', col_delim: str = ',', overwrite: bool = False)

              Write pdarray to CSV file(s). File will contain a single column with the pdarray data.
              All CSV Files written by Arkouda include a header denoting data types of the columns.

              Parameters
              -----------
              prefix_path: str
                  The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                  when they are written to disk.
              dataset: str
                  Column name to save the pdarray under. Defaults to "array".
              col_delim: str
                  Defaults to ",". Value to be used to separate columns within the file.
                  Please be sure that the value used DOES NOT appear in your dataset.
              overwrite: bool
                  Defaults to False. If True, any existing files matching your provided prefix_path will
                  be overwritten. If False, an error will be returned if existing files are found.

              Returns
              --------
              str reponse message

              Raises
              ------
              ValueError
                  Raised if all datasets are not present in all parquet files or if one or
                  more of the specified files do not exist
              RuntimeError
                  Raised if one or more of the specified files cannot be opened.
                  If `allow_errors` is true this may be raised if no values are returned
                  from the server.
              TypeError
                  Raised if we receive an unknown arkouda_type returned from the server

              Notes
              ------
              - CSV format is not currently supported by load/load_all operations
              - The column delimiter is expected to be the same for column names and data
              - Be sure that column delimiters are not found within your data.
              - All CSV files must delimit rows using newline (`
      `) at this time.




   .. py:method:: to_cuda()

      Convert the array to a Numba DeviceND array, transferring array data from the
      arkouda server to Python via ndarray. If the array exceeds a builtin size limit,
      a RuntimeError is raised.

      :returns: A Numba ndarray with the same attributes and data as the pdarray; on GPU
      :rtype: numba.DeviceNDArray

      :raises ImportError: Raised if CUDA is not available
      :raises ModuleNotFoundError: Raised if Numba is either not installed or not enabled
      :raises RuntimeError: Raised if there is a server-side error thrown in the course of retrieving
          the pdarray.

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_cuda()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_cuda())
      numpy.devicendarray



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the pdarray to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_hdf('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_hdf('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving to a single file
      >>> a.to_hdf('path/prefix.hdf5', dataset='array', file_type='single')
      Saves the array in to single hdf5 file on the root node.
      ``cwd/path/name_prefix.hdf5``



   .. py:method:: to_list() -> List

      Convert the array to a list, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A list with the same data as the pdarray
      :rtype: list

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_list()
      [0, 1, 2, 3, 4]

      >>> type(a.to_list())
      list



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A numpy ndarray with the same attributes and data as the pdarray
      :rtype: np.ndarray

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_ndarray()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      Save the pdarray to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_parquet('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_parqet('path/prefix.parquet', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a pdarray to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the pdarray is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister() -> None

      Unregister a pdarray in the arkouda server which was previously
      registered using register() and/or attahced to using attach()


      :rtype: None

      :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion until
      they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'array', repack: bool = True)

      Overwrite the dataset with the name provided with this pdarray. If
      the dataset does not exist it is added

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added



   .. py:method:: value_counts()

      Count the occurrences of the unique values of self.

      :returns: * **unique_values** (*pdarray*) -- The unique values, sorted in ascending order
                * **counts** (*pdarray, int64*) -- The number of times the corresponding unique value occurs

      .. rubric:: Examples

      >>> ak.array([2, 0, 2, 4, 0, 0]).value_counts()
      (array([0, 2, 4]), array([3, 2, 1]))



   .. py:method:: var(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the variance. See ``arkouda.var`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: The scalar variance of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises ValueError: Raised if the ddof >= pdarray size
      :raises RuntimeError: Raised if there's a server-side error thrown



.. py:class:: pdarray(name: str, mydtype: Union[numpy.dtype, str], size: arkouda.numpy.dtypes.int_scalars, ndim: arkouda.numpy.dtypes.int_scalars, shape: Sequence[int], itemsize: arkouda.numpy.dtypes.int_scalars, max_bits: Optional[int] = None)

   The basic arkouda array class. This class contains only the
   attributies of the array; the data resides on the arkouda
   server. When a server operation results in a new array, arkouda
   will create a pdarray instance that points to the array data on
   the server. As such, the user should not initialize pdarray
   instances directly.

   .. attribute:: name

      The server-side identifier for the array

      :type: str

   .. attribute:: dtype

      The element type of the array

      :type: dtype

   .. attribute:: size

      The number of elements in the array

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      A list or tuple containing the sizes of each dimension of the array

      :type: Sequence[int]

   .. attribute:: itemsize

      The size in bytes of each element

      :type: int_scalars


   .. py:attribute:: BinOps


   .. py:attribute:: OpEqOps


   .. py:method:: all(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff all elements of the array evaluate to True.



   .. py:method:: any(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff any element of the array evaluates to True.



   .. py:method:: argmax(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array max value.



   .. py:method:: argmaxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Finds the indices corresponding to the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the  maximum `k` values, sorted
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: argmin(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array min value



   .. py:method:: argmink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: astype(dtype) -> pdarray

      Cast values of pdarray to provided dtype

      :param dtype: Dtype to cast to
      :type dtype: np.dtype or str

      :returns: An arkouda pdarray with values converted to the specified data type
      :rtype: ak.pdarray

      .. rubric:: Notes

      This is essentially shorthand for ak.cast(x, '<dtype>') where x is a pdarray.



   .. py:method:: attach(user_defined_name: str) -> pdarray
      :staticmethod:


      class method to return a pdarray attached to the registered name in the arkouda
      server which was registered using register()

      :param user_defined_name: user defined name which array was registered under
      :type user_defined_name: str

      :returns: pdarray which is bound to the corresponding server side component which was registered
                with user_defined_name
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: bigint_to_uint_arrays() -> List[pdarray]

      Creates a list of uint pdarrays from a bigint pdarray.
      The first item in return will be the highest 64 bits of the
      bigint pdarray and the last item will be the lowest 64 bits.

      :returns: A list of uint pdarrays where:
                The first item in return will be the highest 64 bits of the
                bigint pdarray and the last item will be the lowest 64 bits.
      :rtype: List[pdarrays]

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`pdarraycreation.bigint_from_uint_arrays`

      .. rubric:: Examples

      >>> a = ak.arange(2**64, 2**64 + 5)
      >>> a
      array(["18446744073709551616" "18446744073709551617" "18446744073709551618"
      "18446744073709551619" "18446744073709551620"])

      >>> a.bigint_to_uint_arrays()
      [array([1 1 1 1 1]), array([0 1 2 3 4])]



   .. py:method:: clz() -> pdarray

      Count the number of leading zeros in each element. See `ak.clz`.



   .. py:method:: corr(y: pdarray) -> numpy.float64

      Compute the correlation between self and y using pearson correlation coefficient.

      :param y: Other pdarray used to calculate correlation
      :type y: pdarray

      :returns: The scalar correlation of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: cov(y: pdarray) -> numpy.float64

      Compute the covariance between self and y.

      :param y: Other pdarray used to calculate covariance
      :type y: pdarray

      :returns: The scalar covariance of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: ctz() -> pdarray

      Count the number of trailing zeros in each element. See `ak.ctz`.



   .. py:attribute:: dtype


   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether pdarrays are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the pdarrays are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array([1, 2, 3])
      >>> a_cpy = ak.array([1, 2, 3])
      >>> a.equals(a_cpy)
      True
      >>> a2 = ak.array([1, 2, 5)
      >>> a.equals(a2)
      False



   .. py:method:: fill(value: arkouda.numpy.dtypes.numeric_scalars) -> None

      Fill the array (in place) with a constant value.

      :param value:
      :type value: numeric_scalars

      :raises TypeError: Raised if value is not an int, int64, float, or float64



   .. py:method:: flatten()

      Return a copy of the array collapsed into one dimension.

      :rtype: A copy of the input array, flattened to one dimension.



   .. py:method:: format_other(other) -> str

      Attempt to cast scalar other to the element dtype of this pdarray,
      and print the resulting value to a string (e.g. for sending to a
      server command). The user should not call this function directly.

      :param other: The scalar to be cast to the pdarray.dtype
      :type other: object

      :rtype: string representation of np.dtype corresponding to the other parameter

      :raises TypeError: Raised if the other parameter cannot be converted to
          Numpy dtype



   .. py:property:: inferred_type
      :type: Union[str, None]


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry

      :param None:

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RuntimeError: Raised if there's a server-side error thrown

      .. note::

         This will return True if the object is registered itself or as a component
         of another object



   .. py:method:: is_sorted(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff the array is monotonically non-decreasing.

      :param None:

      :returns: Indicates if the array is monotonically non-decreasing
      :rtype: bool

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:attribute:: itemsize


   .. py:method:: max(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the maximum value of the array.



   .. py:property:: max_bits


   .. py:method:: maxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: mean() -> numpy.float64

      Return the mean of the array.



   .. py:method:: min(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the minimum value of the array.



   .. py:method:: mink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:attribute:: name


   .. py:property:: nbytes

      The size of the pdarray in bytes.

      :returns: The size of the pdarray in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: objType
      :value: 'pdarray'



   .. py:method:: opeq(other, op)


   .. py:method:: parity() -> pdarray

      Find the parity (XOR of all bits) in each element. See `ak.parity`.



   .. py:method:: popcount() -> pdarray

      Find the population (number of bits set) in each element. See `ak.popcount`.



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: prod(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the product of all elements in the array. Return value is
      always a np.float64 or np.int64.



   .. py:method:: register(user_defined_name: str) -> pdarray

      Register this pdarray with a user defined name in the arkouda server
      so it can be attached to later using pdarray.attach()
      This is an in-place operation, registering a pdarray more than once will
      update the name in the registry and remove the previously registered name.
      A name can only be registered to one pdarray at a time.

      :param user_defined_name: user defined name array is to be registered under
      :type user_defined_name: str

      :returns: The same pdarray which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different pdarrays with the same name.
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the pdarray with the user_defined_name
          If the user is attempting to register more than one pdarray with the same name,
          the former should be unregistered first to free up the registration name.

      .. seealso:: :obj:`attach`, :obj:`unregister`, :obj:`is_registered`, :obj:`list_registry`, :obj:`unregister_pdarray_by_name`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reshape(*shape)

      Gives a new shape to an array without changing its data.

      :param shape: The new shape should be compatible with the original shape.
      :type shape: int, tuple of ints, or pdarray

      :returns: a pdarray with the same data, reshaped to the new shape
      :rtype: pdarray



   .. py:method:: rotl(other) -> pdarray

      Rotate bits left by <other>.



   .. py:method:: rotr(other) -> pdarray

      Rotate bits right by <other>.



   .. py:method:: save(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the pdarray to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. HDF5 support single files, in which case the file name will
      only be that provided. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str {'HDF5', 'Parquet'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: Raised if there is an error in parsing the prefix path pointing to
          file write location or if the mode parameter is neither truncate
          nor append
      :raises TypeError: Raised if any one of the prefix_path, dataset, or mode parameters
          is not a string

      .. seealso:: :obj:`save_all`, :obj:`load`, :obj:`read`, :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Notes

      The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales``. If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      Previously all files saved in Parquet format were saved with a ``.parquet`` file extension.
      This will require you to use load as if you saved the file with the extension. Try this if
      an older file is not being found.
      Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.save('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.save('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving with an extension (Parquet)
      >>> a.save('path/prefix.parquet', dataset='array', file_format='Parquet')
      Saves the array in numLocales Parquet files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:property:: shape

      Return the shape of an array.

      :returns: The elements of the shape tuple give the lengths of the corresponding array dimensions.
      :rtype: tuple of int


   .. py:attribute:: size


   .. py:method:: slice_bits(low, high) -> pdarray

      Returns a pdarray containing only bits from low to high of self.

      This is zero indexed and inclusive on both ends, so slicing the bottom 64 bits is
      pda.slice_bits(0, 63)

      :param low: The lowest bit included in the slice (inclusive)
                  zero indexed, so the first bit is 0
      :type low: int
      :param high: The highest bit included in the slice (inclusive)
      :type high: int

      :returns: A new pdarray containing the bits of self from low to high
      :rtype: pdarray

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. rubric:: Examples

      >>> p = ak.array([2**65 + (2**64 - 1)])
      >>> bin(p[0])
      '0b101111111111111111111111111111111111111111111111111111111111111111'

      >>> bin(p.slice_bits(64, 65)[0])
      '0b10'



   .. py:method:: std(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the standard deviation. See ``arkouda.std`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: The scalar standard deviation of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: sum(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the sum of all elements in the array.



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'array', col_delim: str = ',', overwrite: bool = False)

              Write pdarray to CSV file(s). File will contain a single column with the pdarray data.
              All CSV Files written by Arkouda include a header denoting data types of the columns.

              Parameters
              -----------
              prefix_path: str
                  The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                  when they are written to disk.
              dataset: str
                  Column name to save the pdarray under. Defaults to "array".
              col_delim: str
                  Defaults to ",". Value to be used to separate columns within the file.
                  Please be sure that the value used DOES NOT appear in your dataset.
              overwrite: bool
                  Defaults to False. If True, any existing files matching your provided prefix_path will
                  be overwritten. If False, an error will be returned if existing files are found.

              Returns
              --------
              str reponse message

              Raises
              ------
              ValueError
                  Raised if all datasets are not present in all parquet files or if one or
                  more of the specified files do not exist
              RuntimeError
                  Raised if one or more of the specified files cannot be opened.
                  If `allow_errors` is true this may be raised if no values are returned
                  from the server.
              TypeError
                  Raised if we receive an unknown arkouda_type returned from the server

              Notes
              ------
              - CSV format is not currently supported by load/load_all operations
              - The column delimiter is expected to be the same for column names and data
              - Be sure that column delimiters are not found within your data.
              - All CSV files must delimit rows using newline (`
      `) at this time.




   .. py:method:: to_cuda()

      Convert the array to a Numba DeviceND array, transferring array data from the
      arkouda server to Python via ndarray. If the array exceeds a builtin size limit,
      a RuntimeError is raised.

      :returns: A Numba ndarray with the same attributes and data as the pdarray; on GPU
      :rtype: numba.DeviceNDArray

      :raises ImportError: Raised if CUDA is not available
      :raises ModuleNotFoundError: Raised if Numba is either not installed or not enabled
      :raises RuntimeError: Raised if there is a server-side error thrown in the course of retrieving
          the pdarray.

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_cuda()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_cuda())
      numpy.devicendarray



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the pdarray to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_hdf('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_hdf('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving to a single file
      >>> a.to_hdf('path/prefix.hdf5', dataset='array', file_type='single')
      Saves the array in to single hdf5 file on the root node.
      ``cwd/path/name_prefix.hdf5``



   .. py:method:: to_list() -> List

      Convert the array to a list, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A list with the same data as the pdarray
      :rtype: list

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_list()
      [0, 1, 2, 3, 4]

      >>> type(a.to_list())
      list



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A numpy ndarray with the same attributes and data as the pdarray
      :rtype: np.ndarray

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_ndarray()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      Save the pdarray to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_parquet('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_parqet('path/prefix.parquet', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a pdarray to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the pdarray is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister() -> None

      Unregister a pdarray in the arkouda server which was previously
      registered using register() and/or attahced to using attach()


      :rtype: None

      :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion until
      they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'array', repack: bool = True)

      Overwrite the dataset with the name provided with this pdarray. If
      the dataset does not exist it is added

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added



   .. py:method:: value_counts()

      Count the occurrences of the unique values of self.

      :returns: * **unique_values** (*pdarray*) -- The unique values, sorted in ascending order
                * **counts** (*pdarray, int64*) -- The number of times the corresponding unique value occurs

      .. rubric:: Examples

      >>> ak.array([2, 0, 2, 4, 0, 0]).value_counts()
      (array([0, 2, 4]), array([3, 2, 1]))



   .. py:method:: var(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the variance. See ``arkouda.var`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: The scalar variance of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises ValueError: Raised if the ddof >= pdarray size
      :raises RuntimeError: Raised if there's a server-side error thrown



.. py:class:: pdarray(name: str, mydtype: Union[numpy.dtype, str], size: arkouda.numpy.dtypes.int_scalars, ndim: arkouda.numpy.dtypes.int_scalars, shape: Sequence[int], itemsize: arkouda.numpy.dtypes.int_scalars, max_bits: Optional[int] = None)

   The basic arkouda array class. This class contains only the
   attributies of the array; the data resides on the arkouda
   server. When a server operation results in a new array, arkouda
   will create a pdarray instance that points to the array data on
   the server. As such, the user should not initialize pdarray
   instances directly.

   .. attribute:: name

      The server-side identifier for the array

      :type: str

   .. attribute:: dtype

      The element type of the array

      :type: dtype

   .. attribute:: size

      The number of elements in the array

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      A list or tuple containing the sizes of each dimension of the array

      :type: Sequence[int]

   .. attribute:: itemsize

      The size in bytes of each element

      :type: int_scalars


   .. py:attribute:: BinOps


   .. py:attribute:: OpEqOps


   .. py:method:: all(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff all elements of the array evaluate to True.



   .. py:method:: any(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff any element of the array evaluates to True.



   .. py:method:: argmax(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array max value.



   .. py:method:: argmaxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Finds the indices corresponding to the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the  maximum `k` values, sorted
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: argmin(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array min value



   .. py:method:: argmink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: astype(dtype) -> pdarray

      Cast values of pdarray to provided dtype

      :param dtype: Dtype to cast to
      :type dtype: np.dtype or str

      :returns: An arkouda pdarray with values converted to the specified data type
      :rtype: ak.pdarray

      .. rubric:: Notes

      This is essentially shorthand for ak.cast(x, '<dtype>') where x is a pdarray.



   .. py:method:: attach(user_defined_name: str) -> pdarray
      :staticmethod:


      class method to return a pdarray attached to the registered name in the arkouda
      server which was registered using register()

      :param user_defined_name: user defined name which array was registered under
      :type user_defined_name: str

      :returns: pdarray which is bound to the corresponding server side component which was registered
                with user_defined_name
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: bigint_to_uint_arrays() -> List[pdarray]

      Creates a list of uint pdarrays from a bigint pdarray.
      The first item in return will be the highest 64 bits of the
      bigint pdarray and the last item will be the lowest 64 bits.

      :returns: A list of uint pdarrays where:
                The first item in return will be the highest 64 bits of the
                bigint pdarray and the last item will be the lowest 64 bits.
      :rtype: List[pdarrays]

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`pdarraycreation.bigint_from_uint_arrays`

      .. rubric:: Examples

      >>> a = ak.arange(2**64, 2**64 + 5)
      >>> a
      array(["18446744073709551616" "18446744073709551617" "18446744073709551618"
      "18446744073709551619" "18446744073709551620"])

      >>> a.bigint_to_uint_arrays()
      [array([1 1 1 1 1]), array([0 1 2 3 4])]



   .. py:method:: clz() -> pdarray

      Count the number of leading zeros in each element. See `ak.clz`.



   .. py:method:: corr(y: pdarray) -> numpy.float64

      Compute the correlation between self and y using pearson correlation coefficient.

      :param y: Other pdarray used to calculate correlation
      :type y: pdarray

      :returns: The scalar correlation of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: cov(y: pdarray) -> numpy.float64

      Compute the covariance between self and y.

      :param y: Other pdarray used to calculate covariance
      :type y: pdarray

      :returns: The scalar covariance of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: ctz() -> pdarray

      Count the number of trailing zeros in each element. See `ak.ctz`.



   .. py:attribute:: dtype


   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether pdarrays are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the pdarrays are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array([1, 2, 3])
      >>> a_cpy = ak.array([1, 2, 3])
      >>> a.equals(a_cpy)
      True
      >>> a2 = ak.array([1, 2, 5)
      >>> a.equals(a2)
      False



   .. py:method:: fill(value: arkouda.numpy.dtypes.numeric_scalars) -> None

      Fill the array (in place) with a constant value.

      :param value:
      :type value: numeric_scalars

      :raises TypeError: Raised if value is not an int, int64, float, or float64



   .. py:method:: flatten()

      Return a copy of the array collapsed into one dimension.

      :rtype: A copy of the input array, flattened to one dimension.



   .. py:method:: format_other(other) -> str

      Attempt to cast scalar other to the element dtype of this pdarray,
      and print the resulting value to a string (e.g. for sending to a
      server command). The user should not call this function directly.

      :param other: The scalar to be cast to the pdarray.dtype
      :type other: object

      :rtype: string representation of np.dtype corresponding to the other parameter

      :raises TypeError: Raised if the other parameter cannot be converted to
          Numpy dtype



   .. py:property:: inferred_type
      :type: Union[str, None]


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry

      :param None:

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RuntimeError: Raised if there's a server-side error thrown

      .. note::

         This will return True if the object is registered itself or as a component
         of another object



   .. py:method:: is_sorted(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff the array is monotonically non-decreasing.

      :param None:

      :returns: Indicates if the array is monotonically non-decreasing
      :rtype: bool

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:attribute:: itemsize


   .. py:method:: max(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the maximum value of the array.



   .. py:property:: max_bits


   .. py:method:: maxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: mean() -> numpy.float64

      Return the mean of the array.



   .. py:method:: min(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the minimum value of the array.



   .. py:method:: mink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:attribute:: name


   .. py:property:: nbytes

      The size of the pdarray in bytes.

      :returns: The size of the pdarray in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: objType
      :value: 'pdarray'



   .. py:method:: opeq(other, op)


   .. py:method:: parity() -> pdarray

      Find the parity (XOR of all bits) in each element. See `ak.parity`.



   .. py:method:: popcount() -> pdarray

      Find the population (number of bits set) in each element. See `ak.popcount`.



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: prod(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the product of all elements in the array. Return value is
      always a np.float64 or np.int64.



   .. py:method:: register(user_defined_name: str) -> pdarray

      Register this pdarray with a user defined name in the arkouda server
      so it can be attached to later using pdarray.attach()
      This is an in-place operation, registering a pdarray more than once will
      update the name in the registry and remove the previously registered name.
      A name can only be registered to one pdarray at a time.

      :param user_defined_name: user defined name array is to be registered under
      :type user_defined_name: str

      :returns: The same pdarray which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different pdarrays with the same name.
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the pdarray with the user_defined_name
          If the user is attempting to register more than one pdarray with the same name,
          the former should be unregistered first to free up the registration name.

      .. seealso:: :obj:`attach`, :obj:`unregister`, :obj:`is_registered`, :obj:`list_registry`, :obj:`unregister_pdarray_by_name`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reshape(*shape)

      Gives a new shape to an array without changing its data.

      :param shape: The new shape should be compatible with the original shape.
      :type shape: int, tuple of ints, or pdarray

      :returns: a pdarray with the same data, reshaped to the new shape
      :rtype: pdarray



   .. py:method:: rotl(other) -> pdarray

      Rotate bits left by <other>.



   .. py:method:: rotr(other) -> pdarray

      Rotate bits right by <other>.



   .. py:method:: save(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the pdarray to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. HDF5 support single files, in which case the file name will
      only be that provided. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str {'HDF5', 'Parquet'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: Raised if there is an error in parsing the prefix path pointing to
          file write location or if the mode parameter is neither truncate
          nor append
      :raises TypeError: Raised if any one of the prefix_path, dataset, or mode parameters
          is not a string

      .. seealso:: :obj:`save_all`, :obj:`load`, :obj:`read`, :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Notes

      The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales``. If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      Previously all files saved in Parquet format were saved with a ``.parquet`` file extension.
      This will require you to use load as if you saved the file with the extension. Try this if
      an older file is not being found.
      Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.save('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.save('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving with an extension (Parquet)
      >>> a.save('path/prefix.parquet', dataset='array', file_format='Parquet')
      Saves the array in numLocales Parquet files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:property:: shape

      Return the shape of an array.

      :returns: The elements of the shape tuple give the lengths of the corresponding array dimensions.
      :rtype: tuple of int


   .. py:attribute:: size


   .. py:method:: slice_bits(low, high) -> pdarray

      Returns a pdarray containing only bits from low to high of self.

      This is zero indexed and inclusive on both ends, so slicing the bottom 64 bits is
      pda.slice_bits(0, 63)

      :param low: The lowest bit included in the slice (inclusive)
                  zero indexed, so the first bit is 0
      :type low: int
      :param high: The highest bit included in the slice (inclusive)
      :type high: int

      :returns: A new pdarray containing the bits of self from low to high
      :rtype: pdarray

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. rubric:: Examples

      >>> p = ak.array([2**65 + (2**64 - 1)])
      >>> bin(p[0])
      '0b101111111111111111111111111111111111111111111111111111111111111111'

      >>> bin(p.slice_bits(64, 65)[0])
      '0b10'



   .. py:method:: std(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the standard deviation. See ``arkouda.std`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: The scalar standard deviation of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: sum(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the sum of all elements in the array.



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'array', col_delim: str = ',', overwrite: bool = False)

              Write pdarray to CSV file(s). File will contain a single column with the pdarray data.
              All CSV Files written by Arkouda include a header denoting data types of the columns.

              Parameters
              -----------
              prefix_path: str
                  The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                  when they are written to disk.
              dataset: str
                  Column name to save the pdarray under. Defaults to "array".
              col_delim: str
                  Defaults to ",". Value to be used to separate columns within the file.
                  Please be sure that the value used DOES NOT appear in your dataset.
              overwrite: bool
                  Defaults to False. If True, any existing files matching your provided prefix_path will
                  be overwritten. If False, an error will be returned if existing files are found.

              Returns
              --------
              str reponse message

              Raises
              ------
              ValueError
                  Raised if all datasets are not present in all parquet files or if one or
                  more of the specified files do not exist
              RuntimeError
                  Raised if one or more of the specified files cannot be opened.
                  If `allow_errors` is true this may be raised if no values are returned
                  from the server.
              TypeError
                  Raised if we receive an unknown arkouda_type returned from the server

              Notes
              ------
              - CSV format is not currently supported by load/load_all operations
              - The column delimiter is expected to be the same for column names and data
              - Be sure that column delimiters are not found within your data.
              - All CSV files must delimit rows using newline (`
      `) at this time.




   .. py:method:: to_cuda()

      Convert the array to a Numba DeviceND array, transferring array data from the
      arkouda server to Python via ndarray. If the array exceeds a builtin size limit,
      a RuntimeError is raised.

      :returns: A Numba ndarray with the same attributes and data as the pdarray; on GPU
      :rtype: numba.DeviceNDArray

      :raises ImportError: Raised if CUDA is not available
      :raises ModuleNotFoundError: Raised if Numba is either not installed or not enabled
      :raises RuntimeError: Raised if there is a server-side error thrown in the course of retrieving
          the pdarray.

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_cuda()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_cuda())
      numpy.devicendarray



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the pdarray to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_hdf('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_hdf('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving to a single file
      >>> a.to_hdf('path/prefix.hdf5', dataset='array', file_type='single')
      Saves the array in to single hdf5 file on the root node.
      ``cwd/path/name_prefix.hdf5``



   .. py:method:: to_list() -> List

      Convert the array to a list, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A list with the same data as the pdarray
      :rtype: list

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_list()
      [0, 1, 2, 3, 4]

      >>> type(a.to_list())
      list



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A numpy ndarray with the same attributes and data as the pdarray
      :rtype: np.ndarray

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_ndarray()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      Save the pdarray to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_parquet('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_parqet('path/prefix.parquet', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a pdarray to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the pdarray is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister() -> None

      Unregister a pdarray in the arkouda server which was previously
      registered using register() and/or attahced to using attach()


      :rtype: None

      :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion until
      they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'array', repack: bool = True)

      Overwrite the dataset with the name provided with this pdarray. If
      the dataset does not exist it is added

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added



   .. py:method:: value_counts()

      Count the occurrences of the unique values of self.

      :returns: * **unique_values** (*pdarray*) -- The unique values, sorted in ascending order
                * **counts** (*pdarray, int64*) -- The number of times the corresponding unique value occurs

      .. rubric:: Examples

      >>> ak.array([2, 0, 2, 4, 0, 0]).value_counts()
      (array([0, 2, 4]), array([3, 2, 1]))



   .. py:method:: var(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the variance. See ``arkouda.var`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: The scalar variance of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises ValueError: Raised if the ddof >= pdarray size
      :raises RuntimeError: Raised if there's a server-side error thrown



.. py:class:: pdarray(name: str, mydtype: Union[numpy.dtype, str], size: arkouda.numpy.dtypes.int_scalars, ndim: arkouda.numpy.dtypes.int_scalars, shape: Sequence[int], itemsize: arkouda.numpy.dtypes.int_scalars, max_bits: Optional[int] = None)

   The basic arkouda array class. This class contains only the
   attributies of the array; the data resides on the arkouda
   server. When a server operation results in a new array, arkouda
   will create a pdarray instance that points to the array data on
   the server. As such, the user should not initialize pdarray
   instances directly.

   .. attribute:: name

      The server-side identifier for the array

      :type: str

   .. attribute:: dtype

      The element type of the array

      :type: dtype

   .. attribute:: size

      The number of elements in the array

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      A list or tuple containing the sizes of each dimension of the array

      :type: Sequence[int]

   .. attribute:: itemsize

      The size in bytes of each element

      :type: int_scalars


   .. py:attribute:: BinOps


   .. py:attribute:: OpEqOps


   .. py:method:: all(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff all elements of the array evaluate to True.



   .. py:method:: any(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff any element of the array evaluates to True.



   .. py:method:: argmax(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array max value.



   .. py:method:: argmaxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Finds the indices corresponding to the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the  maximum `k` values, sorted
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: argmin(axis: Optional[Union[int, None]] = None, keepdims: bool = False) -> Union[numpy.int64, numpy.uint64, pdarray]

      Return the index of the first occurrence of the array min value



   .. py:method:: argmink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: astype(dtype) -> pdarray

      Cast values of pdarray to provided dtype

      :param dtype: Dtype to cast to
      :type dtype: np.dtype or str

      :returns: An arkouda pdarray with values converted to the specified data type
      :rtype: ak.pdarray

      .. rubric:: Notes

      This is essentially shorthand for ak.cast(x, '<dtype>') where x is a pdarray.



   .. py:method:: attach(user_defined_name: str) -> pdarray
      :staticmethod:


      class method to return a pdarray attached to the registered name in the arkouda
      server which was registered using register()

      :param user_defined_name: user defined name which array was registered under
      :type user_defined_name: str

      :returns: pdarray which is bound to the corresponding server side component which was registered
                with user_defined_name
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: bigint_to_uint_arrays() -> List[pdarray]

      Creates a list of uint pdarrays from a bigint pdarray.
      The first item in return will be the highest 64 bits of the
      bigint pdarray and the last item will be the lowest 64 bits.

      :returns: A list of uint pdarrays where:
                The first item in return will be the highest 64 bits of the
                bigint pdarray and the last item will be the lowest 64 bits.
      :rtype: List[pdarrays]

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`pdarraycreation.bigint_from_uint_arrays`

      .. rubric:: Examples

      >>> a = ak.arange(2**64, 2**64 + 5)
      >>> a
      array(["18446744073709551616" "18446744073709551617" "18446744073709551618"
      "18446744073709551619" "18446744073709551620"])

      >>> a.bigint_to_uint_arrays()
      [array([1 1 1 1 1]), array([0 1 2 3 4])]



   .. py:method:: clz() -> pdarray

      Count the number of leading zeros in each element. See `ak.clz`.



   .. py:method:: corr(y: pdarray) -> numpy.float64

      Compute the correlation between self and y using pearson correlation coefficient.

      :param y: Other pdarray used to calculate correlation
      :type y: pdarray

      :returns: The scalar correlation of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: cov(y: pdarray) -> numpy.float64

      Compute the covariance between self and y.

      :param y: Other pdarray used to calculate covariance
      :type y: pdarray

      :returns: The scalar covariance of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: ctz() -> pdarray

      Count the number of trailing zeros in each element. See `ak.ctz`.



   .. py:attribute:: dtype


   .. py:method:: equals(other) -> arkouda.numpy.dtypes.bool_scalars

      Whether pdarrays are the same size and all entries are equal.

      :param other: object to compare.
      :type other: object

      :returns: True if the pdarrays are the same, o.w. False.
      :rtype: bool

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array([1, 2, 3])
      >>> a_cpy = ak.array([1, 2, 3])
      >>> a.equals(a_cpy)
      True
      >>> a2 = ak.array([1, 2, 5)
      >>> a.equals(a2)
      False



   .. py:method:: fill(value: arkouda.numpy.dtypes.numeric_scalars) -> None

      Fill the array (in place) with a constant value.

      :param value:
      :type value: numeric_scalars

      :raises TypeError: Raised if value is not an int, int64, float, or float64



   .. py:method:: flatten()

      Return a copy of the array collapsed into one dimension.

      :rtype: A copy of the input array, flattened to one dimension.



   .. py:method:: format_other(other) -> str

      Attempt to cast scalar other to the element dtype of this pdarray,
      and print the resulting value to a string (e.g. for sending to a
      server command). The user should not call this function directly.

      :param other: The scalar to be cast to the pdarray.dtype
      :type other: object

      :rtype: string representation of np.dtype corresponding to the other parameter

      :raises TypeError: Raised if the other parameter cannot be converted to
          Numpy dtype



   .. py:property:: inferred_type
      :type: Union[str, None]


      Return a string of the type inferred from the values.


   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry

      :param None:

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RuntimeError: Raised if there's a server-side error thrown

      .. note::

         This will return True if the object is registered itself or as a component
         of another object



   .. py:method:: is_sorted(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.bool_scalars, pdarray]

      Return True iff the array is monotonically non-decreasing.

      :param None:

      :returns: Indicates if the array is monotonically non-decreasing
      :rtype: bool

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:attribute:: itemsize


   .. py:method:: max(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the maximum value of the array.



   .. py:property:: max_bits


   .. py:method:: maxk(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: mean() -> numpy.float64

      Return the mean of the array.



   .. py:method:: min(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the minimum value of the array.



   .. py:method:: mink(k: arkouda.numpy.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:attribute:: name


   .. py:property:: nbytes

      The size of the pdarray in bytes.

      :returns: The size of the pdarray in bytes.
      :rtype: int


   .. py:attribute:: ndim


   .. py:attribute:: objType
      :value: 'pdarray'



   .. py:method:: opeq(other, op)


   .. py:method:: parity() -> pdarray

      Find the parity (XOR of all bits) in each element. See `ak.parity`.



   .. py:method:: popcount() -> pdarray

      Find the population (number of bits set) in each element. See `ak.popcount`.



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: prod(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the product of all elements in the array. Return value is
      always a np.float64 or np.int64.



   .. py:method:: register(user_defined_name: str) -> pdarray

      Register this pdarray with a user defined name in the arkouda server
      so it can be attached to later using pdarray.attach()
      This is an in-place operation, registering a pdarray more than once will
      update the name in the registry and remove the previously registered name.
      A name can only be registered to one pdarray at a time.

      :param user_defined_name: user defined name array is to be registered under
      :type user_defined_name: str

      :returns: The same pdarray which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different pdarrays with the same name.
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the pdarray with the user_defined_name
          If the user is attempting to register more than one pdarray with the same name,
          the former should be unregistered first to free up the registration name.

      .. seealso:: :obj:`attach`, :obj:`unregister`, :obj:`is_registered`, :obj:`list_registry`, :obj:`unregister_pdarray_by_name`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:attribute:: registered_name
      :type:  Optional[str]
      :value: None



   .. py:method:: reshape(*shape)

      Gives a new shape to an array without changing its data.

      :param shape: The new shape should be compatible with the original shape.
      :type shape: int, tuple of ints, or pdarray

      :returns: a pdarray with the same data, reshaped to the new shape
      :rtype: pdarray



   .. py:method:: rotl(other) -> pdarray

      Rotate bits left by <other>.



   .. py:method:: rotr(other) -> pdarray

      Rotate bits right by <other>.



   .. py:method:: save(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the pdarray to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. HDF5 support single files, in which case the file name will
      only be that provided. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str {'HDF5', 'Parquet'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: Raised if there is an error in parsing the prefix path pointing to
          file write location or if the mode parameter is neither truncate
          nor append
      :raises TypeError: Raised if any one of the prefix_path, dataset, or mode parameters
          is not a string

      .. seealso:: :obj:`save_all`, :obj:`load`, :obj:`read`, :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Notes

      The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales``. If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      Previously all files saved in Parquet format were saved with a ``.parquet`` file extension.
      This will require you to use load as if you saved the file with the extension. Try this if
      an older file is not being found.
      Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.save('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.save('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving with an extension (Parquet)
      >>> a.save('path/prefix.parquet', dataset='array', file_format='Parquet')
      Saves the array in numLocales Parquet files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:property:: shape

      Return the shape of an array.

      :returns: The elements of the shape tuple give the lengths of the corresponding array dimensions.
      :rtype: tuple of int


   .. py:attribute:: size


   .. py:method:: slice_bits(low, high) -> pdarray

      Returns a pdarray containing only bits from low to high of self.

      This is zero indexed and inclusive on both ends, so slicing the bottom 64 bits is
      pda.slice_bits(0, 63)

      :param low: The lowest bit included in the slice (inclusive)
                  zero indexed, so the first bit is 0
      :type low: int
      :param high: The highest bit included in the slice (inclusive)
      :type high: int

      :returns: A new pdarray containing the bits of self from low to high
      :rtype: pdarray

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. rubric:: Examples

      >>> p = ak.array([2**65 + (2**64 - 1)])
      >>> bin(p[0])
      '0b101111111111111111111111111111111111111111111111111111111111111111'

      >>> bin(p.slice_bits(64, 65)[0])
      '0b10'



   .. py:method:: std(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the standard deviation. See ``arkouda.std`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: The scalar standard deviation of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: sum(axis: Optional[Union[int, Tuple[int, Ellipsis]]] = None, keepdims: bool = False) -> Union[arkouda.numpy.dtypes.numpy_scalars, pdarray]

      Return the sum of all elements in the array.



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'array', col_delim: str = ',', overwrite: bool = False)

              Write pdarray to CSV file(s). File will contain a single column with the pdarray data.
              All CSV Files written by Arkouda include a header denoting data types of the columns.

              Parameters
              -----------
              prefix_path: str
                  The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                  when they are written to disk.
              dataset: str
                  Column name to save the pdarray under. Defaults to "array".
              col_delim: str
                  Defaults to ",". Value to be used to separate columns within the file.
                  Please be sure that the value used DOES NOT appear in your dataset.
              overwrite: bool
                  Defaults to False. If True, any existing files matching your provided prefix_path will
                  be overwritten. If False, an error will be returned if existing files are found.

              Returns
              --------
              str reponse message

              Raises
              ------
              ValueError
                  Raised if all datasets are not present in all parquet files or if one or
                  more of the specified files do not exist
              RuntimeError
                  Raised if one or more of the specified files cannot be opened.
                  If `allow_errors` is true this may be raised if no values are returned
                  from the server.
              TypeError
                  Raised if we receive an unknown arkouda_type returned from the server

              Notes
              ------
              - CSV format is not currently supported by load/load_all operations
              - The column delimiter is expected to be the same for column names and data
              - Be sure that column delimiters are not found within your data.
              - All CSV files must delimit rows using newline (`
      `) at this time.




   .. py:method:: to_cuda()

      Convert the array to a Numba DeviceND array, transferring array data from the
      arkouda server to Python via ndarray. If the array exceeds a builtin size limit,
      a RuntimeError is raised.

      :returns: A Numba ndarray with the same attributes and data as the pdarray; on GPU
      :rtype: numba.DeviceNDArray

      :raises ImportError: Raised if CUDA is not available
      :raises ModuleNotFoundError: Raised if Numba is either not installed or not enabled
      :raises RuntimeError: Raised if there is a server-side error thrown in the course of retrieving
          the pdarray.

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_cuda()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_cuda())
      numpy.devicendarray



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the pdarray to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_hdf('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_hdf('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving to a single file
      >>> a.to_hdf('path/prefix.hdf5', dataset='array', file_type='single')
      Saves the array in to single hdf5 file on the root node.
      ``cwd/path/name_prefix.hdf5``



   .. py:method:: to_list() -> List

      Convert the array to a list, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A list with the same data as the pdarray
      :rtype: list

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_list()
      [0, 1, 2, 3, 4]

      >>> type(a.to_list())
      list



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A numpy ndarray with the same attributes and data as the pdarray
      :rtype: np.ndarray

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_ndarray()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      Save the pdarray to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_parquet('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_parqet('path/prefix.parquet', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:method:: transfer(hostname: str, port: arkouda.numpy.dtypes.int_scalars)

      Sends a pdarray to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the pdarray is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister() -> None

      Unregister a pdarray in the arkouda server which was previously
      registered using register() and/or attahced to using attach()


      :rtype: None

      :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion until
      they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'array', repack: bool = True)

      Overwrite the dataset with the name provided with this pdarray. If
      the dataset does not exist it is added

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added



   .. py:method:: value_counts()

      Count the occurrences of the unique values of self.

      :returns: * **unique_values** (*pdarray*) -- The unique values, sorted in ascending order
                * **counts** (*pdarray, int64*) -- The number of times the corresponding unique value occurs

      .. rubric:: Examples

      >>> ak.array([2, 0, 2, 4, 0, 0]).value_counts()
      (array([0, 2, 4]), array([3, 2, 1]))



   .. py:method:: var(ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

      Compute the variance. See ``arkouda.var`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: The scalar variance of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises ValueError: Raised if the ddof >= pdarray size
      :raises RuntimeError: Raised if there's a server-side error thrown



.. py:data:: pi
   :type:  float

.. py:function:: plot_dist(b, h, log=True, xlabel=None, newfig=True)

   Plot the distribution and cumulative distribution of histogram Data

   :param b: Bin edges
   :type b: np.ndarray
   :param h: Histogram data
   :type h: np.ndarray
   :param log: use log to scale y
   :type log: bool
   :param xlabel: Label for the x axis of the graph
   :type xlabel: str
   :param newfig: Generate a new figure or not
   :type newfig: bool

   .. rubric:: Notes

   This function does not return or display the plot. A user must have matplotlib imported in
   addition to arkouda to display plots. This could be updated to return the object or have a
   flag to show the resulting plots.
   See Examples Below.

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> from matplotlib import pyplot as plt
   >>> b, h = ak.histogram(ak.arange(10), 3)
   >>> ak.plot_dist(b, h.to_ndarray())
   >>> # to show the plot
   >>> plt.show()


.. py:function:: popcount(pda: pdarray) -> pdarray

   Find the population (number of bits set) for each integer in an array.

   :param pda: Input array (must be integral).
   :type pda: pdarray, int64, uint64, bigint

   :returns: **population** -- The number of bits set (1) in each element
   :rtype: pdarray

   :raises TypeError: If input array is not int64, uint64, or bigint

   .. rubric:: Examples

   >>> A = ak.arange(10)
   >>> ak.popcount(A)
   array([0, 1, 1, 2, 1, 2, 2, 3, 1, 2])


.. py:function:: power(pda: pdarray, pwr: Union[int, float, pdarray], where: Union[arkouda.numpy.dtypes.bool_scalars, pdarray] = True) -> pdarray

   Raises an array to a power. If where is given, the operation will only take place in the positions
   where the where condition is True.

   Note:
   Our implementation of the where argument deviates from numpy. The difference in behavior occurs
   at positions where the where argument contains a False. In numpy, these position will have
   uninitialized memory (which can contain anything and will vary between runs). We have chosen to
   instead return the value of the original array in these positions.

   :param pda: A pdarray of values that will be raised to a power (pwr)
   :type pda: pdarray
   :param pwr: The power(s) that pda is raised to
   :type pwr: integer, float, or pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True, the
                 corresponding value will be raised to the respective power. Elsewhere, it will retain its
                 original value. Default set to True.
   :type where: Boolean or pdarray

   :returns: pdarray
             Returns a pdarray of values raised to a power, under the boolean where condition.

   .. rubric:: Examples

   >>> a = ak.arange(5)
   >>> ak.power(a, 3)
   array([0, 1, 8, 27, 64])
   >>> ak.power(a), 3, a % 2 == 0)
   array([0, 1, 8, 3, 64])


.. py:function:: power_divergence(f_obs, f_exp=None, ddof=0, lambda_=None)

   Computes the power divergence statistic and p-value.

   :param f_obs: The observed frequency.
   :type f_obs: pdarray
   :param f_exp: The expected frequency.
   :type f_exp: pdarray, default = None
   :param ddof: The delta degrees of freedom.
   :type ddof: int
   :param lambda_: The power in the Cressie-Read power divergence statistic.
                   Allowed values: "pearson", "log-likelihood", "freeman-tukey", "mod-log-likelihood",
                   "neyman", "cressie-read"

                   Powers correspond as follows:

                   "pearson": 1

                   "log-likelihood": 0

                   "freeman-tukey": -0.5

                   "mod-log-likelihood": -1

                   "neyman": -2

                   "cressie-read": 2 / 3
   :type lambda_: string, default = "pearson"

   :rtype: arkouda.akstats.Power_divergenceResult

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.connect()
   >>> from arkouda.stats import power_divergence
   >>> x = ak.array([10, 20, 30, 10])
   >>> y = ak.array([10, 30, 20, 10])
   >>> power_divergence(x, y, lambda_="pearson")
   Power_divergenceResult(statistic=8.333333333333334, pvalue=0.03960235520756414)
   >>> power_divergence(x, y, lambda_="log-likelihood")
   Power_divergenceResult(statistic=8.109302162163285, pvalue=0.04380595350226197)

   .. seealso:: :obj:`scipy.stats.power_divergence`, :obj:`arkouda.akstats.chisquare`

   .. rubric:: Notes

   This is a modified version of scipy.stats.power_divergence [2]
   in order to scale using arkouda pdarrays.

   .. rubric:: References

   [1] "scipy.stats.power_divergence",
   https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.power_divergence.html

   [2] Scipy contributors (2024) scipy (Version v1.12.0) [Source code].
   https://github.com/scipy/scipy


.. py:function:: pretty_print_information(names: Union[List[str], str] = RegisteredSymbols) -> None

   Prints verbose information for each object in names in a human readable format

   :param names: names is either the name of an object or list of names of objects to retrieve info
                 if names is ak.AllSymbols, retrieves info for all symbols in the symbol table
                 if names is ak.RegisteredSymbols, retrieves info for all symbols in the registry
   :type names: Union[List[str], str]

   :rtype: None

   :raises RuntimeError: Raised if a server-side error is thrown in the process of
       retrieving information about the objects in names


.. py:function:: promote_to_common_dtype(arrays: List[arkouda.pdarrayclass.pdarray]) -> Tuple[Any, List[arkouda.pdarrayclass.pdarray]]

   Promote a list of pdarrays to a common dtype.

   :param arrays: List of pdarrays to promote
   :type arrays: List[pdarray]

   :returns: The common dtype of the pdarrays and the list of pdarrays promoted to that dtype
   :rtype: dtype, List[pdarray]

   :raises TypeError: Raised if any pdarray is a non-numeric type

   .. seealso:: :obj:`pdarray.promote_dtype`

   .. rubric:: Examples

   >>> a = ak.arange(5)
   >>> b = ak.ones(5, dtype=ak.float64)
   >>> dtype, promoted = ak.promote_to_common_dtype([a, b])
   >>> dtype
   dtype('float64')
   >>> all(isinstance(p, ak.pdarray) and p.dtype == dtype for p in promoted)
   True


.. py:function:: putmask(A: arkouda.pdarrayclass.pdarray, mask: arkouda.pdarrayclass.pdarray, Values: arkouda.pdarrayclass.pdarray) -> None

   Overwrites elements of A with elements from B based upon a mask array.
   Similar to numpy.putmask, where mask = False, A retains its original value,
   but where mask = True, A is overwritten with the corresponding entry from Values.

   This is similar to ak.where, except that (1) no new pdarray is created, and
   (2) Values does not have to be the same size as A and mask.

   :param A: Value(s) used when mask is False (see Notes for allowed dtypes)
   :type A: pdarray
   :param mask: Used to choose values from A or B, must be same size as A, and of type ak.bool_
   :type mask: pdarray
   :param Values: Value(s) used when mask is False (see Notes for allowed dtypes)
   :type Values: pdarray

   .. rubric:: Examples

   >>> a = ak.array(np.arange(10))
   >>> ak.putmask (a,a>2,a**2)
   >>> a
   array([0 1 2 9 16 25 36 49 64 81])

   >>> a = ak.array(np.arange(10))
   >>> values = ak.array([3,2])
   >>> ak.putmask (a,a>2,values)
   >>> a
   array([0 1 2 2 3 2 3 2 3 2])

   :raises RuntimeError: Raised if mask is not same size as A, or if A.dtype and Values.dtype are not
       an allowed pair (see Notes for details).

   .. rubric:: Notes

   | A and mask must be the same size.  Values can be any size.
   | Allowed dtypes for A and Values conform to types accepted by numpy putmask.
   | If A is ak.float64, Values can be ak.float64, ak.int64, ak.uint64, ak.bool_.
   | If A is ak.int64, Values can be ak.int64 or ak.bool_.
   | If A is ak.uint64, Values can be ak.uint64, or ak.bool_.
   | If A is ak.bool_, Values must be ak.bool_.

   Only one conditional clause is supported e.g., n < 5, n > 1.

   multi-dim pdarrays are now implemented.


.. py:function:: rad2deg(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Converts angles element-wise from radians to degrees.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True, the
                 corresponding value will be converted from radians to degrees. Elsewhere, it will retain its
                 original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing an angle converted to degrees, from radians, for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: randint(low: arkouda.numpy.dtypes.numeric_scalars, high: arkouda.numpy.dtypes.numeric_scalars, size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis]] = 1, dtype=akint64, seed: Optional[arkouda.numpy.dtypes.int_scalars] = None) -> arkouda.pdarrayclass.pdarray

   Generate a pdarray of randomized int, float, or bool values in a
   specified range bounded by the low and high parameters.

   :param low: The low value (inclusive) of the range
   :type low: numeric_scalars
   :param high: The high value (exclusive for int, inclusive for float) of the range
   :type high: numeric_scalars
   :param size: The size or shape of the returned array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: The dtype of the array
   :type dtype: Union[int64, float64, bool]
   :param seed: Index for where to pull the first returned value
   :type seed: int_scalars, optional

   :returns: Values drawn uniformly from the specified range having the desired dtype
   :rtype: pdarray

   :raises TypeError: Raised if dtype.name not in DTypes, size is not an int, low or high is
       not an int or float, or seed is not an int
   :raises ValueError: Raised if size < 0 or if high < low

   .. rubric:: Notes

   Calling randint with dtype=float64 will result in uniform non-integral
   floating point values.

   Ranges >= 2**64 in size is undefined behavior because
   it exceeds the maximum value that can be stored on the server (uint64)

   .. rubric:: Examples

   >>> ak.randint(0, 10, 5, seed=1701)
   array([6 5 1 6 3])

   >>> ak.randint(0, 1, 3, seed=1701, dtype=ak.float64)
   array([0.011410423448327005 0.73618171558685619 0.12367222192448891])

   >>> ak.randint(0, 1, 5, seed=1701, dtype=ak.bool_)
   array([False True False True False])


.. py:function:: random_sparse_matrix(size: int, density: float, layout: str, dtype: Union[type, str] = int64) -> arkouda.sparrayclass.sparray

   Create a random sparse matrix with the specified number of rows and columns
   and the specified density. The density is the fraction of non-zero elements
   in the matrix. The non-zero elements are uniformly distributed random
   numbers in the range [0,1).

   :param size: The number of rows in the matrix, columns are equal to rows right now
   :type size: int
   :param density: The fraction of non-zero elements in the matrix
   :type density: float
   :param dtype: The dtype of the elements in the matrix (default is int64)
   :type dtype: Union[DTypes, str]

   :returns: A sparse matrix with the specified number of rows and columns
             and the specified density
   :rtype: sparray

   :raises ValueError: Raised if density is not in the range [0,1]


.. py:function:: random_strings_lognormal(logmean: arkouda.numpy.dtypes.numeric_scalars, logstd: arkouda.numpy.dtypes.numeric_scalars, size: arkouda.numpy.dtypes.int_scalars, characters: str = 'uppercase', seed: Optional[arkouda.numpy.dtypes.int_scalars] = None) -> arkouda.strings.Strings

   Generate random strings with log-normally distributed lengths and
   with characters drawn from a specified set.

   :param logmean: The log-mean of the length distribution
   :type logmean: numeric_scalars
   :param logstd: The log-standard-deviation of the length distribution
   :type logstd: numeric_scalars
   :param size: The number of strings to generate
   :type size: int_scalars
   :param characters: The set of characters to draw from
   :type characters: (uppercase, lowercase, numeric, printable, binary)
   :param seed: Value used to initialize the random number generator
   :type seed: int_scalars, optional

   :returns: The Strings object encapsulating a pdarray of random strings
   :rtype: Strings

   :raises TypeError: Raised if logmean is neither a float nor a int, logstd is not a float,
       seed is not an int, size is not an int, or if characters is not a str
   :raises ValueError: Raised if logstd <= 0 or size < 0

   .. seealso:: :obj:`random_strings_lognormal`, :obj:`randint`

   .. rubric:: Notes

   The lengths of the generated strings are distributed $Lognormal(\mu, \sigma^2)$,
   with :math:`\mu = logmean` and :math:`\sigma = logstd`. Thus, the strings will
   have an average length of :math:`exp(\mu + 0.5*\sigma^2)`, a minimum length of
   zero, and a heavy tail towards longer strings.

   .. rubric:: Examples

   >>> ak.random_strings_lognormal(2, 0.25, 5, seed=1)
   array(['VWHJEX', 'BEBBXJHGM', 'RWOVKBUR', 'LNJCSDXD', 'NKEDQC'])

   >>> ak.random_strings_lognormal(2, 0.25, 5, seed=1, characters='printable')
   array(['eL96<O', ')o-GOe lR', ')PV yHf(', '._b3Yc&K', ',7Wjef'])


.. py:function:: random_strings_uniform(minlen: arkouda.numpy.dtypes.int_scalars, maxlen: arkouda.numpy.dtypes.int_scalars, size: arkouda.numpy.dtypes.int_scalars, characters: str = 'uppercase', seed: Union[None, arkouda.numpy.dtypes.int_scalars] = None) -> arkouda.strings.Strings

   Generate random strings with lengths uniformly distributed between
   minlen and maxlen, and with characters drawn from a specified set.

   :param minlen: The minimum allowed length of string
   :type minlen: int_scalars
   :param maxlen: The maximum allowed length of string
   :type maxlen: int_scalars
   :param size: The number of strings to generate
   :type size: int_scalars
   :param characters: The set of characters to draw from
   :type characters: (uppercase, lowercase, numeric, printable, binary)
   :param seed: Value used to initialize the random number generator
   :type seed: Union[None, int_scalars], optional

   :returns: The array of random strings
   :rtype: Strings

   :raises ValueError: Raised if minlen < 0, maxlen < minlen, or size < 0

   .. seealso:: :obj:`random_strings_lognormal`, :obj:`randint`

   .. rubric:: Examples

   >>> ak.random_strings_uniform(minlen=1, maxlen=5, seed=8675309, size=5)
   array(['ECWO', 'WSS', 'TZG', 'RW', 'C'])

   >>> ak.random_strings_uniform(minlen=1, maxlen=5, seed=8675309, size=5,
   ... characters='printable')
   array(['2 .z', 'aom', '2d|', 'o(', 'M'])


.. py:function:: read(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, iterative: bool = False, strictTypes: bool = True, allow_errors: bool = False, calc_string_offsets=False, column_delim: str = ',', read_nested: bool = True, has_non_float_nulls: bool = False, fixed_len: int = -1) -> Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray, arkouda.categorical.Categorical, arkouda.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.timeclass.Datetime, arkouda.timeclass.Timedelta, arkouda.index.Index]]]

   Read datasets from files.
   File Type is determined automatically.

   :param filenames: Either a list of filenames or shell expression
   :type filenames: list or str
   :param datasets: (List of) name(s) of dataset(s) to read (default: all available)
   :type datasets: list or str or None
   :param iterative: Iterative (True) or Single (False) function call(s) to server
   :type iterative: bool
   :param strictTypes: If True (default), require all dtypes of a given dataset to have the
                       same precision and sign. If False, allow dtypes of different
                       precision and sign across different files. For example, if one
                       file contains a uint32 dataset and another contains an int64
                       dataset with the same name, the contents of both will be read
                       into an int64 pdarray.
   :type strictTypes: bool
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool
   :param calc_string_offsets: Default False, if True this will tell the server to calculate the
                               offsets/segments array on the server versus loading them from HDF5 files.
                               In the future this option may be set to True as the default.
   :type calc_string_offsets: bool
   :param column_delim: Column delimiter to be used if dataset is CSV. Otherwise, unused.
   :type column_delim: str
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Ignored if datasets is not None
                       Parquet Files only.
   :type read_nested: bool
   :param has_non_float_nulls: Default False. This flag must be set to True to read non-float parquet columns
                               that contain null values.
   :type has_non_float_nulls: bool
   :param fixed_len: Default -1. This value can be set for reading Parquet string columns when the
                     length of each string is known at runtime. This can allow for skipping byte
                     calculation, which can have an impact on performance.
   :type fixed_len: int

   :returns: Dictionary of {datasetName: pdarray, String, or SegArray}
   :rtype: Returns a dictionary of Arkouda pdarrays, Arkouda Strings, or Arkouda Segarrays.

   :raises RuntimeError: If invalid filetype is detected

   .. seealso:: :obj:`get_datasets`, :obj:`ls`, :obj:`read_parquet`, :obj:`read_hdf`

   .. rubric:: Notes

   If filenames is a string, it is interpreted as a shell expression
   (a single filename is a valid expression, so it will work) and is
   expanded with glob to read all matching files.

   If iterative == True each dataset name and file names are passed to
   the server as independent sequential strings while if iterative == False
   all dataset names and file names are passed to the server in a single
   string.

   If datasets is None, infer the names of datasets from the first file
   and read all of them. Use ``get_datasets`` to show the names of datasets
   to HDF5/Parquet files.

   CSV files without the Arkouda Header are not supported.

   .. rubric:: Examples

   Read with file Extension
   >>> x = ak.read('path/name_prefix.h5') # load HDF5 - processing determines file type not extension
   Read without file Extension
   >>> x = ak.read('path/name_prefix.parquet') # load Parquet
   Read Glob Expression
   >>> x = ak.read('path/name_prefix*') # Reads HDF5


.. py:function:: read_csv(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, column_delim: str = ',', allow_errors: bool = False) -> Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray, arkouda.categorical.Categorical, arkouda.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.timeclass.Datetime, arkouda.timeclass.Timedelta, arkouda.index.Index]]]

   Read CSV file(s) into Arkouda objects. If more than one dataset is found, the objects
   will be returned in a dictionary mapping the dataset name to the Arkouda object
   containing the data. If the file contains the appropriately formatted header, typed
   data will be returned. Otherwise, all data will be returned as a Strings object.

   :param filenames: The filenames to read data from
   :type filenames: str or List[str]
   :param datasets: names of the datasets to read. When `None`, all datasets will be read.
   :type datasets: str or List[str] (Optional)
   :param column_delim: The delimiter for column names and data. Defaults to ",".
   :type column_delim: str
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool

   :returns: Dictionary of {datasetName: pdarray, String, or SegArray}
   :rtype: Returns a dictionary of Arkouda pdarrays, Arkouda Strings, or Arkouda Segarrays.

   :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
       more of the specified files do not exist
   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
       If `allow_errors` is true this may be raised if no values are returned
       from the server.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. seealso:: :obj:`to_csv`

   .. rubric:: Notes

   - CSV format is not currently supported by load/load_all operations
   - The column delimiter is expected to be the same for column names and data
   - Be sure that column delimiters are not found within your data.
   - All CSV files must delimit rows using newline (``\n``) at this time.
   - Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
     bytes as uint(8).


.. py:function:: read_hdf(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, iterative: bool = False, strict_types: bool = True, allow_errors: bool = False, calc_string_offsets: bool = False, tag_data=False) -> Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray, arkouda.categorical.Categorical, arkouda.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.timeclass.Datetime, arkouda.timeclass.Timedelta, arkouda.index.Index]]]

   Read Arkouda objects from HDF5 file/s

   :param filenames: Filename/s to read objects from
   :type filenames: str, List[str]
   :param datasets: datasets to read from the provided files
   :type datasets: Optional str, List[str]
   :param iterative: Iterative (True) or Single (False) function call(s) to server
   :type iterative: bool
   :param strict_types: If True (default), require all dtypes of a given dataset to have the
                        same precision and sign. If False, allow dtypes of different
                        precision and sign across different files. For example, if one
                        file contains a uint32 dataset and another contains an int64
                        dataset with the same name, the contents of both will be read
                        into an int64 pdarray.
   :type strict_types: bool
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool
   :param calc_string_offsets: Default False, if True this will tell the server to calculate the
                               offsets/segments array on the server versus loading them from HDF5 files.
                               In the future this option may be set to True as the default.
   :type calc_string_offsets: bool
   :param tagData: Default False, if True tag the data with the code associated with the filename
                   that the data was pulled from.
   :type tagData: bool

   :returns: Dictionary of {datasetName: pdarray, String, SegArray}
   :rtype: Returns a dictionary of Arkouda pdarrays, Arkouda Strings, or Arkouda Segarrays.

   :raises ValueError: Raised if all datasets are not present in all hdf5 files or if one or
       more of the specified files do not exist
   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
       If `allow_errors` is true this may be raised if no values are returned
       from the server.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. rubric:: Notes

   If filenames is a string, it is interpreted as a shell expression
   (a single filename is a valid expression, so it will work) and is
   expanded with glob to read all matching files.

   If iterative == True each dataset name and file names are passed to
   the server as independent sequential strings while if iterative == False
   all dataset names and file names are passed to the server in a single
   string.

   If datasets is None, infer the names of datasets from the first file
   and read all of them. Use ``get_datasets`` to show the names of datasets
   to HDF5 files.

   .. seealso:: :obj:`read_tagged_data`

   .. rubric:: Examples

   >>>
   # Read with file Extension
   >>> x = ak.read_hdf('path/name_prefix.h5') # load HDF5
   # Read Glob Expression
   >>> x = ak.read_hdf('path/name_prefix*') # Reads HDF5


.. py:function:: read_parquet(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, iterative: bool = False, strict_types: bool = True, allow_errors: bool = False, tag_data: bool = False, read_nested: bool = True, has_non_float_nulls: bool = False, fixed_len: int = -1) -> Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray, arkouda.categorical.Categorical, arkouda.dataframe.DataFrame, arkouda.client_dtypes.IPv4, arkouda.timeclass.Datetime, arkouda.timeclass.Timedelta, arkouda.index.Index]]]

   Read Arkouda objects from Parquet file/s

   :param filenames: Filename/s to read objects from
   :type filenames: str, List[str]
   :param datasets: datasets to read from the provided files
   :type datasets: Optional str, List[str]
   :param iterative: Iterative (True) or Single (False) function call(s) to server
   :type iterative: bool
   :param strict_types: If True (default), require all dtypes of a given dataset to have the
                        same precision and sign. If False, allow dtypes of different
                        precision and sign across different files. For example, if one
                        file contains a uint32 dataset and another contains an int64
                        dataset with the same name, the contents of both will be read
                        into an int64 pdarray.
   :type strict_types: bool
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool
   :param tagData: Default False, if True tag the data with the code associated with the filename
                   that the data was pulled from.
   :type tagData: bool
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       If datasets is not None, this will be ignored.
   :type read_nested: bool
   :param has_non_float_nulls: Default False. This flag must be set to True to read non-float parquet columns
                               that contain null values.
   :type has_non_float_nulls: bool
   :param fixed_len: Default -1. This value can be set for reading Parquet string columns when the
                     length of each string is known at runtime. This can allow for skipping byte
                     calculation, which can have an impact on performance.
   :type fixed_len: int

   :returns: Dictionary of {datasetName: pdarray, String, or SegArray}
   :rtype: Returns a dictionary of Arkouda pdarrays, Arkouda Strings, or Arkouda Segarrays.

   :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
       more of the specified files do not exist
   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
       If `allow_errors` is true this may be raised if no values are returned
       from the server.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. rubric:: Notes

   If filenames is a string, it is interpreted as a shell expression
   (a single filename is a valid expression, so it will work) and is
   expanded with glob to read all matching files.

   If iterative == True each dataset name and file names are passed to
   the server as independent sequential strings while if iterative == False
   all dataset names and file names are passed to the server in a single
   string.

   If datasets is None, infer the names of datasets from the first file
   and read all of them. Use ``get_datasets`` to show the names of datasets
   to Parquet files.

   Parquet always recomputes offsets at this time
   This will need to be updated once parquets workflow is updated

   .. seealso:: :obj:`read_tagged_data`

   .. rubric:: Examples

   Read without file Extension
   >>> x = ak.read_parquet('path/name_prefix.parquet') # load Parquet
   Read Glob Expression
   >>> x = ak.read_parquet('path/name_prefix*') # Reads Parquet


.. py:function:: read_tagged_data(filenames: Union[str, List[str]], datasets: Optional[Union[str, List[str]]] = None, strictTypes: bool = True, allow_errors: bool = False, calc_string_offsets=False, read_nested: bool = True, has_non_float_nulls: bool = False)

   Read datasets from files and tag each record to the file it was read from.
   File Type is determined automatically.

   :param filenames: Either a list of filenames or shell expression
   :type filenames: list or str
   :param datasets: (List of) name(s) of dataset(s) to read (default: all available)
   :type datasets: list or str or None
   :param strictTypes: If True (default), require all dtypes of a given dataset to have the
                       same precision and sign. If False, allow dtypes of different
                       precision and sign across different files. For example, if one
                       file contains a uint32 dataset and another contains an int64
                       dataset with the same name, the contents of both will be read
                       into an int64 pdarray.
   :type strictTypes: bool
   :param allow_errors: Default False, if True will allow files with read errors to be skipped
                        instead of failing.  A warning will be included in the return containing
                        the total number of files skipped due to failure and up to 10 filenames.
   :type allow_errors: bool
   :param calc_string_offsets: Default False, if True this will tell the server to calculate the
                               offsets/segments array on the server versus loading them from HDF5 files.
                               In the future this option may be set to True as the default.
   :type calc_string_offsets: bool
   :param read_nested: Default True, when True, SegArray objects will be read from the file. When False,
                       SegArray (or other nested Parquet columns) will be ignored.
                       Ignored if datasets is not `None`
                       Parquet Files only.
   :type read_nested: bool
   :param has_non_float_nulls: Default False. This flag must be set to True to read non-float parquet columns
                               that contain null values.
   :type has_non_float_nulls: bool

   .. rubric:: Notes

   Not currently supported for Categorical or GroupBy datasets

   .. rubric:: Examples

   Read files and return data with tagging corresponding to the Categorical returned
   cat.codes will link the codes in data to the filename. Data will contain the code `Filename_Codes`
   >>> data, cat = ak.read_tagged_data('path/name')
   >>> data
   {'Filname_Codes': array([0 3 6 9 12]), 'col_name': array([0 0 0 1])}


.. py:function:: read_zarr(store_path: str, ndim: int, dtype)

   Reads a Zarr store from disk into a pdarray. Supports multi-dimensional pdarrays of numeric types.
   To use this function, ensure you have installed the blosc dependency (`make install-blosc`)
   and have included `ZarrMsg.chpl` in the `ServerModules.cfg` file.

   :param store_path: The path to the Zarr store. The path must be to a directory that contains a `.zarray`
                      file containing the Zarr store metadata.
   :type store_path: str
   :param ndim: The number of dimensions in the array
   :type ndim: int
   :param dtype: The data type of the array
   :type dtype: str

   :returns: The pdarray read from the Zarr store.
   :rtype: pdarray


.. py:function:: receive(hostname: str, port)

   Receive a pdarray sent by `pdarray.transfer()`.

   :param hostname: The hostname of the pdarray that sent the array
   :type hostname: str
   :param port: The port to send the array over. This needs to be an
                open port (i.e., not one that the Arkouda server is
                running on). This will open up `numLocales` ports,
                each of which in succession, so will use ports of the
                range {port..(port+numLocales)} (e.g., running an
                Arkouda server of 4 nodes, port 1234 is passed as
                `port`, Arkouda will use ports 1234, 1235, 1236,
                and 1237 to send the array data).
                This port much match the port passed to the call to
                `pdarray.transfer()`.
   :type port: int_scalars

   :returns: The pdarray sent from the sending server to the current
             receiving server.
   :rtype: pdarray

   :raises ValueError: Raised if the op is not within the pdarray.BinOps set
   :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
       a supported dtype


.. py:function:: receive_dataframe(hostname: str, port)

   Receive a pdarray sent by `dataframe.transfer()`.

   :param hostname: The hostname of the dataframe that sent the array
   :type hostname: str
   :param port: The port to send the dataframe over. This needs to be an
                open port (i.e., not one that the Arkouda server is
                running on). This will open up `numLocales` ports,
                each of which in succession, so will use ports of the
                range {port..(port+numLocales)} (e.g., running an
                Arkouda server of 4 nodes, port 1234 is passed as
                `port`, Arkouda will use ports 1234, 1235, 1236,
                and 1237 to send the array data).
                This port much match the port passed to the call to
                `pdarray.send_array()`.
   :type port: int_scalars

   :returns: The dataframe sent from the sending server to the
             current receiving server.
   :rtype: pdarray

   :raises ValueError: Raised if the op is not within the pdarray.BinOps set
   :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
       a supported dtype


.. py:function:: register_all(data: dict)

   Register all objects in the provided dictionary

   :param data: Maps name to register the object to the object. For example, {"MyArray": ak.array([0, 1, 2])
   :type data: dict

   :rtype: None


.. py:function:: resolve_scalar_dtype(val: object) -> str

   Try to infer what dtype arkouda_server should treat val as.



.. py:function:: restore(filename)

   Return data saved using `ak.snapshot`

   :param filename:
   :type filename: str
   :param Name used to create snapshot to be read:

   :rtype: Dict

   .. rubric:: Notes

   Unlike other save/load methods using snapshot restore will save DataFrames alongside other
   objects in HDF5. Thus, they are returned within the dictionary as a dataframe.


.. py:function:: right_align(left, right)

   Map two arrays of sparse values to the 0-up index set implied by the right array,
   discarding values from left that do not appear in right.

   :param left: Left-hand identifiers
   :type left: pdarray or a sequence of pdarrays
   :param right: Right-hand identifiers that define the index
   :type right: pdarray or a sequence of pdarrays

   :returns: * **keep** (*pdarray, bool*) -- Logical index of left-hand values that survived
             * **aligned** (*(pdarray, pdarray)*) -- Left and right arrays with values replaced by 0-up indices


.. py:function:: rotl(x, rot) -> pdarray

   Rotate bits of <x> to the left by <rot>.

   :param x: Value(s) to rotate left.
   :type x: pdarray(int64/uint64) or integer
   :param rot: Amount(s) to rotate by.
   :type rot: pdarray(int64/uint64) or integer

   :returns: **rotated** -- The rotated elements of x.
   :rtype: pdarray(int64/uint64)

   :raises TypeError: If input array is not int64 or uint64

   .. rubric:: Examples

   >>> A = ak.arange(10)
   >>> ak.rotl(A, A)
   array([0, 2, 8, 24, 64, 160, 384, 896, 2048, 4608])


.. py:function:: rotr(x, rot) -> pdarray

   Rotate bits of <x> to the left by <rot>.

   :param x: Value(s) to rotate left.
   :type x: pdarray(int64/uint64) or integer
   :param rot: Amount(s) to rotate by.
   :type rot: pdarray(int64/uint64) or integer

   :returns: **rotated** -- The rotated elements of x.
   :rtype: pdarray(int64/uint64)

   :raises TypeError: If input array is not int64 or uint64

   .. rubric:: Examples

   >>> A = ak.arange(10)
   >>> ak.rotr(1024 * A, A)
   array([0, 512, 512, 384, 256, 160, 96, 56, 32, 18])


.. py:function:: round(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise rounding of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing input array elements rounded to the nearest integer
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.round(ak.array([1.1, 2.5, 3.14159]))
   array([1.00000000000000000 3.00000000000000000 3.00000000000000000])


.. py:function:: save_all(columns: Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray]], List[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray]]], prefix_path: str, names: Optional[List[str]] = None, file_format='HDF5', mode: str = 'truncate', file_type: str = 'distribute', compression: Optional[str] = None) -> None

   DEPRECATED
   Save multiple named pdarrays to HDF5/Parquet files.
   :param columns: Collection of arrays to save
   :type columns: dict or list of pdarrays
   :param prefix_path: Directory and filename prefix for output files
   :type prefix_path: str
   :param names: Dataset names for the pdarrays
   :type names: list of str
   :param file_format: 'HDF5' or 'Parquet'. Defaults to hdf5
   :type file_format: str
   :param mode: By default, truncate (overwrite) the output files if they exist.
                If 'append', attempt to create new dataset in existing files.
   :type mode: {'truncate' | 'append'}
   :param file_type: Default: distribute
                     Single writes the dataset to a single file
                     Distribute writes the dataset to a file per locale
                     Only used with HDF5
   :type file_type: str ("single" | "distribute")
   :param compression: Optional
                       Select the compression to use with Parquet files.
                       Only used with Parquet.
   :type compression: str (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")

   :rtype: None

   :raises ValueError: Raised if (1) the lengths of columns and values differ or (2) the mode
       is not 'truncate' or 'append'

   .. seealso:: :obj:`save`, :obj:`load_all`, :obj:`to_parquet`, :obj:`to_hdf`

   .. rubric:: Notes

   Creates one file per locale containing that locale's chunk of each pdarray.
   If columns is a dictionary, the keys are used as the HDF5 dataset names.
   Otherwise, if no names are supplied, 0-up integers are used. By default,
   any existing files at path_prefix will be overwritten, unless the user
   specifies the 'append' mode, in which case arkouda will attempt to add
   <columns> as new datasets to existing files. If the wrong number of files
   is present or dataset names already exist, a RuntimeError is raised.

   .. rubric:: Examples

   >>> a = ak.arange(25)
   >>> b = ak.arange(25)
   >>> # Save with mapping defining dataset names
   >>> ak.save_all({'a': a, 'b': b}, 'path/name_prefix', file_format='Parquet')
   >>> # Save using names instead of mapping
   >>> ak.save_all([a, b], 'path/name_prefix', names=['a', 'b'], file_format='Parquet')


.. py:function:: save_checkpoint(name='', path='.akdata', mode: str = 'overwrite')

   Save the server's state. Records some metadata about the server, and saves
   all pdarrays into parquet files.

   :param name: Name of the checkpoint. The default will be the server session ID, which
                is typically in format ``id_<hash>_``. A directory will be created in
                ``path`` with this name.
   :type name: str
   :param path: The directory to save the checkpoint. If the directory doesn't exist, it
                will be created. If it exists, a new directory for the checkpoint
                instance will be created inside this directory.
   :type path: str
   :param mode: By default, overwrite the checkpoint files if they exist.
                If 'error', an error will be raised if a checkpoint with the same name
                exists.
   :type mode: {'overwrite' | 'error'}

   .. rubric:: Notes

   Only ``pdarray``s are saved. Other data structures will not be recorded. We
   expect to expand the coverage in the future.

   :returns: The checkpoint name, which will be the same as the ``name`` argument if
             it was passed.
   :rtype: str

   .. rubric:: Examples

   >>> arr = ak.zeros(10, int)
   >>> arr[2] = 2
   >>> arr[2]
   2
   >>> cp_name = ak.save_checkpoint()
   >>> arr[2] = 3
   >>> arr[2]
   3
   >>> ak.load_checkpoint(cp_name)
   >>> arr[2]
   2

   .. seealso:: :obj:`load_checkpoint`


.. py:function:: scalar_array(value: arkouda.numpy.dtypes.numeric_scalars, dtype: Optional[Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint]] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray from a single scalar value.

   :param value: Value to create pdarray from
   :type value: numeric_scalars

   :returns: pdarray with a single element
   :rtype: pdarray

   .. rubric:: Examples

   >>> ak.scalar_array(5)
   array([5])

   >>> ak.scalar_array(7.0)
   array([7.00000000000000000])

   :raises RuntimeError: Raised if value cannot be cast as dtype


.. py:class:: sctypeDict

   dict() -> new empty dictionary
   dict(mapping) -> new dictionary initialized from a mapping object's
       (key, value) pairs
   dict(iterable) -> new dictionary initialized as if via:
       d = {}
       for k, v in iterable:
           d[k] = v
   dict(**kwargs) -> new dictionary initialized with the name=value pairs
       in the keyword argument list.  For example:  dict(one=1, two=2)



   .. py:method:: clear(*args, **kwargs)

      D.clear() -> None.  Remove all items from D.




   .. py:method:: copy(*args, **kwargs)

      D.copy() -> a shallow copy of D




   .. py:method:: fromkeys(iterable, value=None, /)

      Create a new dictionary with keys from iterable and values set to value.




   .. py:method:: get(key, default=None, /)

      Return the value for key if key is in the dictionary, else default.




   .. py:method:: items(*args, **kwargs)

      D.items() -> a set-like object providing a view on D's items




   .. py:method:: keys(*args, **kwargs)

      D.keys() -> a set-like object providing a view on D's keys




   .. py:method:: pop(*args, **kwargs)

      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

      If key is not found, default is returned if given, otherwise KeyError is raised




   .. py:method:: popitem()

      Remove and return a (key, value) pair as a 2-tuple.

      Pairs are returned in LIFO (last-in, first-out) order.
      Raises KeyError if the dict is empty.




   .. py:method:: setdefault(key, default=None, /)

      Insert key with a value of default if key is not in the dictionary.

      Return the value for key if key is in the dictionary, else default.




   .. py:method:: update(*args, **kwargs)

      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
      In either case, this is followed by: for k in F:  D[k] = F[k]




   .. py:method:: values(*args, **kwargs)

      D.values() -> an object providing a view on D's values




.. py:class:: sctypes

   dict() -> new empty dictionary
   dict(mapping) -> new dictionary initialized from a mapping object's
       (key, value) pairs
   dict(iterable) -> new dictionary initialized as if via:
       d = {}
       for k, v in iterable:
           d[k] = v
   dict(**kwargs) -> new dictionary initialized with the name=value pairs
       in the keyword argument list.  For example:  dict(one=1, two=2)



   .. py:method:: clear(*args, **kwargs)

      D.clear() -> None.  Remove all items from D.




   .. py:method:: copy(*args, **kwargs)

      D.copy() -> a shallow copy of D




   .. py:method:: fromkeys(iterable, value=None, /)

      Create a new dictionary with keys from iterable and values set to value.




   .. py:method:: get(key, default=None, /)

      Return the value for key if key is in the dictionary, else default.




   .. py:method:: items(*args, **kwargs)

      D.items() -> a set-like object providing a view on D's items




   .. py:method:: keys(*args, **kwargs)

      D.keys() -> a set-like object providing a view on D's keys




   .. py:method:: pop(*args, **kwargs)

      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.

      If key is not found, default is returned if given, otherwise KeyError is raised




   .. py:method:: popitem()

      Remove and return a (key, value) pair as a 2-tuple.

      Pairs are returned in LIFO (last-in, first-out) order.
      Raises KeyError if the dict is empty.




   .. py:method:: setdefault(key, default=None, /)

      Insert key with a value of default if key is not in the dictionary.

      Return the value for key if key is in the dictionary, else default.




   .. py:method:: update(*args, **kwargs)

      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.
      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
      In either case, this is followed by: for k in F:  D[k] = F[k]




   .. py:method:: values(*args, **kwargs)

      D.values() -> an object providing a view on D's values




.. py:function:: search_intervals(vals, intervals, tiebreak=None, hierarchical=True)

   Given an array of query vals and non-overlapping, closed intervals, return
   the index of the best (see tiebreak) interval containing each query value,
   or -1 if not present in any interval.

   :param vals: Values to search for in intervals. If multiple arrays, each "row" is an item.
   :type vals: (sequence of) pdarray(int, uint, float)
   :param intervals: Non-overlapping, half-open intervals, as a tuple of
                     (lower_bounds_inclusive, upper_bounds_exclusive)
                     Must have same dtype(s) as vals.
   :type intervals: 2-tuple of (sequences of) pdarrays
   :param tiebreak: When a value is present in more than one interval, the interval with the
                    lowest tiebreak value will be chosen. If no tiebreak is given, the
                    first containing interval will be chosen.
   :type tiebreak: (optional) pdarray, numeric
   :param hierarchical: When True, sequences of pdarrays will be treated as components specifying
                        a single dimension (i.e. hierarchical)
                        When False, sequences of pdarrays will be specifying multi-dimensional intervals
   :type hierarchical: boolean

   :returns: **idx** -- Index of interval containing each query value, or -1 if not found
   :rtype: pdarray(int64)

   .. rubric:: Notes

   The return idx satisfies the following condition:
       present = idx > -1
       ((intervals[0][idx[present]] <= vals[present]) &
        (intervals[1][idx[present]] >= vals[present])).all()

   .. rubric:: Examples

   >>> starts = (ak.array([0, 5]), ak.array([0, 11]))
   >>> ends = (ak.array([5, 9]), ak.array([10, 20]))
   >>> vals = (ak.array([0, 0, 2, 5, 5, 6, 6, 9]), ak.array([0, 20, 1, 5, 15, 0, 12, 30]))
   >>> ak.search_intervals(vals, (starts, ends), hierarchical=False)
   array([0 -1 0 0 1 -1 1 -1])
   >>> ak.search_intervals(vals, (starts, ends))
   array([0 0 0 0 1 1 1 -1])
   >>> bi_starts = ak.bigint_from_uint_arrays([ak.cast(a, ak.uint64) for a in starts])
   >>> bi_ends = ak.bigint_from_uint_arrays([ak.cast(a, ak.uint64) for a in ends])
   >>> bi_vals = ak.bigint_from_uint_arrays([ak.cast(a, ak.uint64) for a in vals])
   >>> bi_starts, bi_ends, bi_vals
   (array([0 92233720368547758091]),
   array([92233720368547758090 166020696663385964564]),
   array([0 20 36893488147419103233 92233720368547758085 92233720368547758095
   110680464442257309696 110680464442257309708 166020696663385964574]))
   >>> ak.search_intervals(bi_vals, (bi_starts, bi_ends))
   array([0 0 0 0 1 1 1 -1])


.. py:function:: segarray(segments: arkouda.pdarrayclass.pdarray, values: arkouda.pdarrayclass.pdarray, lengths=None, grouping=None)

   Alias for the from_parts function. Prevents user from needing to call `ak.SegArray` constructor
   DEPRECATED


.. py:function:: setdiff1d(pda1: arkouda.groupbyclass.groupable, pda2: arkouda.groupbyclass.groupable, assume_unique: bool = False) -> Union[arkouda.pdarrayclass.pdarray, arkouda.groupbyclass.groupable]

   Find the set difference of two arrays.

   Return the sorted, unique values in `pda1` that are not in `pda2`.

   :param pda1: Input array/Sequence of groupable objects
   :type pda1: pdarray/Sequence[pdarray, Strings, Categorical]
   :param pda2: Input array/sequence of groupable objects
   :type pda2: pdarray/List
   :param assume_unique: If True, the input arrays are both assumed to be unique, which
                         can speed up the calculation.  Default is False.
   :type assume_unique: bool

   :returns: Sorted 1D array/List of sorted pdarrays of values in `pda1` that are not in `pda2`.
   :rtype: pdarray/groupable

   :raises TypeError: Raised if either pda1 or pda2 is not a pdarray
   :raises RuntimeError: Raised if the dtype of either pdarray is not supported

   .. seealso:: :obj:`arkouda.groupbyclass.unique`, :obj:`setxor1d`

   .. rubric:: Notes

   ak.setdiff1d is not supported for bool or float64 pdarrays

   .. rubric:: Examples

   >>> a = ak.array([1, 2, 3, 2, 4, 1])
   >>> b = ak.array([3, 4, 5, 6])
   >>> ak.setdiff1d(a, b)
   array([1, 2])
   #Multi-Array Example
   >>> a = ak.arange(1, 6)
   >>> b = ak.array([1, 5, 3, 4, 2])
   >>> c = ak.array([1, 4, 3, 2, 5])
   >>> d = ak.array([1, 2, 3, 5, 4])
   >>> multia = [a, a, a]
   >>> multib = [b, c, d]
   >>> ak.setdiff1d(multia, multib)
   [array([2, 4, 5]), array([2, 4, 5]), array([2, 4, 5])]


.. py:function:: setxor1d(pda1: arkouda.groupbyclass.groupable, pda2: arkouda.groupbyclass.groupable, assume_unique: bool = False) -> Union[arkouda.pdarrayclass.pdarray, arkouda.groupbyclass.groupable]

   Find the set exclusive-or (symmetric difference) of two arrays.

   Return the sorted, unique values that are in only one (not both) of the
   input arrays.

   :param pda1: Input array/Sequence of groupable objects
   :type pda1: pdarray/Sequence[pdarray, Strings, Categorical]
   :param pda2: Input array/sequence of groupable objects
   :type pda2: pdarray/List
   :param assume_unique: If True, the input arrays are both assumed to be unique, which
                         can speed up the calculation.  Default is False.
   :type assume_unique: bool

   :returns: Sorted 1D array/List of sorted pdarrays of unique values that are in only one of the input
             arrays.
   :rtype: pdarray/groupable

   :raises TypeError: Raised if either pda1 or pda2 is not a pdarray
   :raises RuntimeError: Raised if the dtype of either pdarray is not supported

   .. rubric:: Notes

   ak.setxor1d is not supported for bool or float64 pdarrays

   .. rubric:: Examples

   >>> a = ak.array([1, 2, 3, 2, 4])
   >>> b = ak.array([2, 3, 5, 7, 5])
   >>> ak.setxor1d(a,b)
   array([1, 4, 5, 7])
   #Multi-Array Example
   >>> a = ak.arange(1, 6)
   >>> b = ak.array([1, 5, 3, 4, 2])
   >>> c = ak.array([1, 4, 3, 2, 5])
   >>> d = ak.array([1, 2, 3, 5, 4])
   >>> multia = [a, a, a]
   >>> multib = [b, c, d]
   >>> ak.setxor1d(multia, multib)
   [array([2, 2, 4, 4, 5, 5]), array([2, 5, 2, 4, 4, 5]), array([2, 4, 5, 4, 2, 5])]


.. py:function:: shape(a: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, bool, numpy.bool_, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, numpy.str_, str]) -> Tuple

   Return the shape of an array.

   :param a: Input array.
   :type a: pdarray

   :returns: **shape** -- The elements of the shape tuple give the lengths of the
             corresponding array dimensions.
   :rtype: tuple of ints

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.shape(ak.eye(3,2))
   (3, 2)
   >>> ak.shape([[1, 3]])
   (1, 2)
   >>> ak.shape([0])
   (1,)
   >>> ak.shape(0)
   ()


.. py:class:: short(value)

   Bases: :py:obj:`numpy.signedinteger`


   Signed integer type, compatible with C ``short``.

       :Character code: ``'h'``
       :Canonical name: `numpy.short`
       :Alias on this platform (Linux x86_64): `numpy.int16`: 16-bit signed integer (``-32_768`` to ``32_767``).



   .. py:method:: bit_count(*args, **kwargs)

      int16.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.int16(127).bit_count()
              7
              >>> np.int16(-127).bit_count()
              7




.. py:function:: sign(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise sign of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing sign values of the input array elements
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.sign(ak.array([-10, -5, 0, 5, 10]))
   array([-1 -1 0 1 1])


.. py:class:: signedinteger(value)

   Bases: :py:obj:`numpy.integer`


   Abstract base class of all signed integer scalar types.



.. py:function:: sin(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise sine of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the sine will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing sin for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:class:: single(value)

   Bases: :py:obj:`numpy.floating`


   Single-precision floating-point number type, compatible with C ``float``.

       :Character code: ``'f'``
       :Canonical name: `numpy.single`
       :Alias on this platform (Linux x86_64): `numpy.float32`: 32-bit-precision floating-point number type: sign bit, 8 bits exponent, 23 bits mantissa.



   .. py:method:: as_integer_ratio(*args, **kwargs)

      single.as_integer_ratio() -> (int, int)

              Return a pair of integers, whose ratio is exactly equal to the original
              floating point number, and with a positive denominator.
              Raise `OverflowError` on infinities and a `ValueError` on NaNs.

              >>> np.single(10.0).as_integer_ratio()
              (10, 1)
              >>> np.single(0.0).as_integer_ratio()
              (0, 1)
              >>> np.single(-.25).as_integer_ratio()
              (-1, 4)




   .. py:method:: is_integer(*args, **kwargs)

      single.is_integer() -> bool

              Return ``True`` if the floating point number is finite with integral
              value, and ``False`` otherwise.

              .. versionadded:: 1.22

              Examples
              --------
              >>> np.single(-2.0).is_integer()
              True
              >>> np.single(3.2).is_integer()
              False




.. py:function:: sinh(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise hyperbolic sine of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the hyperbolic sine will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing hyperbolic sine for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: skew(pda: pdarray, bias: bool = True) -> numpy.float64

   Computes the sample skewness of an array.
   Skewness > 0 means there's greater weight in the right tail of the distribution.
   Skewness < 0 means there's greater weight in the left tail of the distribution.
   Skewness == 0 means the data is normally distributed.
   Based on the `scipy.stats.skew` function.

   :param pda: A pdarray of values that will be calculated to find the skew
   :type pda: pdarray
   :param bias: If False, then the calculations are corrected for statistical bias.
   :type bias: bool, optional

   :returns: *

               np.float64
                   The skew of all elements in the array
             * *Examples*
             * *>>> a = ak.array([1, 1, 1, 5, 10])*
             * *>>> ak.skew(a)*
             * *0.9442193396379163*


.. py:function:: snapshot(filename)

   Create a snapshot of the current Arkouda namespace. All currently accessible variables containing
   Arkouda objects will be written to an HDF5 file.

   Unlike other save/load functions, this maintains the integrity of dataframes.

   Current Variable names are used as the dataset name when saving.

   :param filename:
   :type filename: str
   :param Name to use when storing file:

   :rtype: None

   .. seealso:: :obj:`ak.restore`


.. py:function:: sort(pda: arkouda.pdarrayclass.pdarray, algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD, axis: arkouda.numpy.dtypes.int_scalars = -1) -> arkouda.pdarrayclass.pdarray

   Return a sorted copy of the array. Only sorts numeric arrays;
   for Strings, use argsort.

   :param pda: The array to sort (int64, uint64, or float64)
   :type pda: pdarray
   :param algorithm: The algorithm to be used for sorting the arrays.
   :type algorithm: SortingAlgorithm, default=SortingAlgorithm.RadixSortLSD
   :param axis: The axis to sort over. Setting to -1 means that it will sort over axis = ndim - 1.
   :type axis: int_scalars, default=-1

   :returns: The sorted copy of pda
   :rtype: pdarray of int64, uint64, or float64

   :raises TypeError: Raised if the parameter is not a pdarray
   :raises ValueError: Raised if sort attempted on a pdarray with an unsupported dtype
       such as bool

   .. seealso:: :obj:`argsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and resilient
   to non-uniformity in data but communication intensive.

   .. rubric:: Examples

   >>> a = ak.randint(0, 10, 10)
   >>> sorted = ak.sort(a)
   >>> sorted
   array([0 1 1 3 4 5 7 8 8 9])


.. py:class:: sparray(name: str, mydtype: Union[numpy.dtype, str], size: arkouda.numpy.dtypes.int_scalars, nnz: arkouda.numpy.dtypes.int_scalars, ndim: arkouda.numpy.dtypes.int_scalars, shape: Sequence[int], layout: str, itemsize: arkouda.numpy.dtypes.int_scalars, max_bits: Optional[int] = None)

   The class for sparse arrays. This class contains only the
   attributies of the array; the data resides on the arkouda
   server. When a server operation results in a new array, arkouda
   will create a sparray instance that points to the array data on
   the server. As such, the user should not initialize sparray
   instances directly.

   .. attribute:: name

      The server-side identifier for the array

      :type: str

   .. attribute:: dtype

      The element type of the array

      :type: dtype

   .. attribute:: size

      The size of any one dimension of the array (all dimensions are assumed to be equal sized for now)

      :type: int_scalars

   .. attribute:: nnz

      The number of non-zero elements in the array

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 2 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      A list or tuple containing the sizes of each dimension of the array

      :type: Sequence[int]

   .. attribute:: layout

      The layout of the array ("CSR" or "CSC" are the only valid values)

      :type: str

   .. attribute:: itemsize

      The size in bytes of each element

      :type: int_scalars


   .. py:attribute:: dtype


   .. py:method:: fill_vals(a: arkouda.pdarrayclass.pdarray)


   .. py:attribute:: itemsize


   .. py:attribute:: layout


   .. py:attribute:: name


   .. py:attribute:: ndim


   .. py:attribute:: nnz


   .. py:attribute:: shape


   .. py:attribute:: size


   .. py:method:: to_pdarray() -> List[arkouda.pdarrayclass.pdarray]


.. py:function:: sparse_matrix_matrix_mult(A, B: arkouda.sparrayclass.sparray) -> arkouda.sparrayclass.sparray

   Multiply two sparse matrices.

   :param A: The left-hand sparse matrix
   :type A: sparray
   :param B: The right-hand sparse matrix
   :type B: sparray

   :returns: The product of the two sparse matrices
   :rtype: sparray


.. py:function:: sqrt(pda: pdarray, where: Union[arkouda.numpy.dtypes.bool_scalars, pdarray] = True) -> pdarray

   Takes the square root of array. If where is given, the operation will only take place in
   the positions where the where condition is True.

   :param pda: A pdarray of values that will be square rooted
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True, the
                 corresponding value will be square rooted. Elsewhere, it will retain its original value.
                 Default set to True.
   :type where: Boolean or pdarray

   :returns: * pdarray
               Returns a pdarray of square rooted values, under the boolean where condition.
             * *Examples*
             * *>>> a = ak.arange(5)*
             * *>>> ak.sqrt(a)*
             * *array([0 1 1.4142135623730951 1.7320508075688772 2])*
             * *>>> ak.sqrt(a, ak.sqrt([True, True, False, False, True]))*
             * *array([0, 1, 2, 3, 2])*


.. py:function:: square(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise square of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing square values of the input
             array elements
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.square(ak.arange(1,5))
   array([1 4 9 16])


.. py:function:: squeeze(x: Union[arkouda.pdarrayclass.pdarray, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, bool, numpy.bool_], /, axis: Union[NoneType, int, Tuple[int, Ellipsis]] = None) -> arkouda.pdarrayclass.pdarray

   Remove degenerate (size one) dimensions from an array.

   :param x: The array to squeeze
   :type x: pdarray
   :param axis: The axis or axes to squeeze (must have a size of one).
                If axis = None, all dimensions of size 1 will be squeezed.
   :type axis: int or Tuple[int, ...]

   :returns: A copy of x with the dimensions specified in the axis argument removed.
   :rtype: pdarray

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.connect()
   >>> x = ak.arange(10).reshape((1, 10, 1))
   >>> x
   array([array([array([0]) array([1]) array([2]) array([3])....
    array([4]) array([5]) array([6]) array([7]) array([8]) array([9])])])
   >>> x.shape
   (1, 10, 1)
   >>> ak.squeeze(x,axis=None)
   array([0 1 2 3 4 5 6 7 8 9])
   >>> ak.squeeze(x,axis=None).shape
   (10,)
   >>> ak.squeeze(x,axis=2)
   array([array([0 1 2 3 4 5 6 7 8 9])])
   >>> ak.squeeze(x,axis=2).shape
   (1, 10)
   >>> ak.squeeze(x,axis=(0,2))
   array([0 1 2 3 4 5 6 7 8 9])
   >>> ak.squeeze(x,axis=(0,2)).shape
   (10,)


.. py:function:: standard_normal(size: arkouda.numpy.dtypes.int_scalars, seed: Union[None, arkouda.numpy.dtypes.int_scalars] = None) -> arkouda.pdarrayclass.pdarray

   Draw real numbers from the standard normal distribution.

   :param size: The number of samples to draw (size of the returned array)
   :type size: int_scalars
   :param seed: Value used to initialize the random number generator
   :type seed: int_scalars

   :returns: The array of random numbers
   :rtype: pdarray, float64

   :raises TypeError: Raised if size is not an int
   :raises ValueError: Raised if size < 0

   .. seealso:: :obj:`randint`

   .. rubric:: Notes

   For random samples from :math:`N(\mu, \sigma^2)`, use:

   ``(sigma * standard_normal(size)) + mu``

   .. rubric:: Examples

   >>> ak.standard_normal(3,1)
   array([-0.68586185091150265 1.1723810583573377 0.567584107142031])


.. py:function:: std(pda: pdarray, ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

   Return the standard deviation of values in the array. The standard
   deviation is implemented as the square root of the variance.

   :param pda: values for which to calculate the standard deviation
   :type pda: pdarray
   :param ddof: "Delta Degrees of Freedom" used in calculating std
   :type ddof: int_scalars

   :returns: The scalar standard deviation of the array
   :rtype: np.float64

   :raises TypeError: Raised if pda is not a pdarray instance or ddof is not an integer
   :raises ValueError: Raised if ddof is an integer < 0
   :raises RuntimeError: Raised if there's a server-side error thrown

   .. seealso:: :obj:`mean`, :obj:`var`

   .. rubric:: Notes

   The standard deviation is the square root of the average of the squared
   deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

   The average squared deviation is normally calculated as
   ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
   the divisor ``N - ddof`` is used instead. In standard statistical
   practice, ``ddof=1`` provides an unbiased estimator of the variance
   of the infinite population. ``ddof=0`` provides a maximum likelihood
   estimate of the variance for normally distributed variables. The
   standard deviation computed in this function is the square root of
   the estimated variance, so even with ``ddof=1``, it will not be an
   unbiased estimate of the standard deviation per se.


.. py:class:: str_

   A unicode string.

       This type strips trailing null codepoints.

       >>> s = np.str_("abc\x00")
       >>> s
       'abc'

       Unlike the builtin `str`, this supports the :ref:`python:bufferobjects`, exposing its
       contents as UCS4:

       >>> m = memoryview(np.str_("abc"))
       >>> m.format
       '3w'
       >>> m.tobytes()
       b'a\x00\x00\x00b\x00\x00\x00c\x00\x00\x00'

       :Character code: ``'U'``
       :Alias: `numpy.unicode_`



   .. py:method:: T(*args, **kwargs)

      Scalar attribute identical to the corresponding array attribute.

          Please see `ndarray.T`.




   .. py:method:: all(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.all`.




   .. py:method:: any(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.any`.




   .. py:method:: argmax(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argmax`.




   .. py:method:: argmin(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argmin`.




   .. py:method:: argsort(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argsort`.




   .. py:method:: astype(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.astype`.




   .. py:method:: base(*args, **kwargs)

      Scalar attribute identical to the corresponding array attribute.

          Please see `ndarray.base`.




   .. py:method:: byteswap(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.byteswap`.




   .. py:method:: choose(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.choose`.




   .. py:method:: clip(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.clip`.




   .. py:method:: compress(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.compress`.




   .. py:method:: conj(*args, **kwargs)


   .. py:method:: conjugate(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.conjugate`.




   .. py:method:: copy(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.copy`.




   .. py:method:: cumprod(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.cumprod`.




   .. py:method:: cumsum(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.cumsum`.




   .. py:method:: data(*args, **kwargs)

      Pointer to start of data.




   .. py:method:: diagonal(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.diagonal`.




   .. py:method:: dtype(*args, **kwargs)

      Get array data-descriptor.




   .. py:method:: dump(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.dump`.




   .. py:method:: dumps(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.dumps`.




   .. py:method:: fill(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.fill`.




   .. py:method:: flags(*args, **kwargs)

      The integer value of flags.




   .. py:method:: flat(*args, **kwargs)

      A 1-D view of the scalar.




   .. py:method:: flatten(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.flatten`.




   .. py:method:: getfield(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.getfield`.




   .. py:method:: imag(*args, **kwargs)

      The imaginary part of the scalar.




   .. py:method:: item(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.item`.




   .. py:method:: itemset(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.itemset`.




   .. py:method:: itemsize(*args, **kwargs)

      The length of one element in bytes.




   .. py:method:: max(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.max`.




   .. py:method:: mean(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.mean`.




   .. py:method:: min(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.min`.




   .. py:method:: nbytes(*args, **kwargs)

      The length of the scalar in bytes.




   .. py:method:: ndim(*args, **kwargs)

      The number of array dimensions.




   .. py:method:: newbyteorder(*args, **kwargs)

      newbyteorder(new_order='S', /)

          Return a new `dtype` with a different byte order.

          Changes are also made in all fields and sub-arrays of the data type.

          The `new_order` code can be any from the following:

          * 'S' - swap dtype from current to opposite endian
          * {'<', 'little'} - little endian
          * {'>', 'big'} - big endian
          * {'=', 'native'} - native order
          * {'|', 'I'} - ignore (no change to byte order)

          Parameters
          ----------
          new_order : str, optional
              Byte order to force; a value from the byte order specifications
              above.  The default value ('S') results in swapping the current
              byte order.


          Returns
          -------
          new_dtype : dtype
              New `dtype` object with the given change to the byte order.




   .. py:method:: nonzero(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.nonzero`.




   .. py:method:: prod(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.prod`.




   .. py:method:: ptp(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.ptp`.




   .. py:method:: put(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.put`.




   .. py:method:: ravel(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.ravel`.




   .. py:method:: real(*args, **kwargs)

      The real part of the scalar.




   .. py:method:: repeat(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.repeat`.




   .. py:method:: reshape(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.reshape`.




   .. py:method:: resize(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.resize`.




   .. py:method:: round(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.round`.




   .. py:method:: searchsorted(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.searchsorted`.




   .. py:method:: setfield(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.setfield`.




   .. py:method:: setflags(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.setflags`.




   .. py:method:: shape(*args, **kwargs)

      Tuple of array dimensions.




   .. py:method:: size(*args, **kwargs)

      The number of elements in the gentype.




   .. py:method:: sort(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.sort`.




   .. py:method:: squeeze(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.squeeze`.




   .. py:method:: std(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.std`.




   .. py:method:: strides(*args, **kwargs)

      Tuple of bytes steps in each dimension.




   .. py:method:: sum(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.sum`.




   .. py:method:: swapaxes(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.swapaxes`.




   .. py:method:: take(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.take`.




   .. py:method:: tobytes(*args, **kwargs)


   .. py:method:: tofile(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tofile`.




   .. py:method:: tolist(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tolist`.




   .. py:method:: tostring(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tostring`.




   .. py:method:: trace(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.trace`.




   .. py:method:: transpose(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.transpose`.




   .. py:method:: var(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.var`.




   .. py:method:: view(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.view`.




.. py:class:: str_

   A unicode string.

       This type strips trailing null codepoints.

       >>> s = np.str_("abc\x00")
       >>> s
       'abc'

       Unlike the builtin `str`, this supports the :ref:`python:bufferobjects`, exposing its
       contents as UCS4:

       >>> m = memoryview(np.str_("abc"))
       >>> m.format
       '3w'
       >>> m.tobytes()
       b'a\x00\x00\x00b\x00\x00\x00c\x00\x00\x00'

       :Character code: ``'U'``
       :Alias: `numpy.unicode_`



   .. py:method:: T(*args, **kwargs)

      Scalar attribute identical to the corresponding array attribute.

          Please see `ndarray.T`.




   .. py:method:: all(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.all`.




   .. py:method:: any(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.any`.




   .. py:method:: argmax(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argmax`.




   .. py:method:: argmin(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argmin`.




   .. py:method:: argsort(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.argsort`.




   .. py:method:: astype(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.astype`.




   .. py:method:: base(*args, **kwargs)

      Scalar attribute identical to the corresponding array attribute.

          Please see `ndarray.base`.




   .. py:method:: byteswap(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.byteswap`.




   .. py:method:: choose(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.choose`.




   .. py:method:: clip(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.clip`.




   .. py:method:: compress(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.compress`.




   .. py:method:: conj(*args, **kwargs)


   .. py:method:: conjugate(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.conjugate`.




   .. py:method:: copy(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.copy`.




   .. py:method:: cumprod(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.cumprod`.




   .. py:method:: cumsum(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.cumsum`.




   .. py:method:: data(*args, **kwargs)

      Pointer to start of data.




   .. py:method:: diagonal(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.diagonal`.




   .. py:method:: dtype(*args, **kwargs)

      Get array data-descriptor.




   .. py:method:: dump(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.dump`.




   .. py:method:: dumps(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.dumps`.




   .. py:method:: fill(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.fill`.




   .. py:method:: flags(*args, **kwargs)

      The integer value of flags.




   .. py:method:: flat(*args, **kwargs)

      A 1-D view of the scalar.




   .. py:method:: flatten(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.flatten`.




   .. py:method:: getfield(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.getfield`.




   .. py:method:: imag(*args, **kwargs)

      The imaginary part of the scalar.




   .. py:method:: item(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.item`.




   .. py:method:: itemset(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.itemset`.




   .. py:method:: itemsize(*args, **kwargs)

      The length of one element in bytes.




   .. py:method:: max(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.max`.




   .. py:method:: mean(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.mean`.




   .. py:method:: min(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.min`.




   .. py:method:: nbytes(*args, **kwargs)

      The length of the scalar in bytes.




   .. py:method:: ndim(*args, **kwargs)

      The number of array dimensions.




   .. py:method:: newbyteorder(*args, **kwargs)

      newbyteorder(new_order='S', /)

          Return a new `dtype` with a different byte order.

          Changes are also made in all fields and sub-arrays of the data type.

          The `new_order` code can be any from the following:

          * 'S' - swap dtype from current to opposite endian
          * {'<', 'little'} - little endian
          * {'>', 'big'} - big endian
          * {'=', 'native'} - native order
          * {'|', 'I'} - ignore (no change to byte order)

          Parameters
          ----------
          new_order : str, optional
              Byte order to force; a value from the byte order specifications
              above.  The default value ('S') results in swapping the current
              byte order.


          Returns
          -------
          new_dtype : dtype
              New `dtype` object with the given change to the byte order.




   .. py:method:: nonzero(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.nonzero`.




   .. py:method:: prod(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.prod`.




   .. py:method:: ptp(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.ptp`.




   .. py:method:: put(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.put`.




   .. py:method:: ravel(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.ravel`.




   .. py:method:: real(*args, **kwargs)

      The real part of the scalar.




   .. py:method:: repeat(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.repeat`.




   .. py:method:: reshape(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.reshape`.




   .. py:method:: resize(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.resize`.




   .. py:method:: round(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.round`.




   .. py:method:: searchsorted(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.searchsorted`.




   .. py:method:: setfield(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.setfield`.




   .. py:method:: setflags(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.setflags`.




   .. py:method:: shape(*args, **kwargs)

      Tuple of array dimensions.




   .. py:method:: size(*args, **kwargs)

      The number of elements in the gentype.




   .. py:method:: sort(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.sort`.




   .. py:method:: squeeze(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.squeeze`.




   .. py:method:: std(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.std`.




   .. py:method:: strides(*args, **kwargs)

      Tuple of bytes steps in each dimension.




   .. py:method:: sum(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.sum`.




   .. py:method:: swapaxes(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.swapaxes`.




   .. py:method:: take(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.take`.




   .. py:method:: tobytes(*args, **kwargs)


   .. py:method:: tofile(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tofile`.




   .. py:method:: tolist(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tolist`.




   .. py:method:: tostring(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.tostring`.




   .. py:method:: trace(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.trace`.




   .. py:method:: transpose(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.transpose`.




   .. py:method:: var(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.var`.




   .. py:method:: view(*args, **kwargs)

      Scalar method identical to the corresponding array attribute.

          Please see `ndarray.view`.




.. py:class:: str_scalars(origin, params, *, inst=True, name=None)

   Bases: :py:obj:`_GenericAlias`


   The central part of internal API.

   This represents a generic version of type 'origin' with type arguments 'params'.
   There are two kind of these aliases: user defined and special. The special ones
   are wrappers around builtin collections and ABCs in collections.abc. These must
   have 'name' always set. If 'inst' is False, then the alias can't be instantiated,
   this is used by e.g. typing.List and typing.Dict.


.. py:function:: string_operators(cls)

.. py:function:: tan(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise tangent of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the tangent will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing tangent for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:function:: tanh(pda: arkouda.pdarrayclass.pdarray, where: Union[bool, arkouda.pdarrayclass.pdarray] = True) -> arkouda.pdarrayclass.pdarray

   Return the element-wise hyperbolic tangent of the array.

   :param pda:
   :type pda: pdarray
   :param where: This condition is broadcast over the input. At locations where the condition is True,
                 the hyperbolic tangent will be applied to the corresponding value. Elsewhere, it will retain
                 its original value. Default set to True.
   :type where: bool or pdarray, default=True

   :returns: A pdarray containing hyperbolic tangent for each element
             of the original pdarray
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray


.. py:class:: timedelta64(value)

   Bases: :py:obj:`numpy.signedinteger`


   A timedelta stored as a 64-bit integer.

       See :ref:`arrays.datetime` for more information.

       :Character code: ``'m'``



.. py:function:: timedelta_range(start=None, end=None, periods=None, freq=None, name=None, closed=None, **kwargs)

   Return a fixed frequency TimedeltaIndex, with day as the default
   frequency. Alias for ``ak.Timedelta(pd.timedelta_range(args))``.
   Subject to size limit imposed by client.maxTransferBytes.

   :param start: Left bound for generating timedeltas.
   :type start: str or timedelta-like, default None
   :param end: Right bound for generating timedeltas.
   :type end: str or timedelta-like, default None
   :param periods: Number of periods to generate.
   :type periods: int, default None
   :param freq: Frequency strings can have multiples, e.g. '5H'.
   :type freq: str or DateOffset, default 'D'
   :param name: Name of the resulting TimedeltaIndex.
   :type name: str, default None
   :param closed: Make the interval closed with respect to the given frequency to
                  the 'left', 'right', or both sides (None).
   :type closed: str, default None

   :returns: **rng**
   :rtype: TimedeltaIndex

   .. rubric:: Notes

   Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
   exactly three must be specified. If ``freq`` is omitted, the resulting
   ``TimedeltaIndex`` will have ``periods`` linearly spaced elements between
   ``start`` and ``end`` (closed on both sides).

   To learn more about the frequency strings, please see `this link
   <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.


.. py:function:: timedelta_range(start=None, end=None, periods=None, freq=None, name=None, closed=None, **kwargs)

   Return a fixed frequency TimedeltaIndex, with day as the default
   frequency. Alias for ``ak.Timedelta(pd.timedelta_range(args))``.
   Subject to size limit imposed by client.maxTransferBytes.

   :param start: Left bound for generating timedeltas.
   :type start: str or timedelta-like, default None
   :param end: Right bound for generating timedeltas.
   :type end: str or timedelta-like, default None
   :param periods: Number of periods to generate.
   :type periods: int, default None
   :param freq: Frequency strings can have multiples, e.g. '5H'.
   :type freq: str or DateOffset, default 'D'
   :param name: Name of the resulting TimedeltaIndex.
   :type name: str, default None
   :param closed: Make the interval closed with respect to the given frequency to
                  the 'left', 'right', or both sides (None).
   :type closed: str, default None

   :returns: **rng**
   :rtype: TimedeltaIndex

   .. rubric:: Notes

   Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
   exactly three must be specified. If ``freq`` is omitted, the resulting
   ``TimedeltaIndex`` will have ``periods`` linearly spaced elements between
   ``start`` and ``end`` (closed on both sides).

   To learn more about the frequency strings, please see `this link
   <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.


.. py:function:: to_csv(columns: Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]], List[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]]], prefix_path: str, names: Optional[List[str]] = None, col_delim: str = ',', overwrite: bool = False)

   Write Arkouda object(s) to CSV file(s). All CSV Files written by Arkouda
   include a header denoting data types of the columns.

   :param columns: The objects to be written to CSV file. If a mapping is used and `names` is None
                   the keys of the mapping will be used as the dataset names.
   :type columns: Mapping[str, pdarray] or List[pdarray]
   :param prefix_path: The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                       when they are written to disk.
   :type prefix_path: str
   :param names: names of dataset to be written. Order should correspond to the order of data
                 provided in `columns`.
   :type names: List[str] (Optional)
   :param col_delim: Defaults to ",". Value to be used to separate columns within the file.
                     Please be sure that the value used DOES NOT appear in your dataset.
   :type col_delim: str
   :param overwrite: Defaults to False. If True, any existing files matching your provided prefix_path will
                     be overwritten. If False, an error will be returned if existing files are found.
   :type overwrite: bool

   :rtype: None

   :raises ValueError: Raised if any datasets are present in all csv files or if one or
       more of the specified files do not exist
   :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
       If `allow_errors` is true this may be raised if no values are returned
       from the server.
   :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

   .. seealso:: :obj:`read_csv`

   .. rubric:: Notes

   - CSV format is not currently supported by load/load_all operations
   - The column delimiter is expected to be the same for column names and data
   - Be sure that column delimiters are not found within your data.
   - All CSV files must delimit rows using newline (``\n``) at this time.
   - Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
     bytes as uint(8).


.. py:function:: to_hdf(columns: Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray]], List[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray]]], prefix_path: str, names: Optional[List[str]] = None, mode: str = 'truncate', file_type: str = 'distribute') -> None

   Save multiple named pdarrays to HDF5 files.

   :param columns: Collection of arrays to save
   :type columns: dict or list of pdarrays
   :param prefix_path: Directory and filename prefix for output files
   :type prefix_path: str
   :param names: Dataset names for the pdarrays
   :type names: list of str
   :param mode: By default, truncate (overwrite) the output files if they exist.
                If 'append', attempt to create new dataset in existing files.
   :type mode: {'truncate' | 'append'}
   :param file_type: Default: distribute
                     Single writes the dataset to a single file
                     Distribute writes the dataset to a file per locale
   :type file_type: str ("single" | "distribute")

   :rtype: None

   :raises ValueError: Raised if (1) the lengths of columns and values differ or (2) the mode
       is not 'truncate' or 'append'
   :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

   .. seealso:: :obj:`to_parquet`, :obj:`load`, :obj:`load_all`, :obj:`read`

   .. rubric:: Notes

   Creates one file per locale containing that locale's chunk of each pdarray.
   If columns is a dictionary, the keys are used as the HDF5 dataset names.
   Otherwise, if no names are supplied, 0-up integers are used. By default,
   any existing files at path_prefix will be overwritten, unless the user
   specifies the 'append' mode, in which case arkouda will attempt to add
   <columns> as new datasets to existing files. If the wrong number of files
   is present or dataset names already exist, a RuntimeError is raised.

   .. rubric:: Examples

   >>> a = ak.arange(25)
   >>> b = ak.arange(25)

   >>> # Save with mapping defining dataset names
   >>> ak.to_hdf({'a': a, 'b': b}, 'path/name_prefix')

   >>> # Save using names instead of mapping
   >>> ak.to_hdf([a, b], 'path/name_prefix', names=['a', 'b'])


.. py:function:: to_parquet(columns: Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray]], List[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray]]], prefix_path: str, names: Optional[List[str]] = None, mode: str = 'truncate', compression: Optional[str] = None, convert_categoricals: bool = False) -> None

   Save multiple named pdarrays to Parquet files.

   :param columns: Collection of arrays to save
   :type columns: dict or list of pdarrays
   :param prefix_path: Directory and filename prefix for output files
   :type prefix_path: str
   :param names: Dataset names for the pdarrays
   :type names: list of str
   :param mode: By default, truncate (overwrite) the output files if they exist.
                If 'append', attempt to create new dataset in existing files.
                'append' is deprecated, please use the multi-column write
   :type mode: {'truncate' | 'append'}
   :param compression:     Default None
                           Provide the compression type to use when writing the file.
                           Supported values: snappy, gzip, brotli, zstd, lz4
                       convert_categoricals: bool
                           Defaults to False
                           Parquet requires all columns to be the same size and Categoricals
                           don't satisfy that requirement.
                           if set, write the equivalent Strings in place of any Categorical columns.
   :type compression: str (Optional)

   :rtype: None

   :raises ValueError: Raised if (1) the lengths of columns and values differ or (2) the mode
       is not 'truncate' or 'append'
   :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

   .. seealso:: :obj:`to_hdf`, :obj:`load`, :obj:`load_all`, :obj:`read`

   .. rubric:: Notes

   Creates one file per locale containing that locale's chunk of each pdarray.
   If columns is a dictionary, the keys are used as the Parquet column names.
   Otherwise, if no names are supplied, 0-up integers are used. By default,
   any existing files at path_prefix will be deleted
   (regardless of whether they would be overwritten), unless the user
   specifies the 'append' mode, in which case arkouda will attempt to add
   <columns> as new datasets to existing files. If the wrong number of files
   is present or dataset names already exist, a RuntimeError is raised.

   .. rubric:: Examples

   >>> a = ak.arange(25)
   >>> b = ak.arange(25)

   >>> # Save with mapping defining dataset names
   >>> ak.to_parquet({'a': a, 'b': b}, 'path/name_prefix')

   >>> # Save using names instead of mapping
   >>> ak.to_parquet([a, b], 'path/name_prefix', names=['a', 'b'])


.. py:function:: to_zarr(store_path: str, arr: arkouda.pdarrayclass.pdarray, chunk_shape)

   Writes a pdarray to disk as a Zarr store. Supports multi-dimensional pdarrays of numeric types.
   To use this function, ensure you have installed the blosc dependency (`make install-blosc`)
   and have included `ZarrMsg.chpl` in the `ServerModules.cfg` file.

   :param store_path: The path at which Zarr store should be written
   :type store_path: str
   :param arr: The pdarray to be written to disk
   :type arr: pdarray
   :param chunk_shape: The shape of the chunks to be used in the Zarr store
   :type chunk_shape: tuple

   :raises ValueError: Raised if the number of dimensions in the chunk shape does not match
       the number of dimensions in the array or if the array is not a 32 or 64 bit numeric type


.. py:function:: transpose(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Compute the transpose of a matrix.

   :param pda:
   :type pda: pdarray

   :returns: the transpose of the input matrix
   :rtype: pdarray

   .. rubric:: Examples

   >>> a = ak.array([[1,2,3,4,5],[1,2,3,4,5]])
   >>> ak.transpose(a)
   array([array([1 1]) array([2 2]) array([3 3]) array([4 4]) array([5 5])])

   .. rubric:: Notes

   Server returns an error if rank of pda < 2


.. py:function:: tril(pda: arkouda.pdarrayclass.pdarray, diag: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64] = 0) -> arkouda.pdarrayclass.pdarray

   Return a copy of the pda with the upper triangle zeroed out

   :param pda:
   :type pda: pdarray
   :param diag: | if diag = 0, zeros start just above the main diagonal
                | if diag = 1, zeros start at the main diagonal
                | if diag = 2, zeros start just below the main diagonal
                | etc. Default set to 0.
   :type diag: int_scalars, optional

   :returns: a copy of pda with zeros in the upper triangle
   :rtype: pdarray

   .. rubric:: Examples

   >>> a = ak.array([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7],[4,5,6,7,8],[5,6,7,8,9]])
   >>> ak.tril(a,diag=4)
   array([array([1 2 3 4 5]) array([2 3 4 5 6]) array([3 4 5 6 7])
   array([4 5 6 7 8]) array([5 6 7 8 9])])
   >>> ak.tril(a,diag=3)
   array([array([1 2 3 4 0]) array([2 3 4 5 6]) array([3 4 5 6 7])
   array([4 5 6 7 8]) array([5 6 7 8 9])])
   >>> ak.tril(a,diag=2)
   array([array([1 2 3 0 0]) array([2 3 4 5 0]) array([3 4 5 6 7])
   array([4 5 6 7 8]) array([5 6 7 8 9])])
   >>> ak.tril(a,diag=1)
   array([array([1 2 0 0 0]) array([2 3 4 0 0]) array([3 4 5 6 0])
   array([4 5 6 7 8]) array([5 6 7 8 9])])
   >>> ak.tril(a,diag=0)
   array([array([1 0 0 0 0]) array([2 3 0 0 0]) array([3 4 5 0 0])
   array([4 5 6 7 0]) array([5 6 7 8 9])])

   .. rubric:: Notes

   Server returns an error if rank of pda < 2


.. py:function:: triu(pda: arkouda.pdarrayclass.pdarray, diag: Union[int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64] = 0) -> arkouda.pdarrayclass.pdarray

   Return a copy of the pda with the lower triangle zeroed out

   :param pda:
   :type pda: pdarray
   :param diag: | if diag = 0, zeros start just below the main diagonal
                | if diag = 1, zeros start at the main diagonal
                | if diag = 2, zeros start just above the main diagonal
                | etc. Default set to 0.
   :type diag: int_scalars, default=0

   :returns: a copy of pda with zeros in the lower triangle
   :rtype: pdarray

   .. rubric:: Examples

   >>> a = ak.array([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7],[4,5,6,7,8],[5,6,7,8,9]])
   >>> ak.triu(a,diag=0)
   array([array([1 2 3 4 5]) array([0 3 4 5 6]) array([0 0 5 6 7])
   array([0 0 0 7 8]) array([0 0 0 0 9])])
   >>> ak.triu(a,diag=1)
   array([array([0 2 3 4 5]) array([0 0 4 5 6]) array([0 0 0 6 7])
   array([0 0 0 0 8]) array([0 0 0 0 0])])
   >>> ak.triu(a,diag=2)
   array([array([0 0 3 4 5]) array([0 0 0 5 6]) array([0 0 0 0 7])
   array([0 0 0 0 0]) array([0 0 0 0 0])])
   >>> ak.triu(a,diag=3)
   array([array([0 0 0 4 5]) array([0 0 0 0 6]) array([0 0 0 0 0])
   array([0 0 0 0 0]) array([0 0 0 0 0])])
   >>> ak.triu(a,diag=4)
   array([array([0 0 0 0 5]) array([0 0 0 0 0]) array([0 0 0 0 0])
   array([0 0 0 0 0]) array([0 0 0 0 0])])

   .. rubric:: Notes

   Server returns an error if rank of pda < 2


.. py:function:: trunc(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Return the element-wise truncation of the array.

   :param pda:
   :type pda: pdarray

   :returns: A pdarray containing input array elements truncated to the nearest integer
   :rtype: pdarray

   :raises TypeError: Raised if the parameter is not a pdarray

   .. rubric:: Examples

   >>> ak.trunc(ak.array([1.1, 2.5, 3.14159]))
   array([1.00000000000000000 2.00000000000000000 3.00000000000000000])


.. py:function:: typename(char)

   Return a description for the given data type code.

   :param char: Data type code.
   :type char: str

   :returns: **out** -- Description of the input data type code.
   :rtype: str

   .. seealso:: :obj:`dtype`, :obj:`typecodes`

   .. rubric:: Examples

   >>> typechars = ['S1', '?', 'B', 'D', 'G', 'F', 'I', 'H', 'L', 'O', 'Q',
   ...              'S', 'U', 'V', 'b', 'd', 'g', 'f', 'i', 'h', 'l', 'q']
   >>> for typechar in typechars:
   ...     print(typechar, ' : ', np.typename(typechar))
   ...
   S1  :  character
   ?  :  bool
   B  :  unsigned char
   D  :  complex double precision
   G  :  complex long double precision
   F  :  complex single precision
   I  :  unsigned integer
   H  :  unsigned short
   L  :  unsigned long integer
   O  :  object
   Q  :  unsigned long long integer
   S  :  string
   U  :  unicode
   V  :  void
   b  :  signed char
   d  :  double precision
   g  :  long precision
   f  :  single precision
   i  :  integer
   h  :  short
   l  :  long integer
   q  :  long long integer


.. py:class:: ubyte(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned char``.

       :Character code: ``'B'``
       :Canonical name: `numpy.ubyte`
       :Alias on this platform (Linux x86_64): `numpy.uint8`: 8-bit unsigned integer (``0`` to ``255``).



   .. py:method:: bit_count(*args, **kwargs)

      uint8.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint8(127).bit_count()
              7




.. py:class:: uint(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned long``.

       :Character code: ``'L'``
       :Canonical name: `numpy.uint`
       :Alias on this platform (Linux x86_64): `numpy.uint64`: 64-bit unsigned integer (``0`` to ``18_446_744_073_709_551_615``).
       :Alias on this platform (Linux x86_64): `numpy.uintp`: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      uint64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint64(127).bit_count()
              7




.. py:class:: uint16(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned short``.

       :Character code: ``'H'``
       :Canonical name: `numpy.ushort`
       :Alias on this platform (Linux x86_64): `numpy.uint16`: 16-bit unsigned integer (``0`` to ``65_535``).



   .. py:method:: bit_count(*args, **kwargs)

      uint16.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint16(127).bit_count()
              7




.. py:class:: uint32(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned int``.

       :Character code: ``'I'``
       :Canonical name: `numpy.uintc`
       :Alias on this platform (Linux x86_64): `numpy.uint32`: 32-bit unsigned integer (``0`` to ``4_294_967_295``).



   .. py:method:: bit_count(*args, **kwargs)

      uint32.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint32(127).bit_count()
              7




.. py:class:: uint64(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned long``.

       :Character code: ``'L'``
       :Canonical name: `numpy.uint`
       :Alias on this platform (Linux x86_64): `numpy.uint64`: 64-bit unsigned integer (``0`` to ``18_446_744_073_709_551_615``).
       :Alias on this platform (Linux x86_64): `numpy.uintp`: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      uint64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint64(127).bit_count()
              7




.. py:class:: uint8(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned char``.

       :Character code: ``'B'``
       :Canonical name: `numpy.ubyte`
       :Alias on this platform (Linux x86_64): `numpy.uint8`: 8-bit unsigned integer (``0`` to ``255``).



   .. py:method:: bit_count(*args, **kwargs)

      uint8.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint8(127).bit_count()
              7




.. py:class:: uintc(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned int``.

       :Character code: ``'I'``
       :Canonical name: `numpy.uintc`
       :Alias on this platform (Linux x86_64): `numpy.uint32`: 32-bit unsigned integer (``0`` to ``4_294_967_295``).



   .. py:method:: bit_count(*args, **kwargs)

      uint32.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint32(127).bit_count()
              7




.. py:class:: uintp(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned long``.

       :Character code: ``'L'``
       :Canonical name: `numpy.uint`
       :Alias on this platform (Linux x86_64): `numpy.uint64`: 64-bit unsigned integer (``0`` to ``18_446_744_073_709_551_615``).
       :Alias on this platform (Linux x86_64): `numpy.uintp`: Unsigned integer large enough to fit pointer, compatible with C ``uintptr_t``.



   .. py:method:: bit_count(*args, **kwargs)

      uint64.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint64(127).bit_count()
              7




.. py:class:: ulonglong(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Signed integer type, compatible with C ``unsigned long long``.

       :Character code: ``'Q'``



   .. py:method:: bit_count(*args, **kwargs)


.. py:function:: uniform(size: arkouda.numpy.dtypes.int_scalars, low: arkouda.numpy.dtypes.numeric_scalars = float(0.0), high: arkouda.numpy.dtypes.numeric_scalars = 1.0, seed: Union[None, arkouda.numpy.dtypes.int_scalars] = None) -> arkouda.pdarrayclass.pdarray

   Generate a pdarray with uniformly distributed random float values
   in a specified range.

   :param low: The low value (inclusive) of the range, defaults to 0.0
   :type low: float_scalars
   :param high: The high value (inclusive) of the range, defaults to 1.0
   :type high: float_scalars
   :param size: The length of the returned array
   :type size: int_scalars
   :param seed: Value used to initialize the random number generator
   :type seed: int_scalars, optional

   :returns: Values drawn uniformly from the specified range
   :rtype: pdarray, float64

   :raises TypeError: Raised if dtype.name not in DTypes, size is not an int, or if
       either low or high is not an int or float
   :raises ValueError: Raised if size < 0 or if high < low

   .. rubric:: Notes

   The logic for uniform is delegated to the ak.randint method which
   is invoked with a dtype of float64

   .. rubric:: Examples

   >>> ak.uniform(3,seed=1701)
   array([0.011410423448327005 0.73618171558685619 0.12367222192448891])

   >>> ak.uniform(size=3,low=0,high=5,seed=0)
   array([0.30013431967121934 0.47383036230759112 1.0441791878997098])


.. py:function:: union1d(pda1: arkouda.groupbyclass.groupable, pda2: arkouda.groupbyclass.groupable) -> arkouda.groupbyclass.groupable

   Find the union of two arrays/List of Arrays.

   Return the unique, sorted array of values that are in either
   of the two input arrays.

   :param pda1: Input array/Sequence of groupable objects
   :type pda1: pdarray/Sequence[pdarray, Strings, Categorical]
   :param pda2: Input array/sequence of groupable objects
   :type pda2: pdarray/List

   :returns: Unique, sorted union of the input arrays.
   :rtype: pdarray/groupable

   :raises TypeError: Raised if either pda1 or pda2 is not a pdarray
   :raises RuntimeError: Raised if the dtype of either array is not supported

   .. seealso:: :obj:`intersect1d`, :obj:`arkouda.groupbyclass.unique`

   .. rubric:: Notes

   ak.union1d is not supported for bool or float64 pdarrays

   .. rubric:: Examples

   >>>
   # 1D Example
   >>> ak.union1d(ak.array([-1, 0, 1]), ak.array([-2, 0, 2]))
   array([-2, -1, 0, 1, 2])
   #Multi-Array Example
   >>> a = ak.arange(1, 6)
   >>> b = ak.array([1, 5, 3, 4, 2])
   >>> c = ak.array([1, 4, 3, 2, 5])
   >>> d = ak.array([1, 2, 3, 5, 4])
   >>> multia = [a, a, a]
   >>> multib = [b, c, d]
   >>> ak.union1d(multia, multib)
   [array[1, 2, 2, 3, 4, 4, 5, 5], array[1, 2, 5, 3, 2, 4, 4, 5], array[1, 2, 4, 3, 5, 4, 2, 5]]


.. py:function:: unique(pda: groupable, return_groups: bool = False, assume_sorted: bool = False, return_indices: bool = False) -> Union[groupable, Tuple[groupable, pdarray, pdarray, int]]

   Find the unique elements of an array.

   Returns the unique elements of an array, sorted if the values are integers.
   There is an optional output in addition to the unique elements: the number
   of times each unique value comes up in the input array.

   :param pda: Input array.
   :type pda: (list of) pdarray, Strings, or Categorical
   :param return_groups: If True, also return grouping information for the array.
   :type return_groups: bool, optional
   :param assume_sorted: If True, assume pda is sorted and skip sorting step
   :type assume_sorted: bool, optional
   :param return_indices: Only applicable if return_groups is True.
                          If True, return unique key indices along with other groups
   :type return_indices: bool, optional

   :returns: * **unique** (*(list of) pdarray, Strings, or Categorical*) -- The unique values. If input dtype is int64, return values will be sorted.
             * **permutation** (*pdarray, optional*) -- Permutation that groups equivalent values together (only when return_groups=True)
             * **segments** (*pdarray, optional*) -- The offset of each group in the permuted array (only when return_groups=True)

   :raises TypeError: Raised if pda is not a pdarray or Strings object
   :raises RuntimeError: Raised if the pdarray or Strings dtype is unsupported

   .. rubric:: Notes

   For integer arrays, this function checks to see whether `pda` is sorted
   and, if so, whether it is already unique. This step can save considerable
   computation. Otherwise, this function will sort `pda`.

   .. rubric:: Examples

   >>> A = ak.array([3, 2, 1, 1, 2, 3])
   >>> ak.unique(A)
   array([1, 2, 3])


.. py:function:: unique(pda: groupable, return_groups: bool = False, assume_sorted: bool = False, return_indices: bool = False) -> Union[groupable, Tuple[groupable, pdarray, pdarray, int]]

   Find the unique elements of an array.

   Returns the unique elements of an array, sorted if the values are integers.
   There is an optional output in addition to the unique elements: the number
   of times each unique value comes up in the input array.

   :param pda: Input array.
   :type pda: (list of) pdarray, Strings, or Categorical
   :param return_groups: If True, also return grouping information for the array.
   :type return_groups: bool, optional
   :param assume_sorted: If True, assume pda is sorted and skip sorting step
   :type assume_sorted: bool, optional
   :param return_indices: Only applicable if return_groups is True.
                          If True, return unique key indices along with other groups
   :type return_indices: bool, optional

   :returns: * **unique** (*(list of) pdarray, Strings, or Categorical*) -- The unique values. If input dtype is int64, return values will be sorted.
             * **permutation** (*pdarray, optional*) -- Permutation that groups equivalent values together (only when return_groups=True)
             * **segments** (*pdarray, optional*) -- The offset of each group in the permuted array (only when return_groups=True)

   :raises TypeError: Raised if pda is not a pdarray or Strings object
   :raises RuntimeError: Raised if the pdarray or Strings dtype is unsupported

   .. rubric:: Notes

   For integer arrays, this function checks to see whether `pda` is sorted
   and, if so, whether it is already unique. This step can save considerable
   computation. Otherwise, this function will sort `pda`.

   .. rubric:: Examples

   >>> A = ak.array([3, 2, 1, 1, 2, 3])
   >>> ak.unique(A)
   array([1, 2, 3])


.. py:function:: unique(pda: groupable, return_groups: bool = False, assume_sorted: bool = False, return_indices: bool = False) -> Union[groupable, Tuple[groupable, pdarray, pdarray, int]]

   Find the unique elements of an array.

   Returns the unique elements of an array, sorted if the values are integers.
   There is an optional output in addition to the unique elements: the number
   of times each unique value comes up in the input array.

   :param pda: Input array.
   :type pda: (list of) pdarray, Strings, or Categorical
   :param return_groups: If True, also return grouping information for the array.
   :type return_groups: bool, optional
   :param assume_sorted: If True, assume pda is sorted and skip sorting step
   :type assume_sorted: bool, optional
   :param return_indices: Only applicable if return_groups is True.
                          If True, return unique key indices along with other groups
   :type return_indices: bool, optional

   :returns: * **unique** (*(list of) pdarray, Strings, or Categorical*) -- The unique values. If input dtype is int64, return values will be sorted.
             * **permutation** (*pdarray, optional*) -- Permutation that groups equivalent values together (only when return_groups=True)
             * **segments** (*pdarray, optional*) -- The offset of each group in the permuted array (only when return_groups=True)

   :raises TypeError: Raised if pda is not a pdarray or Strings object
   :raises RuntimeError: Raised if the pdarray or Strings dtype is unsupported

   .. rubric:: Notes

   For integer arrays, this function checks to see whether `pda` is sorted
   and, if so, whether it is already unique. This step can save considerable
   computation. Otherwise, this function will sort `pda`.

   .. rubric:: Examples

   >>> A = ak.array([3, 2, 1, 1, 2, 3])
   >>> ak.unique(A)
   array([1, 2, 3])


.. py:function:: unregister(name: str) -> str

.. py:function:: unregister_all(names: list)

   Unregister all names provided

   :param names: List of names used to register objects to be unregistered
   :type names: list

   :rtype: None


.. py:function:: unregister_pdarray_by_name(user_defined_name: str) -> None

   Unregister a named pdarray in the arkouda server which was previously
   registered using register() and/or attahced to using attach_pdarray()

   :param user_defined_name: user defined name which array was registered under
   :type user_defined_name: str

   :rtype: None

   :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

   .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`list_registry`, :obj:`attach`

   .. rubric:: Notes

   Registered names/pdarrays in the server are immune to deletion until
   they are unregistered.

   .. rubric:: Examples

   >>> a = zeros(100)
   >>> a.register("my_zeros")
   >>> # potentially disconnect from server and reconnect to server
   >>> b = ak.attach_pdarray("my_zeros")
   >>> # ...other work...
   >>> ak.unregister_pdarray_by_name(b)


.. py:class:: unsignedinteger(value)

   Bases: :py:obj:`numpy.integer`


   Abstract base class of all unsigned integer scalar types.



.. py:function:: unsqueeze(p)

.. py:function:: update_hdf(columns: Union[Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray]], List[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.segarray.SegArray]]], prefix_path: str, names: Optional[List[str]] = None, repack: bool = True)

   Overwrite the datasets with name appearing in names or keys in columns if columns
   is a dictionary

   :param columns: Collection of arrays to save
   :type columns: dict or list of pdarrays
   :param prefix_path: Directory and filename prefix for output files
   :type prefix_path: str
   :param names: Dataset names for the pdarrays
   :type names: list of str
   :param repack: Default: True
                  HDF5 does not release memory on delete. When True, the inaccessible
                  data (that was overwritten) is removed. When False, the data remains, but is
                  inaccessible. Setting to false will yield better performance, but will cause
                  file sizes to expand.
   :type repack: bool

   :raises RuntimeError: Raised if a server-side error is thrown saving the datasets

   .. rubric:: Notes

   - If file does not contain File_Format attribute to indicate how it was saved,
     the file name is checked for _LOCALE#### to determine if it is distributed.
   - If the datasets provided do not exist, they will be added
   - Because HDF5 deletes do not release memory, this will create a copy of the
     file with the new data
   - This workflow is slightly different from `to_hdf` to prevent reading and
     creating a copy of the file for each dataset


.. py:class:: ushort(value)

   Bases: :py:obj:`numpy.unsignedinteger`


   Unsigned integer type, compatible with C ``unsigned short``.

       :Character code: ``'H'``
       :Canonical name: `numpy.ushort`
       :Alias on this platform (Linux x86_64): `numpy.uint16`: 16-bit unsigned integer (``0`` to ``65_535``).



   .. py:method:: bit_count(*args, **kwargs)

      uint16.bit_count() -> int

              Computes the number of 1-bits in the absolute value of the input.
              Analogous to the builtin `int.bit_count` or ``popcount`` in C++.

              Examples
              --------
              >>> np.uint16(127).bit_count()
              7




.. py:function:: value_counts(pda: arkouda.pdarrayclass.pdarray) -> tuple

   Count the occurrences of the unique values of an array.

   :param pda: The array of values to count
   :type pda: pdarray

   :returns: * **unique_values** (*pdarray, int64 or Strings*) -- The unique values, sorted in ascending order
             * **counts** (*pdarray, int64*) -- The number of times the corresponding unique value occurs

   :raises TypeError: Raised if the parameter is not a pdarray

   .. seealso:: :obj:`unique`, :obj:`histogram`

   .. rubric:: Notes

   This function differs from ``histogram()`` in that it only returns
   counts for values that are present, leaving out empty "bins". This
   function delegates all logic to the unique() method where the
   return_counts parameter is set to True.

   .. rubric:: Examples

   >>> A = ak.array([2, 0, 2, 4, 0, 0])
   >>> ak.value_counts(A)
   (array([0 2 4]), array([3 2 1]))


.. py:function:: var(pda: pdarray, ddof: arkouda.numpy.dtypes.int_scalars = 0) -> numpy.float64

   Return the variance of values in the array.

   :param pda: Values for which to calculate the variance
   :type pda: pdarray
   :param ddof: "Delta Degrees of Freedom" used in calculating var
   :type ddof: int_scalars

   :returns: The scalar variance of the array
   :rtype: np.float64

   :raises TypeError: Raised if pda is not a pdarray instance
   :raises ValueError: Raised if the ddof >= pdarray size
   :raises RuntimeError: Raised if there's a server-side error thrown

   .. seealso:: :obj:`mean`, :obj:`std`

   .. rubric:: Notes

   The variance is the average of the squared deviations from the mean,
   i.e.,  ``var = mean((x - x.mean())**2)``.

   The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
   If, however, `ddof` is specified, the divisor ``N - ddof`` is used
   instead.  In standard statistical practice, ``ddof=1`` provides an
   unbiased estimator of the variance of a hypothetical infinite population.
   ``ddof=0`` provides a maximum likelihood estimate of the variance for
   normally distributed variables.


.. py:function:: vecdot(x1: arkouda.pdarrayclass.pdarray, x2: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Compute the generalized dot product of two vectors along the given axis.
   Assumes that both tensors have already been broadcast to the same shape.

   :param x1:
   :type x1: pdarray
   :param x2:
   :type x2: pdarray

   :returns: x1 vecdot x2
   :rtype: pdarray

   .. rubric:: Examples

   >>> a = ak.array([[1,2,3,4,5],[1,2,3,4,5]])
   >>> b = ak.array([[2,2,2,2,2],[2,2,2,2,2]])
   >>> ak.vecdot(a,b)
   array([4 8 12 16 20])
   >>> ak.vecdot(b,a)
   array([4 8 12 16 20])

   :raises ValueTypeError: Raised if x1 and x2 are not of matching shape or if rank of x1 < 2


.. py:class:: void(value)

   Bases: :py:obj:`numpy.flexible`


   np.void(length_or_data, /, dtype=None)

       Create a new structured or unstructured void scalar.

       Parameters
       ----------
       length_or_data : int, array-like, bytes-like, object
          One of multiple meanings (see notes).  The length or
          bytes data of an unstructured void.  Or alternatively,
          the data to be stored in the new scalar when `dtype`
          is provided.
          This can be an array-like, in which case an array may
          be returned.
       dtype : dtype, optional
           If provided the dtype of the new scalar.  This dtype must
           be "void" dtype (i.e. a structured or unstructured void,
           see also :ref:`defining-structured-types`).

          ..versionadded:: 1.24

       Notes
       -----
       For historical reasons and because void scalars can represent both
       arbitrary byte data and structured dtypes, the void constructor
       has three calling conventions:

       1. ``np.void(5)`` creates a ``dtype="V5"`` scalar filled with five
          ``\0`` bytes.  The 5 can be a Python or NumPy integer.
       2. ``np.void(b"bytes-like")`` creates a void scalar from the byte string.
          The dtype itemsize will match the byte string length, here ``"V10"``.
       3. When a ``dtype=`` is passed the call is roughly the same as an
          array creation.  However, a void scalar rather than array is returned.

       Please see the examples which show all three different conventions.

       Examples
       --------
       >>> np.void(5)
       void(b'\x00\x00\x00\x00\x00')
       >>> np.void(b'abcd')
       void(b'\x61\x62\x63\x64')
       >>> np.void((5, 3.2, "eggs"), dtype="i,d,S5")
       (5, 3.2, b'eggs')  # looks like a tuple, but is `np.void`
       >>> np.void(3, dtype=[('x', np.int8), ('y', np.int8)])
       (3, 3)  # looks like a tuple, but is `np.void`

       :Character code: ``'V'``



.. py:function:: vstack(tup: Union[Tuple[arkouda.pdarrayclass.pdarray], List[arkouda.pdarrayclass.pdarray]], *, dtype: Optional[Union[type, str]] = None, casting: Literal['no', 'equiv', 'safe', 'same_kind', 'unsafe'] = 'same_kind') -> arkouda.pdarrayclass.pdarray

   Stack a sequence of arrays vertically (row-wise).

   This is equivalent to concatenation along the first axis after 1-D arrays of
   shape `(N,)` have been reshaped to `(1,N)`.

   :param tup: The arrays to be stacked
   :type tup: Tuple[pdarray]
   :param dtype: The data-type of the output array. If not provided, the output
                 array will be determined using `np.common_type` on the
                 input arrays Defaults to None
   :type dtype: Optional[Union[type, str]], optional
   :param casting: Controls what kind of data casting may occur - currently unused
   :type casting: {"no", "equiv", "safe", "same_kind", "unsafe"], optional

   :returns: *
             * *pdarray* -- The stacked array


.. py:function:: where(condition: arkouda.pdarrayclass.pdarray, A: Union[str, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical], B: Union[str, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical]) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical]

   Returns an array with elements chosen from A and B based upon a
   conditioning array. As is the case with numpy.where, the return array
   consists of values from the first array (A) where the conditioning array
   elements are True and from the second array (B) where the conditioning
   array elements are False.

   :param condition: Used to choose values from A or B
   :type condition: pdarray
   :param A: Value(s) used when condition is True
   :type A: str, numeric_scalars, pdarray, Strings, or Categorical
   :param B: Value(s) used when condition is False
   :type B: str, numeric_scalars, pdarray, Strings, or Categorical

   :returns: Values chosen from A where the condition is True and B where
             the condition is False
   :rtype: pdarray

   :raises TypeError: Raised if the condition object is not a pdarray, if A or B is not
       an int, np.int64, float, np.float64, bool, pdarray, str, Strings, Categorical
       if pdarray dtypes are not supported or do not match, or multiple
       condition clauses (see Notes section) are applied
   :raises ValueError: Raised if the shapes of the condition, A, and B pdarrays are unequal

   .. rubric:: Examples

   >>> a1 = ak.arange(1,10)
   >>> a2 = ak.ones(9, dtype=np.int64)
   >>> cond = a1 < 5
   >>> ak.where(cond,a1,a2)
   array([1 2 3 4 1 1 1 1 1])

   >>> a1 = ak.arange(1,10)
   >>> a2 = ak.ones(9, dtype=np.int64)
   >>> cond = a1 == 5
   >>> ak.where(cond,a1,a2)
   array([1 1 1 1 5 1 1 1 1])

   >>> a1 = ak.arange(1,10)
   >>> a2 = 10
   >>> cond = a1 < 5
   >>> ak.where(cond,a1,a2)
   array([1 2 3 4 10 10 10 10 10])

   >>> s1 = ak.array([f'str {i}' for i in range(10)])
   >>> s2 = 'str 21'
   >>> cond = (ak.arange(10) % 2 == 0)
   >>> ak.where(cond,s1,s2)
   array(['str 0', 'str 21', 'str 2', 'str 21', 'str 4',
   'str 21', 'str 6', 'str 21', 'str 8', 'str 21'])

   >>> c1 = ak.Categorical(ak.array([f'str {i}' for i in range(10)]))
   >>> c2 = ak.Categorical(ak.array([f'str {i}' for i in range(9, -1, -1)]))
   >>> cond = (ak.arange(10) % 2 == 0)
   >>> ak.where(cond,c1,c2)
   array(['str 0', 'str 8', 'str 2', 'str 6', 'str 4',
   'str 4', 'str 6', 'str 2', 'str 8', 'str 0'])

   .. rubric:: Notes

   A and B must have the same dtype and only one conditional clause
   is supported e.g., n < 5, n > 1, which is supported in numpy
   is not currently supported in Arkouda


.. py:function:: where(condition: arkouda.pdarrayclass.pdarray, A: Union[str, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical], B: Union[str, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical]) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical]

   Returns an array with elements chosen from A and B based upon a
   conditioning array. As is the case with numpy.where, the return array
   consists of values from the first array (A) where the conditioning array
   elements are True and from the second array (B) where the conditioning
   array elements are False.

   :param condition: Used to choose values from A or B
   :type condition: pdarray
   :param A: Value(s) used when condition is True
   :type A: str, numeric_scalars, pdarray, Strings, or Categorical
   :param B: Value(s) used when condition is False
   :type B: str, numeric_scalars, pdarray, Strings, or Categorical

   :returns: Values chosen from A where the condition is True and B where
             the condition is False
   :rtype: pdarray

   :raises TypeError: Raised if the condition object is not a pdarray, if A or B is not
       an int, np.int64, float, np.float64, bool, pdarray, str, Strings, Categorical
       if pdarray dtypes are not supported or do not match, or multiple
       condition clauses (see Notes section) are applied
   :raises ValueError: Raised if the shapes of the condition, A, and B pdarrays are unequal

   .. rubric:: Examples

   >>> a1 = ak.arange(1,10)
   >>> a2 = ak.ones(9, dtype=np.int64)
   >>> cond = a1 < 5
   >>> ak.where(cond,a1,a2)
   array([1 2 3 4 1 1 1 1 1])

   >>> a1 = ak.arange(1,10)
   >>> a2 = ak.ones(9, dtype=np.int64)
   >>> cond = a1 == 5
   >>> ak.where(cond,a1,a2)
   array([1 1 1 1 5 1 1 1 1])

   >>> a1 = ak.arange(1,10)
   >>> a2 = 10
   >>> cond = a1 < 5
   >>> ak.where(cond,a1,a2)
   array([1 2 3 4 10 10 10 10 10])

   >>> s1 = ak.array([f'str {i}' for i in range(10)])
   >>> s2 = 'str 21'
   >>> cond = (ak.arange(10) % 2 == 0)
   >>> ak.where(cond,s1,s2)
   array(['str 0', 'str 21', 'str 2', 'str 21', 'str 4',
   'str 21', 'str 6', 'str 21', 'str 8', 'str 21'])

   >>> c1 = ak.Categorical(ak.array([f'str {i}' for i in range(10)]))
   >>> c2 = ak.Categorical(ak.array([f'str {i}' for i in range(9, -1, -1)]))
   >>> cond = (ak.arange(10) % 2 == 0)
   >>> ak.where(cond,c1,c2)
   array(['str 0', 'str 8', 'str 2', 'str 6', 'str 4',
   'str 4', 'str 6', 'str 2', 'str 8', 'str 0'])

   .. rubric:: Notes

   A and B must have the same dtype and only one conditional clause
   is supported e.g., n < 5, n > 1, which is supported in numpy
   is not currently supported in Arkouda


.. py:function:: where(condition: arkouda.pdarrayclass.pdarray, A: Union[str, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical], B: Union[str, float, numpy.float64, numpy.float32, int, numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64, arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical]) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, ~Categorical]

   Returns an array with elements chosen from A and B based upon a
   conditioning array. As is the case with numpy.where, the return array
   consists of values from the first array (A) where the conditioning array
   elements are True and from the second array (B) where the conditioning
   array elements are False.

   :param condition: Used to choose values from A or B
   :type condition: pdarray
   :param A: Value(s) used when condition is True
   :type A: str, numeric_scalars, pdarray, Strings, or Categorical
   :param B: Value(s) used when condition is False
   :type B: str, numeric_scalars, pdarray, Strings, or Categorical

   :returns: Values chosen from A where the condition is True and B where
             the condition is False
   :rtype: pdarray

   :raises TypeError: Raised if the condition object is not a pdarray, if A or B is not
       an int, np.int64, float, np.float64, bool, pdarray, str, Strings, Categorical
       if pdarray dtypes are not supported or do not match, or multiple
       condition clauses (see Notes section) are applied
   :raises ValueError: Raised if the shapes of the condition, A, and B pdarrays are unequal

   .. rubric:: Examples

   >>> a1 = ak.arange(1,10)
   >>> a2 = ak.ones(9, dtype=np.int64)
   >>> cond = a1 < 5
   >>> ak.where(cond,a1,a2)
   array([1 2 3 4 1 1 1 1 1])

   >>> a1 = ak.arange(1,10)
   >>> a2 = ak.ones(9, dtype=np.int64)
   >>> cond = a1 == 5
   >>> ak.where(cond,a1,a2)
   array([1 1 1 1 5 1 1 1 1])

   >>> a1 = ak.arange(1,10)
   >>> a2 = 10
   >>> cond = a1 < 5
   >>> ak.where(cond,a1,a2)
   array([1 2 3 4 10 10 10 10 10])

   >>> s1 = ak.array([f'str {i}' for i in range(10)])
   >>> s2 = 'str 21'
   >>> cond = (ak.arange(10) % 2 == 0)
   >>> ak.where(cond,s1,s2)
   array(['str 0', 'str 21', 'str 2', 'str 21', 'str 4',
   'str 21', 'str 6', 'str 21', 'str 8', 'str 21'])

   >>> c1 = ak.Categorical(ak.array([f'str {i}' for i in range(10)]))
   >>> c2 = ak.Categorical(ak.array([f'str {i}' for i in range(9, -1, -1)]))
   >>> cond = (ak.arange(10) % 2 == 0)
   >>> ak.where(cond,c1,c2)
   array(['str 0', 'str 8', 'str 2', 'str 6', 'str 4',
   'str 4', 'str 6', 'str 2', 'str 8', 'str 0'])

   .. rubric:: Notes

   A and B must have the same dtype and only one conditional clause
   is supported e.g., n < 5, n > 1, which is supported in numpy
   is not currently supported in Arkouda


.. py:function:: write_log(log_msg: str, tag: str = 'ClientGeneratedLog', log_lvl: LogLevel = LogLevel.INFO)

   Allows the user to write custom logs.

   :param log_msg: The message to be added to the server log
   :type log_msg: str
   :param tag: The tag to use in the log. This takes the place of the server function name.
               Allows for easy identification of custom logs.
               Defaults to "ClientGeneratedLog"
   :type tag: str
   :param log_lvl: The type of log to be written
                   Defaults to LogLevel.INFO
   :type log_lvl: LogLevel

   .. seealso:: :obj:`LogLevel`


.. py:function:: xlogy(x: Union[arkouda.pdarrayclass.pdarray, numpy.float64], y: arkouda.pdarrayclass.pdarray)

   Computes x * log(y).

   :param x: x must have a datatype that is castable to float64
   :type x: pdarray or np.float64
   :param y:
   :type y: pdarray

   :rtype: arkouda.pdarrayclass.pdarray

   .. rubric:: Examples

   >>> import arkouda as ak
   >>> ak.connect()
   >>> from arkouda.scipy.special import xlogy
   >>> xlogy( ak.array([1, 2, 3, 4]),  ak.array([5,6,7,8]))
   array([1.6094379124341003 3.5835189384561099 5.8377304471659395 8.317766166719343])
   >>> xlogy( 5.0, ak.array([1, 2, 3, 4]))
   array([0.00000000000000000 3.4657359027997265 5.4930614433405491 6.9314718055994531])


.. py:function:: zero_up(vals)

   Map an array of sparse values to 0-up indices.

   :param vals: Array to map to dense index
   :type vals: pdarray

   :returns: **aligned** -- Array with values replaced by 0-up indices
   :rtype: pdarray


.. py:function:: zeros(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with zeros.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: Type of resulting array, default ak.float64
   :type dtype: all_scalars
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
                    Included for consistency, as zeros are represented as all zeros, regardless
                    of the value of max_bits
   :type max_bits: int

   :returns: Zeros of the requested size or shape and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`ones`, :obj:`zeros_like`

   .. rubric:: Examples

   >>> ak.zeros(5, dtype=ak.int64)
   array([0 0 0 0 0])

   >>> ak.zeros(5, dtype=ak.float64)
   array([0.00000000000000000 0.00000000000000000 0.00000000000000000
          0.00000000000000000 0.00000000000000000])

   >>> ak.zeros(5, dtype=ak.bool_)
   array([False False False False False])

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.


.. py:function:: zeros(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with zeros.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: Type of resulting array, default ak.float64
   :type dtype: all_scalars
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
                    Included for consistency, as zeros are represented as all zeros, regardless
                    of the value of max_bits
   :type max_bits: int

   :returns: Zeros of the requested size or shape and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`ones`, :obj:`zeros_like`

   .. rubric:: Examples

   >>> ak.zeros(5, dtype=ak.int64)
   array([0 0 0 0 0])

   >>> ak.zeros(5, dtype=ak.float64)
   array([0.00000000000000000 0.00000000000000000 0.00000000000000000
          0.00000000000000000 0.00000000000000000])

   >>> ak.zeros(5, dtype=ak.bool_)
   array([False False False False False])

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.


.. py:function:: zeros(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with zeros.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: Type of resulting array, default ak.float64
   :type dtype: all_scalars
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
                    Included for consistency, as zeros are represented as all zeros, regardless
                    of the value of max_bits
   :type max_bits: int

   :returns: Zeros of the requested size or shape and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`ones`, :obj:`zeros_like`

   .. rubric:: Examples

   >>> ak.zeros(5, dtype=ak.int64)
   array([0 0 0 0 0])

   >>> ak.zeros(5, dtype=ak.float64)
   array([0.00000000000000000 0.00000000000000000 0.00000000000000000
          0.00000000000000000 0.00000000000000000])

   >>> ak.zeros(5, dtype=ak.bool_)
   array([False False False False False])

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.


.. py:function:: zeros(size: Union[arkouda.numpy.dtypes.int_scalars, Tuple[arkouda.numpy.dtypes.int_scalars, Ellipsis], str], dtype: Union[numpy.dtype, type, str, arkouda.numpy.dtypes.bigint] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with zeros.

   :param size: Size or shape of the array
   :type size: int_scalars or tuple of int_scalars
   :param dtype: Type of resulting array, default ak.float64
   :type dtype: all_scalars
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
                    Included for consistency, as zeros are represented as all zeros, regardless
                    of the value of max_bits
   :type max_bits: int

   :returns: Zeros of the requested size or shape and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported
   :raises RuntimeError: Raised if the size parameter is neither an int nor a str that is parseable to an int.
   :raises ValueError: Raised if the given shape exceeds get_max_array_rank() or is empty

   .. seealso:: :obj:`ones`, :obj:`zeros_like`

   .. rubric:: Examples

   >>> ak.zeros(5, dtype=ak.int64)
   array([0 0 0 0 0])

   >>> ak.zeros(5, dtype=ak.float64)
   array([0.00000000000000000 0.00000000000000000 0.00000000000000000
          0.00000000000000000 0.00000000000000000])

   >>> ak.zeros(5, dtype=ak.bool_)
   array([False False False False False])

   .. rubric:: Notes

   Logic for generating the pdarray is delegated to the ak.full method.


.. py:function:: zeros_like(pda: arkouda.pdarrayclass.pdarray) -> arkouda.pdarrayclass.pdarray

   Create a zero-filled pdarray of the same size and dtype as an existing
   pdarray.

   :param pda: Array to use for size and dtype
   :type pda: pdarray

   :returns: Equivalent to ak.zeros(pda.size, pda.dtype)
   :rtype: pdarray

   :raises TypeError: Raised if the pda parameter is not a pdarray.

   .. seealso:: :obj:`zeros`, :obj:`ones_like`

   .. rubric:: Examples

   >>> ak.zeros_like(ak.ones(5,dtype=ak.int64))
   array([0 0 0 0 0])

   >>> ak.zeros_like(ak.ones(5,dtype=ak.float64))
   array([0.00000000000000000 0.00000000000000000 0.00000000000000000
          0.00000000000000000 0.00000000000000000])

   >>> ak.zeros_like(ak.ones(5,dtype=ak.bool_))
   array([False False False False False])


