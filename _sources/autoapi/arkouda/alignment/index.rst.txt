arkouda.alignment
=================

.. py:module:: arkouda.alignment


Attributes
----------

.. autoapisummary::

   arkouda.alignment.akfloat64
   arkouda.alignment.akint64
   arkouda.alignment.akuint64
   arkouda.alignment.bigint


Exceptions
----------

.. autoapisummary::

   arkouda.alignment.NonUniqueError


Classes
-------

.. autoapisummary::

   arkouda.alignment.Categorical
   arkouda.alignment.GroupBy
   arkouda.alignment.Strings
   arkouda.alignment.pdarray


Functions
---------

.. autoapisummary::

   arkouda.alignment.align
   arkouda.alignment.arange
   arkouda.alignment.argsort
   arkouda.alignment.broadcast
   arkouda.alignment.coargsort
   arkouda.alignment.concatenate
   arkouda.alignment.find
   arkouda.alignment.full
   arkouda.alignment.in1d
   arkouda.alignment.in1d_intervals
   arkouda.alignment.interval_lookup
   arkouda.alignment.is_cosorted
   arkouda.alignment.left_align
   arkouda.alignment.lookup
   arkouda.alignment.ones
   arkouda.alignment.right_align
   arkouda.alignment.search_intervals
   arkouda.alignment.unique
   arkouda.alignment.unsqueeze
   arkouda.alignment.where
   arkouda.alignment.zero_up
   arkouda.alignment.zeros


Module Contents
---------------

.. py:class:: Categorical(values, **kwargs)

   Represents an array of values belonging to named categories. Converting a
   Strings object to Categorical often saves memory and speeds up operations,
   especially if there are many repeated values, at the cost of some one-time
   work in initialization.

   :param values: String values to convert to categories
   :type values: Strings
   :param NAvalue: The value to use to represent missing/null data
   :type NAvalue: str scalar

   .. attribute:: categories

      The set of category labels (determined automatically)

      :type: Strings

   .. attribute:: codes

      The category indices of the values or -1 for N/A

      :type: pdarray, int64

   .. attribute:: permutation

      The permutation that groups the values in the same order as categories

      :type: pdarray, int64

   .. attribute:: segments

      When values are grouped, the starting offset of each group

      :type: pdarray, int64

   .. attribute:: size

      The number of items in the array

      :type: Union[int,np.int64]

   .. attribute:: nlevels

      The number of distinct categories

      :type: Union[int,np.int64]

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: Union[int,np.int64]

   .. attribute:: shape

      The sizes of each dimension of the array

      :type: tuple


   .. py:attribute:: BinOps


   .. py:attribute:: RegisterablePieces


   .. py:attribute:: RequiredPieces


   .. py:method:: argsort()


   .. py:method:: attach(user_defined_name: str) -> Categorical
      :staticmethod:


      DEPRECATED
      Function to return a Categorical object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which Categorical object was registered under
      :type user_defined_name: str

      :returns: The Categorical object created by re-attaching to the corresponding server components
      :rtype: Categorical

      :raises TypeError: if user_defined_name is not a string

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_categorical_by_name`



   .. py:method:: concatenate(others: Sequence[Categorical], ordered: bool = True) -> Categorical

      Merge this Categorical with other Categorical objects in the array,
      concatenating the arrays and synchronizing the categories.

      :param others: The Categorical arrays to concatenate and merge with this one
      :type others: Sequence[Categorical]
      :param ordered: If True (default), the arrays will be appended in the
                      order given. If False, array data may be interleaved
                      in blocks, which can greatly improve performance but
                      results in non-deterministic ordering of elements.
      :type ordered: bool

      :returns: The merged Categorical object
      :rtype: Categorical

      :raises TypeError: Raised if any others array objects are not Categorical objects

      .. rubric:: Notes

      This operation can be expensive -- slower than concatenating Strings.



   .. py:method:: contains(substr: Union[bytes, arkouda.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element contains the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that contain substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.startswith`, :obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:attribute:: dtype


   .. py:method:: endswith(substr: Union[bytes, arkouda.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element ends with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that end with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.startswith`, :obj:`Categorical.contains`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding method
      on Strings objects, because it searches the unique category labels
      instead of the full array.



   .. py:method:: from_codes(codes: arkouda.pdarrayclass.pdarray, categories: arkouda.strings.Strings, permutation=None, segments=None, **kwargs) -> Categorical
      :classmethod:


      Make a Categorical from codes and categories arrays. If codes and
      categories have already been pre-computed, this constructor saves
      time. If not, please use the normal constructor.

      :param codes: Category indices of each value
      :type codes: pdarray, int64
      :param categories: Unique category labels
      :type categories: Strings
      :param permutation: The permutation that groups the values in the same order
                          as categories
      :type permutation: pdarray, int64
      :param segments: When values are grouped, the starting offset of each group
      :type segments: pdarray, int64

      :returns: The Categorical object created from the input parameters
      :rtype: Categorical

      :raises TypeError: Raised if codes is not a pdarray of int64 objects or if
          categories is not a Strings object



   .. py:method:: from_return_msg(rep_msg) -> Categorical
      :classmethod:


      Create categorical from return message from server

      .. rubric:: Notes

      This is currently only used when reading a Categorical from HDF5 files.



   .. py:method:: group() -> arkouda.pdarrayclass.pdarray

      Return the permutation that groups the array, placing equivalent
      categories together. All instances of the same category are guaranteed
      to lie in one contiguous block of the permuted array, but the blocks
      are not necessarily ordered.

      :returns: The permutation that groups the array by value
      :rtype: pdarray

      .. seealso:: :obj:`GroupBy`, :obj:`unique`

      .. rubric:: Notes

      This method is faster than the corresponding Strings method. If the
      Categorical was created from a Strings object, then this function
      simply returns the cached permutation. Even if the Categorical was
      created using from_codes(), this function will be faster than
      Strings.group() because it sorts dense integer values, rather than
      128-bit hash values.



   .. py:method:: hash() -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

      Compute a 128-bit hash of each element of the Categorical.

      :returns: A tuple of two int64 pdarrays. The ith hash value is the concatenation
                of the ith values from each array.
      :rtype: Tuple[pdarray,pdarray]

      .. rubric:: Notes

      The implementation uses SipHash128, a fast and balanced hash function (used
      by Python for dictionaries and sets). For realistic numbers of strings (up
      to about 10**15), the probability of a collision between two 128-bit hash
      values is negligible.



   .. py:method:: in1d(test: Union[arkouda.strings.Strings, Categorical]) -> arkouda.pdarrayclass.pdarray

      Test whether each element of the Categorical object is
      also present in the test Strings or Categorical object.

      Returns a boolean array the same length as `self` that is True
      where an element of `self` is in `test` and False otherwise.

      :param test: The values against which to test each value of 'self`.
      :type test: Union[Strings,Categorical]

      :returns: The values `self[in1d]` are in the `test` Strings or Categorical object.
      :rtype: pdarray, bool

      :raises TypeError: Raised if test is not a Strings or Categorical object

      .. seealso:: :obj:`unique`, :obj:`intersect1d`, :obj:`union1d`

      .. rubric:: Notes

      `in1d` can be considered as an element-wise function version of the
      python keyword `in`, for 1-D sequences. ``in1d(a, b)`` is logically
      equivalent to ``ak.array([item in b for item in a])``, but is much
      faster and scales to arbitrarily large ``a``.

      .. rubric:: Examples

      >>> strings = ak.array([f'String {i}' for i in range(0,5)])
      >>> cat = ak.Categorical(strings)
      >>> ak.in1d(cat,strings)
      array([True, True, True, True, True])
      >>> strings = ak.array([f'String {i}' for i in range(5,9)])
      >>> catTwo = ak.Categorical(strings)
      >>> ak.in1d(cat,catTwo)
      array([False, False, False, False, False])



   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

       Return True iff the object is contained in the registry or is a component of a
       registered object.

      :returns: Indicates if the object is contained in the registry
      :rtype: numpy.bool

      :raises RegistrationError: Raised if there's a server-side error or a mis-match of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_categorical_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: isna()

      Find where values are missing or null (as defined by self.NAvalue)



   .. py:property:: nbytes
      The size of the Categorical in bytes.

      :returns: The size of the Categorical in bytes.
      :rtype: int


   .. py:attribute:: objType
      :value: 'Categorical'



   .. py:method:: parse_hdf_categoricals(d: Mapping[str, Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]]) -> Tuple[List[str], Dict[str, Categorical]]
      :staticmethod:


      This function should be used in conjunction with the load_all function which reads hdf5 files
      and reconstitutes Categorical objects.
      Categorical objects use a naming convention and HDF5 structure so they can be identified and
      constructed for the user.

      In general you should not call this method directly

      :param d:
      :type d: Dictionary of String to either Pdarray or Strings object

      :returns: * *2-Tuple of List of strings containing key names which should be removed and Dictionary of*
                * *base name to Categorical object*

      .. seealso:: :obj:`Categorical.save`, :obj:`load_all`



   .. py:attribute:: permutation
      :value: None



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: register(user_defined_name: str) -> Categorical

      Register this Categorical object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the Categorical is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same Categorical which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support
                a fluid programming style.
                Please note you cannot register two different Categoricals with the same name.
      :rtype: Categorical

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Categorical with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_categorical_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: reset_categories() -> Categorical

      Recompute the category labels, discarding any unused labels. This
      method is often useful after slicing or indexing a Categorical array,
      when the resulting array only contains a subset of the original
      categories. In this case, eliminating unused categories can speed up
      other operations.

      :returns: A Categorical object generated from the current instance
      :rtype: Categorical



   .. py:method:: save(prefix_path: str, dataset: str = 'categorical_array', file_format: str = 'HDF5', mode: str = 'truncate', file_type: str = 'distribute', compression: Optional[str] = None) -> str

      DEPRECATED
      Save the Categorical object to HDF5 or Parquet. The result is a collection of HDF5/Parquet files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path and dataset. Each locale saves its chunk of the Strings array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in HDF5 files (must not already exist)
      :type dataset: str
      :param file_format: The format to save the file to.
      :type file_format: str {'HDF5 | 'Parquet'}
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Categorical dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")
      :param compression: {None | 'snappy' | 'gzip' | 'brotli' | 'zstd' | 'lz4'}
                          The compression type to use when writing.
                          This is only supported for Parquet files and will not be used with HDF5.
      :type compression: str (Optional)

      :rtype: String message indicating result of save operation

      :raises ValueError: Raised if the lengths of columns and values differ, or the mode is
          neither 'truncate' nor 'append'
      :raises TypeError: Raised if prefix_path, dataset, or mode is not a str

      .. rubric:: Notes

      Important implementation notes: (1) Strings state is saved as two datasets
      within an hdf5 group: one for the string characters and one for the
      segments corresponding to the start of each string, (2) the hdf5 group is named
      via the dataset parameter.

      .. seealso:: :obj:`-`, :obj:`-`



   .. py:attribute:: segments
      :value: None



   .. py:method:: set_categories(new_categories, NAvalue=None)

      Set categories to user-defined values.

      :param new_categories: The array of new categories to use. Must be unique.
      :type new_categories: Strings
      :param NAvalue: The value to use to represent missing/null data
      :type NAvalue: str scalar

      :returns: A new Categorical with the user-defined categories. Old values present
                in new categories will appear unchanged. Old values not present will
                be assigned the NA value.
      :rtype: Categorical



   .. py:method:: sort_values()


   .. py:method:: standardize_categories(arrays, NAvalue='N/A')
      :classmethod:


      Standardize an array of Categoricals so that they share the same categories.

      :param arrays: The Categoricals to standardize
      :type arrays: sequence of Categoricals
      :param NAvalue: The value to use to represent missing/null data
      :type NAvalue: str scalar

      :returns: A list of the original Categoricals remapped to the shared categories.
      :rtype: List of Categoricals



   .. py:method:: startswith(substr: Union[bytes, arkouda.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element starts with the given substring.

      :param substr: The substring to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that start with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Categorical.contains`, :obj:`Categorical.endswith`

      .. rubric:: Notes

      This method can be significantly faster than the corresponding
      method on Strings objects, because it searches the unique category
      labels instead of the full array.



   .. py:method:: to_hdf(prefix_path, dataset='categorical_array', mode='truncate', file_type='distribute')

      Save the Categorical to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
      :type file_type: str ("single" | "distribute")

      :rtype: None

      .. seealso:: :obj:`load`



   .. py:method:: to_list() -> List

      Convert the Categorical to a list, transferring data from
      the arkouda server to Python. This conversion discards category
      information and produces a list of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A list of strings corresponding to the values in
                this Categorical
      :rtype: list

      .. rubric:: Notes

      The number of bytes in the Categorical cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from
      the arkouda server to Python. This conversion discards category
      information and produces an ndarray of strings. If the arrays
      exceeds a built-in size limit, a RuntimeError is raised.

      :returns: A numpy ndarray of strings corresponding to the values in
                this array
      :rtype: np.ndarray

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'categorical_array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      This functionality is currently not supported and will also raise a RuntimeError.
      Support is in development.
      Save the Categorical to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in HDF5 files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Categorical dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: Default None
                          Provide the compression type to use when writing the file.
                          Supported values: snappy, gzip, brotli, zstd, lz4
      :type compression: str (Optional)

      :rtype: String message indicating result of save operation

      :raises RuntimeError: On run due to compatability issues of Categorical with Parquet.

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. seealso:: :obj:`to_hdf`



   .. py:method:: to_strings() -> List

      Convert the Categorical to Strings.

      :returns: A Strings object corresponding to the values in
                this Categorical.
      :rtype: arkouda.strings.Strings

      .. rubric:: Examples

      >>> import arkouda as ak
      >>> ak.connect()
      >>> a = ak.array(["a","b","c"])
      >>> a
      >>> c = ak.Categorical(a)
      >>>  c.to_strings()
      c.to_strings()

      >>> isinstance(c.to_strings(), ak.Strings)
      True



   .. py:method:: transfer(hostname: str, port: arkouda.dtypes.int_scalars)

      Sends a Categorical object to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the Categorical is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unique() -> Categorical


   .. py:method:: unregister() -> None

      Unregister this Categorical object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_categorical_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_categorical_by_name(user_defined_name: str) -> None
      :staticmethod:


      Function to unregister Categorical object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the Categorical object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path, dataset='categorical_array', repack=True)

      Overwrite the dataset with the name provided with this Categorical object. If
      the dataset does not exist it is added.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: None

      :raises RuntimeError: Raised if a server-side error is thrown saving the Categorical

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added
      - Because HDF5 deletes do not release memory, the repack option allows for
        automatic creation of a file without the inaccessible data.



.. py:class:: GroupBy

   Group an array or list of arrays by value, usually in preparation
   for aggregating the within-group values of another array.

   :param keys: The array to group by value, or if list, the column arrays to group by row
   :type keys: (list of) pdarray, Strings, or Categorical
   :param assume_sorted: If True, assume keys is already sorted (Default: False)
   :type assume_sorted: bool

   .. attribute:: nkeys

      The number of key arrays (columns)

      :type: int

   .. attribute:: size

      The length of the input array(s), i.e. number of rows

      :type: int

   .. attribute:: permutation

      The permutation that sorts the keys array(s) by value (row)

      :type: pdarray

   .. attribute:: unique_keys

      The unique values of the keys array(s), in grouped order

      :type: (list of) pdarray, Strings, or Categorical

   .. attribute:: ngroups

      The length of the unique_keys array(s), i.e. number of groups

      :type: int

   .. attribute:: segments

      The start index of each group in the grouped array(s)

      :type: pdarray

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. attribute:: dropna

      If True, and the groupby keys contain NaN values,
      the NaN values together with the corresponding row will be dropped.
      Otherwise, the rows corresponding to NaN values will be kept.

      :type: bool (default=True)

   :raises TypeError: Raised if keys is a pdarray with a dtype other than int64

   .. rubric:: Notes

   Integral pdarrays, Strings, and Categoricals are natively supported, but
   float64 and bool arrays are not.

   For a user-defined class to be groupable, it must inherit from pdarray
   and define or overload the grouping API:
     1) a ._get_grouping_keys() method that returns a list of pdarrays
        that can be (co)argsorted.
     2) (Optional) a .group() method that returns the permutation that
        groups the array
   If the input is a single array with a .group() method defined, method 2
   will be used; otherwise, method 1 will be used.


   .. py:method:: AND(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise AND of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise AND reduction on
      each group.

      :param values: The values to group and reduce with AND
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise AND of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: OR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise OR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise OR reduction on
      each group.

      :param values: The values to group and reduce with OR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise OR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: Reductions(*args, **kwargs)

      frozenset() -> empty frozenset object
      frozenset(iterable) -> frozenset object

      Build an immutable unordered collection of unique elements.




   .. py:method:: XOR(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Bitwise XOR of values in each segment.

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform a bitwise XOR reduction on
      each group.

      :param values: The values to group and reduce with XOR
      :type values: pdarray, int64

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **result** (*pdarray, int64*) -- Bitwise XOR of values in segments corresponding to keys

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not int64
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: aggregate(values: groupable, operator: str, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, groupable]

      Using the permutation stored in the GroupBy instance, group another
      array of values and apply a reduction to each group's values.

      :param values: The values to group and reduce
      :type values: pdarray
      :param operator: The name of the reduction operator to use
      :type operator: str
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **aggregates** (*groupable*) -- One aggregate value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if the requested operator is not supported for the
          values dtype

      .. rubric:: Examples

      >>> keys = ak.arange(0, 10)
      >>> vals = ak.linspace(-1, 1, 10)
      >>> g = ak.GroupBy(keys)
      >>> g.aggregate(vals, 'sum')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777768,
      -0.55555555555555536, -0.33333333333333348, -0.11111111111111116,
      0.11111111111111116, 0.33333333333333348, 0.55555555555555536, 0.77777777777777768,
      1]))
      >>> g.aggregate(vals, 'min')
      (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([-1, -0.77777777777777779,
      -0.55555555555555558, -0.33333333333333337, -0.11111111111111116, 0.11111111111111116,
      0.33333333333333326, 0.55555555555555536, 0.77777777777777768, 1]))



   .. py:method:: all(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and perform an "and" reduction on
      each group.

      :param values: The values to group and reduce with "and"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if all is not supported for the values dtype



   .. py:method:: any(values: pdarray) -> Tuple[Union[pdarray, List[Union[pdarray, Strings]]], pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and perform an "or" reduction on each group.

      :param values: The values to group and reduce with "or"
      :type values: pdarray, bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_any** (*pdarray, bool*) -- One bool per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray or if the pdarray
          dtype is not bool
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array



   .. py:method:: argmax(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      maximum of each group's values.

      :param values: The values to group and find argmax
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argmaxima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The returned indices refer to the original values array as passed in,
      not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmax(b)
      (array([2, 3, 4]), array([9, 3, 2]))



   .. py:method:: argmin(values: pdarray) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the location of the first
      minimum of each group's values.

      :param values: The values to group and find argmin
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_argminima** (*pdarray, int64*) -- One index per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if argmax
          is not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values
          size or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if argmin is not supported for the values dtype

      .. rubric:: Notes

      The returned indices refer to the original values array as
      passed in, not the permutation applied by the GroupBy instance.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.argmin(b)
      (array([2, 3, 4]), array([5, 4, 2]))



   .. py:method:: attach(user_defined_name: str) -> GroupBy

      Function to return a GroupBy object attached to the registered name in the
      arkouda server which was registered using register()

      :param user_defined_name: user defined name which GroupBy object was registered under
      :type user_defined_name: str

      :returns: The GroupBy object created by re-attaching to the corresponding server components
      :rtype: GroupBy

      :raises RegistrationError: if user_defined_name is not registered

      .. seealso:: :obj:`register`, :obj:`is_registered`, :obj:`unregister`, :obj:`unregister_groupby_by_name`



   .. py:method:: broadcast(values: Union[pdarray, Strings], permute: bool = True) -> Union[pdarray, Strings]

      Fill each group's segment with a constant value.

      :param values: The values to put in each group's segment
      :type values: pdarray, Strings
      :param permute: If True (default), permute broadcast values back to the ordering
                      of the original array on which GroupBy was called. If False, the
                      broadcast values are grouped by value.
      :type permute: bool

      :returns: The broadcasted values
      :rtype: pdarray, Strings

      :raises TypeError: Raised if value is not a pdarray object
      :raises ValueError: Raised if the values array does not have one
          value per segment

      .. rubric:: Notes

      This function is a sparse analog of ``np.broadcast``. If a
      GroupBy object represents a sparse matrix (tensor), then
      this function takes a (dense) column vector and replicates
      each value to the non-zero elements in the corresponding row.

      .. rubric:: Examples

      >>> a = ak.array([0, 1, 0, 1, 0])
      >>> values = ak.array([3, 5])
      >>> g = ak.GroupBy(a)
      # By default, result is in original order
      >>> g.broadcast(values)
      array([3, 5, 3, 5, 3])
      # With permute=False, result is in grouped order
      >>> g.broadcast(values, permute=False)
      array([3, 3, 3, 5, 5]
      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 1, 4, 4, 4, 1, 3, 3, 2, 2])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> g.broadcast(counts > 2)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts == 3)
      array([True False True True True False True True False False])
      >>> g.broadcast(counts < 4)
      array([True True True True True True True True True True])



   .. py:method:: build_from_components(user_defined_name: Optional[str] = None, **kwargs) -> GroupBy

      function to build a new GroupBy object from component keys and permutation.

      :param user_defined_name: and assign it the given name
      :type user_defined_name: str (Optional) Passing a name will init the new GroupBy
      :param kwargs: Expected keys are "orig_keys", "permutation", "unique_keys", and "segments"
      :type kwargs: dict Dictionary of components required for rebuilding the GroupBy.

      :returns: The GroupBy object created by using the given components
      :rtype: GroupBy



   .. py:method:: count(values: pdarray) -> Tuple[groupable, pdarray]

      Count the number of elements in each group.  NaN values will be excluded from the total.

      :param values: The values to be count by group (excluding NaN values).
      :type values: pdarray

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears (excluding NaN values).

      .. rubric:: Examples

      >>> a = ak.array([1, 0, -1, 1, 0, -1])
      >>> a
      array([1 0 -1 1 0 -1])
      >>> b = ak.array([1, np.nan, -1, np.nan, np.nan, -1], dtype = "float64")
      >>> b
      array([1.00000000000000000 nan -1.00000000000000000 nan nan -1.00000000000000000])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.count(b)
      >>> keys
      array([-1 0 1])
      >>> counts
      array([2 0 1])



   .. py:method:: first(values: groupable_element_type) -> Tuple[groupable, groupable_element_type]

      First value in each group.

      :param values: The values from which to take the first of each group
      :type values: pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*pdarray-like*) -- The first value of each group



   .. py:method:: from_return_msg(rep_msg)


   .. py:method:: is_registered() -> bool

      Return True if the object is contained in the registry

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RegistrationError: Raised if there's a server-side error or a mismatch of registered components

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister`, :obj:`unregister_groupby_by_name`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: max(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the maximum of each
      group's values.

      :param values: The values to group and find maxima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_maxima** (*pdarray*) -- One maximum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if max is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if max is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.max(b)
      (array([2, 3, 4]), array([4, 4, 3]))



   .. py:method:: mean(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the mean of each group's
      values.

      :param values: The values to group and average
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_means** (*pdarray, float64*) -- One mean value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.mean(b)
      (array([2, 3, 4]), array([2.6666666666666665, 2.7999999999999998, 3]))



   .. py:method:: median(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the median of each group's
      values.

      :param values: The values to group and find median
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_medians** (*pdarray, float64*) -- One median value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,9)
      >>> a
      array([4 1 4 3 2 2 2 3 3])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([4 1 4 3 2 2 2 3 3])
      >>> b = ak.linspace(-5,5,9)
      >>> b
      array([-5 -3.75 -2.5 -1.25 0 1.25 2.5 3.75 5])
      >>> g.median(b)
      (array([1 2 3 4]), array([-3.75 1.25 3.75 -3.75]))



   .. py:method:: min(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and return the minimum of each group's
      values.

      :param values: The values to group and find minima
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_minima** (*pdarray*) -- One minimum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object or if min is
          not supported for the values dtype
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if min is not supported for the values dtype

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.min(b)
      (array([2, 3, 4]), array([1, 1, 3]))



   .. py:method:: mode(values: groupable) -> Tuple[groupable, groupable]

      Most common value in each group. If a group is multi-modal, return the
      modal value that occurs first.

      :param values: The values from which to take the mode of each group
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) pdarray-like*) -- The most common value of each group



   .. py:method:: most_common(values)

      (Deprecated) See `GroupBy.mode()`.




   .. py:method:: nunique(values: groupable) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group another
      array of values and return the number of unique values in each group.

      :param values: The values to group and find unique values
      :type values: pdarray, int64

      :returns: * **unique_keys** (*groupable*) -- The unique keys, in grouped order
                * **group_nunique** (*groupable*) -- Number of unique values per unique key in the GroupBy instance

      :raises TypeError: Raised if the dtype(s) of values array(s) does/do not support
          the nunique method
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if nunique is not supported for the values dtype

      .. rubric:: Examples

      >>> data = ak.array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> data
      array([3, 4, 3, 1, 1, 4, 3, 4, 1, 4])
      >>> labels = ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> labels
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g = ak.GroupBy(labels)
      >>> g.keys
      ak.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4])
      >>> g.nunique(data)
      array([1,2,3,4]), array([2, 2, 3, 1])
      #    Group (1,1,1) has values [3,4,3] -> there are 2 unique values 3&4
      #    Group (2,2,2) has values [1,1,4] -> 2 unique values 1&4
      #    Group (3,3,3) has values [3,4,1] -> 3 unique values
      #    Group (4) has values [4] -> 1 unique value



   .. py:method:: objType(*args, **kwargs)

      str(object='') -> str
      str(bytes_or_buffer[, encoding[, errors]]) -> str

      Create a new string object from the given object. If encoding or
      errors is specified, then the object must expose a data buffer
      that will be decoded using the given encoding and error handler.
      Otherwise, returns the result of object.__str__() (if defined)
      or repr(object).
      encoding defaults to sys.getdefaultencoding().
      errors defaults to 'strict'.




   .. py:method:: prod(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the product of each group's
      values.

      :param values: The values to group and multiply
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_products** (*pdarray, float64*) -- One product per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array
      :raises RuntimeError: Raised if prod is not supported for the values dtype

      .. rubric:: Notes

      The return dtype is always float64.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.prod(b)
      (array([2, 3, 4]), array([12, 108.00000000000003, 8.9999999999999982]))



   .. py:method:: register(user_defined_name: str) -> GroupBy

      Register this GroupBy object and underlying components with the Arkouda server

      :param user_defined_name: user defined name the GroupBy is to be registered under,
                                this will be the root name for underlying components
      :type user_defined_name: str

      :returns: The same GroupBy which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different GroupBys with the same name.
      :rtype: GroupBy

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the GroupBy with the user_defined_name

      .. seealso:: :obj:`unregister`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: sample(values: groupable, n=None, frac=None, replace=False, weights=None, random_state=None, return_indices=False, permute_samples=False)

      Return a random sample from each group. You can either specify the number of elements
      or the fraction of elements to be sampled. random_state can be used for reproducibility

      :param values: The values from which to sample, according to their group membership.
      :type values: (list of) pdarray-like
      :param n: Number of items to return for each group.
                Cannot be used with frac and must be no larger than
                the smallest group unless replace is True.
                Default is one if frac is None.
      :type n: int, optional
      :param frac: Fraction of items to return. Cannot be used with n.
      :type frac: float, optional
      :param replace: Allow or disallow sampling of the value more than once.
      :type replace: bool, default False
      :param weights: Default None results in equal probability weighting.
                      If passed a pdarray, then values must have the same length as the groupby keys
                      and will be used as sampling probabilities after normalization within each group.
                      Weights must be non-negative with at least one positive element within each group.
      :type weights: pdarray, optional
      :param random_state: If int, seed for random number generator.
                           If ak.random.Generator, use as given.
      :type random_state: int or ak.random.Generator, optional
      :param return_indices: if True, return the indices of the sampled values.
                             Otherwise, return the sample values.
      :type return_indices: bool, default False
      :param permute_samples: if True, return permute the samples according to group
                              Otherwise, keep samples in original order.
      :type permute_samples: bool, default False

      :returns: if return_indices is True, return the indices of the sampled values.
                Otherwise, return the sample values.
      :rtype: pdarray



   .. py:method:: size() -> Tuple[groupable, pdarray]

      Count the number of elements in each group, i.e. the number of times
      each key appears.  This counts the total number of rows (including NaN values).

      :param none:

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **counts** (*pdarray, int64*) -- The number of times each unique key appears

      .. seealso:: :obj:`count`

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 2, 3, 1, 2, 4, 3, 4, 3, 4])
      >>> g = ak.GroupBy(a)
      >>> keys,counts = g.size()
      >>> keys
      array([1, 2, 3, 4])
      >>> counts
      array([1, 2, 4, 3])



   .. py:method:: std(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the standard deviation of
      each group's values.

      :param values: The values to group and find standard deviation
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_stds** (*pdarray, float64*) -- One std value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The standard deviation is the square root of the average of the squared
      deviations from the mean, i.e., ``std = sqrt(mean((x - x.mean())**2))``.

      The average squared deviation is normally calculated as
      ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified,
      the divisor ``N - ddof`` is used instead. In standard statistical
      practice, ``ddof=1`` provides an unbiased estimator of the variance
      of the infinite population. ``ddof=0`` provides a maximum likelihood
      estimate of the variance for normally distributed variables. The
      standard deviation computed in this function is the square root of
      the estimated variance, so even with ``ddof=1``, it will not be an
      unbiased estimate of the standard deviation per se.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.std(b)
      (array([2 3 4]), array([1.5275252316519465 1.0954451150103321 0]))



   .. py:method:: sum(values: pdarray, skipna: bool = True) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and sum each group's values.

      :param values: The values to group and sum
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_sums** (*pdarray*) -- One sum per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size or
          if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The grouped sum of a boolean ``pdarray`` returns integers.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.sum(b)
      (array([2, 3, 4]), array([8, 14, 6]))



   .. py:method:: to_hdf(prefix_path, dataset='groupby', mode='truncate', file_type='distribute')

      Save the GroupBy to HDF5. The result is a collection of HDF5 files, one file
      per locale of the arkouda server, where each filename starts with prefix_path.

      :param prefix_path: Directory and filename prefix that all output files will share
      :type prefix_path: str
      :param dataset: Name prefix for saved data within the HDF5 file
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', add data as a new column to existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :returns: * *None*
                * *GroupBy is not currently supported by Parquet*



   .. py:method:: unique(values: groupable)

      Return the set of unique values in each group, as a SegArray.

      :param values: The values to unique
      :type values: (list of) pdarray-like

      :returns: * **unique_keys** (*(list of) pdarray-like*) -- The unique keys, in grouped order
                * **result** (*(list of) SegArray*) -- The unique values of each group

      :raises TypeError: Raised if values is or contains Strings or Categorical



   .. py:method:: unregister()

      Unregister this GroupBy object in the arkouda server which was previously
      registered using register() and/or attached to using attach()

      :raises RegistrationError: If the object is already unregistered or if there is a server error
          when attempting to unregister

      .. seealso:: :obj:`register`, :obj:`attach`, :obj:`unregister_groupby_by_name`, :obj:`is_registered`

      .. rubric:: Notes

      Objects registered with the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_groupby_by_name(user_defined_name: str) -> None

      Function to unregister GroupBy object by name which was registered
      with the arkouda server via register()

      :param user_defined_name: Name under which the GroupBy object was registered
      :type user_defined_name: str

      :raises TypeError: if user_defined_name is not a string
      :raises RegistrationError: if there is an issue attempting to unregister any underlying components

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'groupby', repack: bool = True)


   .. py:method:: var(values: pdarray, skipna: bool = True, ddof: int_scalars = 1) -> Tuple[groupable, pdarray]

      Using the permutation stored in the GroupBy instance, group
      another array of values and compute the variance of
      each group's values.

      :param values: The values to group and find variance
      :type values: pdarray
      :param skipna: boolean which determines if NANs should be skipped
      :type skipna: bool
      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: * **unique_keys** (*(list of) pdarray or Strings*) -- The unique keys, in grouped order
                * **group_vars** (*pdarray, float64*) -- One var value per unique key in the GroupBy instance

      :raises TypeError: Raised if the values array is not a pdarray object
      :raises ValueError: Raised if the key array size does not match the values size
          or if the operator is not in the GroupBy.Reductions array

      .. rubric:: Notes

      The return dtype is always float64.

      The variance is the average of the squared deviations from the mean,
      i.e.,  ``var = mean((x - x.mean())**2)``.

      The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.
      If, however, `ddof` is specified, the divisor ``N - ddof`` is used
      instead.  In standard statistical practice, ``ddof=1`` provides an
      unbiased estimator of the variance of a hypothetical infinite population.
      ``ddof=0`` provides a maximum likelihood estimate of the variance for
      normally distributed variables.

      .. rubric:: Examples

      >>> a = ak.randint(1,5,10)
      >>> a
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> g = ak.GroupBy(a)
      >>> g.keys
      array([3, 3, 4, 3, 3, 2, 3, 2, 4, 2])
      >>> b = ak.randint(1,5,10)
      >>> b
      array([3, 3, 3, 4, 1, 1, 3, 3, 3, 4])
      >>> g.var(b)
      (array([2 3 4]), array([2.333333333333333 1.2 0]))



.. py:exception:: NonUniqueError

   Bases: :py:obj:`ValueError`


   Inappropriate argument value (of correct type).


.. py:class:: Strings(strings_pdarray: arkouda.pdarrayclass.pdarray, bytes_size: arkouda.dtypes.int_scalars)

   Represents an array of strings whose data resides on the
   arkouda server. The user should not call this class directly;
   rather its instances are created by other arkouda functions.

   .. attribute:: entry

      Encapsulation of a Segmented Strings array contained on
      the arkouda server.  This is a composite of
       - offsets array: starting indices for each string
       - bytes array: raw bytes of all strings joined by nulls

      :type: pdarray

   .. attribute:: size

      The number of strings in the array

      :type: int_scalars

   .. attribute:: nbytes

      The total number of bytes in all strings

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      The sizes of each dimension of the array

      :type: tuple

   .. attribute:: dtype

      The dtype is ak.str

      :type: dtype

   .. attribute:: logger

      Used for all logging operations

      :type: ArkoudaLogger

   .. rubric:: Notes

   Strings is composed of two pdarrays: (1) offsets, which contains the
   starting indices for each string and (2) bytes, which contains the
   raw bytes of all strings, delimited by nulls.


   .. py:attribute:: BinOps


   .. py:method:: astype(dtype) -> arkouda.pdarrayclass.pdarray

      Cast values of Strings object to provided dtype

      :param dtype: Dtype to cast to
      :type dtype: np.dtype or str

      :returns: An arkouda pdarray with values converted to the specified data type
      :rtype: ak.pdarray

      .. rubric:: Notes

      This is essentially shorthand for ak.cast(x, '<dtype>') where x is a pdarray.



   .. py:method:: attach(user_defined_name: str) -> Strings
      :staticmethod:


      class method to return a Strings object attached to the registered name in the arkouda
      server which was registered using register()

      :param user_defined_name: user defined name which the Strings object was registered under
      :type user_defined_name: str

      :returns: the Strings object registered with user_defined_name in the arkouda server
      :rtype: Strings object

      :raises TypeError: Raised if user_defined_name is not a str

      .. seealso:: :obj:`register`, :obj:`unregister`

      .. rubric:: Notes

      Registered names/Strings objects in the server are immune to deletion
      until they are unregistered.



   .. py:method:: cached_regex_patterns() -> List

      Returns the regex patterns for which Match objects have been cached



   .. py:method:: capitalize() -> Strings

      Returns a new Strings from the original replaced with the first letter capitilzed
      and the remaining letters lowercase.

      :returns: Strings from the original replaced with the capitalized equivalent.
      :rtype: Strings

      :raises RuntimeError: Raised if there is a server-side error thrown.

      .. seealso:: :obj:`Strings.lower`, :obj:`String.upper`, :obj:`String.title`

      .. rubric:: Examples

      >>> strings = ak.array([f'StrINgS aRe Here {i}' for i in range(5)])
      >>> strings
      array(['StrINgS aRe Here 0', 'StrINgS aRe Here 1', 'StrINgS aRe Here 2', 'StrINgS aRe Here 3',
      ... 'StrINgS aRe Here 4'])
      >>> strings.title()
      array(['Strings are here 0', 'Strings are here 1', 'Strings are here 2', 'Strings are here 3',
      ... 'Strings are here 4'])



   .. py:method:: contains(substr: Union[bytes, arkouda.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element contains the given substring.

      :param substr: The substring in the form of string or byte array to search for
      :type substr: str_scalars
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that contain substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.startswith`, :obj:`Strings.endswith`

      .. rubric:: Examples

      >>> strings = ak.array([f'{i} string {i}' for i in range(1, 6)])
      >>> strings
      array(['1 string 1', '2 string 2', '3 string 3', '4 string 4', '5 string 5'])
      >>> strings.contains('string')
      array([True, True, True, True, True])
      >>> strings.contains('string \d', regex=True)
      array([True, True, True, True, True])



   .. py:method:: decode(fromEncoding, toEncoding='UTF-8')

      Return a new strings object in `fromEncoding`, expecting that the
      current Strings is encoded in `toEncoding`

      :param fromEncoding: The current encoding of the strings object
      :type fromEncoding: str
      :param toEncoding: The encoding that the strings will be converted to,
                         default to UTF-8
      :type toEncoding: str

      :returns: A new Strings object in `toEncoding`
      :rtype: Strings

      :raises RuntimeError: Raised if there is a server-side error thrown



   .. py:method:: encode(toEncoding: str, fromEncoding: str = 'UTF-8')

      Return a new strings object in `toEncoding`, expecting that the
      current Strings is encoded in `fromEncoding`

      :param toEncoding: The encoding that the strings will be converted to
      :type toEncoding: str
      :param fromEncoding: The current encoding of the strings object, default to
                           UTF-8
      :type fromEncoding: str

      :returns: A new Strings object in `toEncoding`
      :rtype: Strings

      :raises RuntimeError: Raised if there is a server-side error thrown



   .. py:method:: endswith(substr: Union[bytes, arkouda.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element ends with the given substring.

      :param substr: The suffix to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that end with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not bytes or str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.contains`, :obj:`Strings.startswith`

      .. rubric:: Examples

      >>> strings_start = ak.array([f'{i} string' for i in range(1,6)])
      >>> strings_start
      array(['1 string', '2 string', '3 string', '4 string', '5 string'])
      >>> strings_start.endswith('ing')
      array([True, True, True, True, True])
      >>> strings_end = ak.array([f'string {i}' for i in range(1, 6)])
      >>> strings_end
      array(['string 1', 'string 2', 'string 3', 'string 4', 'string 5'])
      >>> strings_end.endswith('ing \d', regex = True)
      array([True, True, True, True, True])



   .. py:method:: find_locations(pattern: Union[bytes, arkouda.dtypes.str_scalars]) -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

      Finds pattern matches and returns pdarrays containing the number, start postitions,
      and lengths of matches

      :param pattern: The regex pattern used to find matches
      :type pattern: str_scalars

      :returns: * *pdarray, int64* -- For each original string, the number of pattern matches
                * *pdarray, int64* -- The start positons of pattern matches
                * *pdarray, int64* -- The lengths of pattern matches

      :raises TypeError: Raised if the pattern parameter is not bytes or str_scalars
      :raises ValueError: Raised if pattern is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.findall`, :obj:`Strings.match`

      .. rubric:: Examples

      >>> strings = ak.array([f'{i} string {i}' for i in range(1, 6)])
      >>> num_matches, starts, lens = strings.find_locations('\d')
      >>> num_matches
      array([2, 2, 2, 2, 2])
      >>> starts
      array([0, 9, 0, 9, 0, 9, 0, 9, 0, 9])
      >>> lens
      array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))



   .. py:method:: findall(pattern: Union[bytes, arkouda.dtypes.str_scalars], return_match_origins: bool = False) -> Union[Strings, Tuple]

      Return a new Strings containg all non-overlapping matches of pattern

      :param pattern: Regex used to find matches
      :type pattern: str_scalars
      :param return_match_origins: If True, return a pdarray containing the index of the original string each
                                   pattern match is from
      :type return_match_origins: bool

      :returns: * *Strings* -- Strings object containing only pattern matches
                * *pdarray, int64 (optional)* -- The index of the original string each pattern match is from

      :raises TypeError: Raised if the pattern parameter is not bytes or str_scalars
      :raises ValueError: Raised if pattern is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.find_locations`

      .. rubric:: Examples

      >>> strings = ak.array(['1_2___', '____', '3', '__4___5____6___7', ''])
      >>> strings.findall('_+', return_match_origins=True)
      (array(['_', '___', '____', '__', '___', '____', '___']), array([0 0 1 3 3 3 3]))



   .. py:method:: flatten(delimiter: str, return_segments: bool = False, regex: bool = False) -> Union[Strings, Tuple]

      Unpack delimiter-joined substrings into a flat array.

      :param delimiter: Characters used to split strings into substrings
      :type delimiter: str
      :param return_segments: If True, also return mapping of original strings to first substring
                              in return array.
      :type return_segments: bool
      :param regex: Indicates whether delimiter is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: * *Strings* -- Flattened substrings with delimiters removed
                * *pdarray, int64 (optional)* -- For each original string, the index of first corresponding substring
                  in the return array

      .. seealso:: :obj:`peel`, :obj:`rpeel`

      .. rubric:: Examples

      >>> orig = ak.array(['one|two', 'three|four|five', 'six'])
      >>> orig.flatten('|')
      array(['one', 'two', 'three', 'four', 'five', 'six'])
      >>> flat, map = orig.flatten('|', return_segments=True)
      >>> map
      array([0, 2, 5])
      >>> under = ak.array(['one_two', 'three_____four____five', 'six'])
      >>> under_flat, under_map = under.flatten('_+', return_segments=True, regex=True)
      >>> under_flat
      array(['one', 'two', 'three', 'four', 'five', 'six'])
      >>> under_map
      array([0, 2, 5])



   .. py:method:: from_parts(offset_attrib: Union[arkouda.pdarrayclass.pdarray, str], bytes_attrib: Union[arkouda.pdarrayclass.pdarray, str]) -> Strings
      :staticmethod:


      Factory method for creating a Strings object from an Arkouda server
      response where the arrays are separate components.

      :param offset_attrib: the array containing the offsets
      :type offset_attrib: Union[pdarray, str]
      :param bytes_attrib: the array containing the string values
      :type bytes_attrib: Union[pdarray, str]

      :returns: object representing a segmented strings array on the server
      :rtype: Strings

      :raises RuntimeError: Raised if there's an error converting a server-returned str-descriptor

      .. rubric:: Notes

      This factory method is used when we construct the parts of a Strings
      object on the client side and transfer the offsets & bytes separately
      to the server.  This results in two entries in the symbol table and we
      need to instruct the server to assemble the into a composite entity.



   .. py:method:: from_return_msg(rep_msg: str) -> Strings
      :staticmethod:


      Factory method for creating a Strings object from an Arkouda server
      response message

      :param rep_msg: Server response message currently of form
                      `created name type size ndim shape itemsize+created bytes.size 1234`
      :type rep_msg: str

      :returns: object representing a segmented strings array on the server
      :rtype: Strings

      :raises RuntimeError: Raised if there's an error converting a server-returned str-descriptor

      .. rubric:: Notes

      We really don't have an itemsize because these are variable length strings.
      In the future we could probably use this position to store the total bytes.



   .. py:method:: fullmatch(pattern: Union[bytes, arkouda.dtypes.str_scalars]) -> arkouda.match.Match

      Returns a match object where elements match only if the whole string matches the
      regular expression pattern

      :param pattern: Regex used to find matches
      :type pattern: str

      :returns: Match object where elements match only if the whole string matches the
                regular expression pattern
      :rtype: Match

      .. rubric:: Examples

      >>> strings = ak.array(['1_2___', '____', '3', '__4___5____6___7', ''])
      >>> strings.fullmatch('_+')
      <ak.Match object: matched=False; matched=True, span=(0, 4); matched=False;
      matched=False; matched=False>



   .. py:method:: get_bytes()

      Getter for the bytes component (uint8 pdarray) of this Strings.

      :returns: Pdarray of bytes of the string accessed
      :rtype: pdarray, uint8

      .. rubric:: Example

      >>> x = ak.array(['one', 'two', 'three'])
      >>> x.get_bytes()
      [111 110 101 0 116 119 111 0 116 104 114 101 101 0]



   .. py:method:: get_lengths() -> arkouda.pdarrayclass.pdarray

      Return the length of each string in the array.

      :returns: The length of each string
      :rtype: pdarray, int

      :raises RuntimeError: Raised if there is a server-side error thrown



   .. py:method:: get_offsets()

      Getter for the offsets component (int64 pdarray) of this Strings.

      :returns: Pdarray of offsets of the string accessed
      :rtype: pdarray, int64

      .. rubric:: Example

      >>> x = ak.array(['one', 'two', 'three'])
      >>> x.get_offsets()
      [0 4 8]



   .. py:method:: get_prefixes(n: arkouda.dtypes.int_scalars, return_origins: bool = True, proper: bool = True) -> Union[Strings, Tuple[Strings, arkouda.pdarrayclass.pdarray]]

      Return the n-long prefix of each string, where possible

      :param n: Length of prefix
      :type n: int
      :param return_origins: If True, return a logical index indicating which strings
                             were long enough to return an n-prefix
      :type return_origins: bool
      :param proper: If True, only return proper prefixes, i.e. from strings
                     that are at least n+1 long. If False, allow the entire
                     string to be returned as a prefix.
      :type proper: bool

      :returns: * **prefixes** (*Strings*) -- The array of n-character prefixes; the number of elements is the number of
                  True values in the returned mask.
                * **origin_indices** (*pdarray, bool*) -- Boolean array that is True where the string was long enough to return
                  an n-character prefix, False otherwise.



   .. py:method:: get_suffixes(n: arkouda.dtypes.int_scalars, return_origins: bool = True, proper: bool = True) -> Union[Strings, Tuple[Strings, arkouda.pdarrayclass.pdarray]]

      Return the n-long suffix of each string, where possible

      :param n: Length of suffix
      :type n: int
      :param return_origins: If True, return a logical index indicating which strings
                             were long enough to return an n-suffix
      :type return_origins: bool
      :param proper: If True, only return proper suffixes, i.e. from strings
                     that are at least n+1 long. If False, allow the entire
                     string to be returned as a suffix.
      :type proper: bool

      :returns: * **suffixes** (*Strings*) -- The array of n-character suffixes; the number of elements is the number of
                  True values in the returned mask.
                * **origin_indices** (*pdarray, bool*) -- Boolean array that is True where the string was long enough to return
                  an n-character suffix, False otherwise.



   .. py:method:: group() -> arkouda.pdarrayclass.pdarray

      Return the permutation that groups the array, placing equivalent
      strings together. All instances of the same string are guaranteed to lie
      in one contiguous block of the permuted array, but the blocks are not
      necessarily ordered.

      :returns: The permutation that groups the array by value
      :rtype: pdarray

      .. seealso:: :obj:`GroupBy`, :obj:`unique`

      .. rubric:: Notes

      If the arkouda server is compiled with "-sSegmentedString.useHash=true",
      then arkouda uses 128-bit hash values to group strings, rather than sorting
      the strings directly. This method is fast, but the resulting permutation
      merely groups equivalent strings and does not sort them. If the "useHash"
      parameter is false, then a full sort is performed.

      :raises RuntimeError: Raised if there is a server-side error in executing group request or
          creating the pdarray encapsulating the return message



   .. py:method:: hash() -> Tuple[arkouda.pdarrayclass.pdarray, arkouda.pdarrayclass.pdarray]

      Compute a 128-bit hash of each string.

      :returns: A tuple of two int64 pdarrays. The ith hash value is the concatenation
                of the ith values from each array.
      :rtype: Tuple[pdarray,pdarray]

      .. rubric:: Notes

      The implementation uses SipHash128, a fast and balanced hash function (used
      by Python for dictionaries and sets). For realistic numbers of strings (up
      to about 10**15), the probability of a collision between two 128-bit hash
      values is negligible.



   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry

      :param None:

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: isalnum() -> arkouda.pdarrayclass.pdarray

      Returns a boolean pdarray where index i indicates whether string i of the
      Strings is alphanumeric.

      :returns: True for elements that are alphanumeric, False otherwise
      :rtype: pdarray, bool

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.islower`, :obj:`Strings.isupper`, :obj:`Strings.istitle`

      .. rubric:: Examples

      >>> not_alnum = ak.array([f'%Strings {i}' for i in range(3)])
      >>> alnum = ak.array([f'Strings{i}' for i in range(3)])
      >>> strings = ak.concatenate([not_alnum, alnum])
      >>> strings
      array(['%Strings 0', '%Strings 1', '%Strings 2', 'Strings0', 'Strings1', 'Strings2'])
      >>> strings.isalnum()
      array([False False False True True True])



   .. py:method:: isalpha() -> arkouda.pdarrayclass.pdarray

      Returns a boolean pdarray where index i indicates whether string i of the
      Strings is alphabetic.  This means there is at least one character,
      and all the characters are alphabetic.

      :returns: True for elements that are alphabetic, False otherwise
      :rtype: pdarray, bool

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.islower`, :obj:`Strings.isupper`, :obj:`Strings.istitle`, :obj:`Strings.isalnum`

      .. rubric:: Examples

      >>> not_alpha = ak.array([f'%Strings {i}' for i in range(3)])
      >>> alpha = ak.array(['StringA','StringB','StringC'])
      >>> strings = ak.concatenate([not_alpha, alpha])
      >>> strings
      array(['%Strings 0', '%Strings 1', '%Strings 2', 'StringA','StringB','StringC'])
      >>> strings.isalpha()
      array([False False False True True True])



   .. py:method:: isdecimal() -> arkouda.pdarrayclass.pdarray

      Returns a boolean pdarray where index i indicates whether string i of the
      Strings has all decimal characters.

      :returns: True for elements that are decimals, False otherwise
      :rtype: pdarray, bool

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.isdigit`

      .. rubric:: Examples

      >>> not_decimal = ak.array([f'Strings {i}' for i in range(3)])
      >>> decimal = ak.array([f'12{i}' for i in range(3)])
      >>> strings = ak.concatenate([not_decimal, decimal])
      >>> strings
      array(['Strings 0', 'Strings 1', 'Strings 2', '120', '121', '122'])
      >>> strings.isdecimal()
      array([False False False True True True])
      Special Character Examples
      >>> special_strings = ak.array(["3.14", "0", "", "2", "2x"])
      >>> special_strings
      array(['3.14', '0', '', '2', '2x'])
      >>> special_strings.isdecimal()
      array([False True False False False])



   .. py:method:: isdigit() -> arkouda.pdarrayclass.pdarray

      Returns a boolean pdarray where index i indicates whether string i of the
      Strings has all digit characters.

      :returns: True for elements that are digits, False otherwise
      :rtype: pdarray, bool

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.islower`, :obj:`Strings.isupper`, :obj:`Strings.istitle`

      .. rubric:: Examples

      >>> not_digit = ak.array([f'Strings {i}' for i in range(3)])
      >>> digit = ak.array([f'12{i}' for i in range(3)])
      >>> strings = ak.concatenate([not_digit, digit])
      >>> strings
      array(['Strings 0', 'Strings 1', 'Strings 2', '120', '121', '122'])
      >>> strings.isdigit()
      array([False False False True True True])
      Special Character Examples
      >>> special_strings = ak.array(["3.14", "0", "", "2", "2x"])
      >>> special_strings
      array(['3.14', '0', '', '2', '2x'])
      >>> special_strings.isdigit()
      array([False True True True False])



   .. py:method:: isempty() -> arkouda.pdarrayclass.pdarray

      Returns a boolean pdarray where index i indicates whether string i of the
      Strings is empty.


      True for elements that are the empty string, False otherwise

      :returns: True for elements that are digits, False otherwise
      :rtype: pdarray, bool

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.islower`, :obj:`Strings.isupper`, :obj:`Strings.istitle`

      .. rubric:: Examples

      >>> not_empty = ak.array([f'Strings {i}' for i in range(3)])
      >>> empty = ak.array(['' for i in range(3)])
      >>> strings = ak.concatenate([not_empty, empty])
      >>> strings
      array(['%Strings 0', '%Strings 1', '%Strings 2', '', '', ''])
      >>> strings.isempty()



   .. py:method:: islower() -> arkouda.pdarrayclass.pdarray

      Returns a boolean pdarray where index i indicates whether string i of the
      Strings is entirely lowercase

      :returns: True for elements that are entirely lowercase, False otherwise
      :rtype: pdarray, bool

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.isupper`

      .. rubric:: Examples

      >>> lower = ak.array([f'strings {i}' for i in range(3)])
      >>> upper = ak.array([f'STRINGS {i}' for i in range(3)])
      >>> strings = ak.concatenate([lower, upper])
      >>> strings
      array(['strings 0', 'strings 1', 'strings 2', 'STRINGS 0', 'STRINGS 1', 'STRINGS 2'])
      >>> strings.islower()
      array([True True True False False False])



   .. py:method:: isspace() -> arkouda.pdarrayclass.pdarray

              Returns a boolean pdarray where index i indicates whether string i has all
              whitespace characters ( ,    , 
      , 
      , 
      , 
      ).

              Returns
              -------
              pdarray, bool
                  True for elements that are whitespace, False otherwise

              Raises
              ------
              RuntimeError
                  Raised if there is a server-side error thrown

              See Also
              --------
              Strings.islower
              Strings.isupper
              Strings.istitle

              Examples
              --------
              >>> not_space = ak.array([f'Strings {i}' for i in range(3)])
              >>> space = ak.array([' ', '    ', '
      ', '
      ', '
      ', '
      ', '



      '])
              >>> strings = ak.concatenate([not_space, space])
              >>> strings
              array(['Strings 0', 'Strings 1', 'Strings 2', ' ',
              ... 'u0009', 'n', 'u000B', 'u000C', 'u000D', ' u0009nu000Bu000Cu000D'])
              >>> strings.isspace()
              array([False False False True True True True True True True])




   .. py:method:: istitle() -> arkouda.pdarrayclass.pdarray

      Returns a boolean pdarray where index i indicates whether string i of the
      Strings is titlecase

      :returns: True for elements that are titlecase, False otherwise
      :rtype: pdarray, bool

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.islower`, :obj:`Strings.isupper`

      .. rubric:: Examples

      >>> mixed = ak.array([f'sTrINgs {i}' for i in range(3)])
      >>> title = ak.array([f'Strings {i}' for i in range(3)])
      >>> strings = ak.concatenate([mixed, title])
      >>> strings
      array(['sTrINgs 0', 'sTrINgs 1', 'sTrINgs 2', 'Strings 0', 'Strings 1', 'Strings 2'])
      >>> strings.istitle()
      array([False False False True True True])



   .. py:method:: isupper() -> arkouda.pdarrayclass.pdarray

      Returns a boolean pdarray where index i indicates whether string i of the
      Strings is entirely uppercase

      :returns: True for elements that are entirely uppercase, False otherwise
      :rtype: pdarray, bool

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.islower`

      .. rubric:: Examples

      >>> lower = ak.array([f'strings {i}' for i in range(3)])
      >>> upper = ak.array([f'STRINGS {i}' for i in range(3)])
      >>> strings = ak.concatenate([lower, upper])
      >>> strings
      array(['strings 0', 'strings 1', 'strings 2', 'STRINGS 0', 'STRINGS 1', 'STRINGS 2'])
      >>> strings.isupper()
      array([False False False True True True])



   .. py:method:: lower() -> Strings

      Returns a new Strings with all uppercase characters from the original replaced with
      their lowercase equivalent

      :returns: Strings with all uppercase characters from the original replaced with
                their lowercase equivalent
      :rtype: Strings

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.upper`

      .. rubric:: Examples

      >>> strings = ak.array([f'StrINgS {i}' for i in range(5)])
      >>> strings
      array(['StrINgS 0', 'StrINgS 1', 'StrINgS 2', 'StrINgS 3', 'StrINgS 4'])
      >>> strings.lower()
      array(['strings 0', 'strings 1', 'strings 2', 'strings 3', 'strings 4'])



   .. py:method:: lstick(other: Strings, delimiter: Union[bytes, arkouda.dtypes.str_scalars] = '') -> Strings

      Join the strings from another array onto the left of the strings
      of this array, optionally inserting a delimiter.
      *Warning*: This function is experimental and not guaranteed to work.

      :param other: The strings to join onto self's strings
      :type other: Strings
      :param delimiter: String inserted between self and other
      :type delimiter: Union[bytes,str_scalars]

      :returns: The array of joined strings, as other + self
      :rtype: Strings

      :raises TypeError: Raised if the delimiter parameter is neither bytes nor a str
          or if the other parameter is not a Strings instance
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`stick`, :obj:`peel`, :obj:`rpeel`

      .. rubric:: Examples

      >>> s = ak.array(['a', 'c', 'e'])
      >>> t = ak.array(['b', 'd', 'f'])
      >>> s.lstick(t, delimiter='.')
      array(['b.a', 'd.c', 'f.e'])



   .. py:method:: match(pattern: Union[bytes, arkouda.dtypes.str_scalars]) -> arkouda.match.Match

      Returns a match object where elements match only if the beginning of the string matches the
      regular expression pattern

      :param pattern: Regex used to find matches
      :type pattern: str

      :returns: Match object where elements match only if the beginning of the string matches the
                regular expression pattern
      :rtype: Match

      .. rubric:: Examples

      >>> strings = ak.array(['1_2___', '____', '3', '__4___5____6___7', ''])
      >>> strings.match('_+')
      <ak.Match object: matched=False; matched=True, span=(0, 4); matched=False;
      matched=True, span=(0, 2); matched=False>



   .. py:attribute:: objType
      :value: 'Strings'



   .. py:method:: peel(delimiter: Union[bytes, arkouda.dtypes.str_scalars], times: arkouda.dtypes.int_scalars = 1, includeDelimiter: bool = False, keepPartial: bool = False, fromRight: bool = False, regex: bool = False) -> Tuple

      Peel off one or more delimited fields from each string (similar
      to string.partition), returning two new arrays of strings.
      *Warning*: This function is experimental and not guaranteed to work.

      :param delimiter: The separator where the split will occur
      :type delimiter: Union[bytes, str_scalars]
      :param times: The number of times the delimiter is sought, i.e. skip over
                    the first (times-1) delimiters
      :type times: Union[int, np.int64]
      :param includeDelimiter: If true, append the delimiter to the end of the first return
                               array. By default, it is prepended to the beginning of the
                               second return array.
      :type includeDelimiter: bool
      :param keepPartial: If true, a string that does not contain <times> instances of
                          the delimiter will be returned in the first array. By default,
                          such strings are returned in the second array.
      :type keepPartial: bool
      :param fromRight: If true, peel from the right instead of the left (see also rpeel)
      :type fromRight: bool
      :param regex: Indicates whether delimiter is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns:

                left: Strings
                    The field(s) peeled from the end of each string (unless
                    fromRight is true)
                right: Strings
                    The remainder of each string after peeling (unless fromRight
                    is true)
      :rtype: Tuple[Strings, Strings]

      :raises TypeError: Raised if the delimiter parameter is not byte or str_scalars, if
          times is not int64, or if includeDelimiter, keepPartial, or
          fromRight is not bool
      :raises ValueError: Raised if times is < 1 or if delimiter is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`rpeel`, :obj:`stick`, :obj:`lstick`

      .. rubric:: Examples

      >>> s = ak.array(['a.b', 'c.d', 'e.f.g'])
      >>> s.peel('.')
      (array(['a', 'c', 'e']), array(['b', 'd', 'f.g']))
      >>> s.peel('.', includeDelimiter=True)
      (array(['a.', 'c.', 'e.']), array(['b', 'd', 'f.g']))
      >>> s.peel('.', times=2)
      (array(['', '', 'e.f']), array(['a.b', 'c.d', 'g']))
      >>> s.peel('.', times=2, keepPartial=True)
      (array(['a.b', 'c.d', 'e.f']), array(['', '', 'g']))



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: purge_cached_regex_patterns() -> None

      purges cached regex patterns



   .. py:method:: register(user_defined_name: str) -> Strings

      Register this Strings object with a user defined name in the arkouda server
      so it can be attached to later using Strings.attach()
      This is an in-place operation, registering a Strings object more than once will
      update the name in the registry and remove the previously registered name.
      A name can only be registered to one object at a time.

      :param user_defined_name: user defined name which the Strings object is to be registered under
      :type user_defined_name: str

      :returns: The same Strings object which is now registered with the arkouda server and
                has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different objects with the same name.
      :rtype: Strings

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the Strings object with the user_defined_name
          If the user is attempting to register more than one object with the same name,
          the former should be unregistered first to free up the registration name.

      .. seealso:: :obj:`attach`, :obj:`unregister`

      .. rubric:: Notes

      Registered names/Strings objects in the server are immune to deletion
      until they are unregistered.



   .. py:method:: rpeel(delimiter: Union[bytes, arkouda.dtypes.str_scalars], times: arkouda.dtypes.int_scalars = 1, includeDelimiter: bool = False, keepPartial: bool = False, regex: bool = False)

      Peel off one or more delimited fields from the end of each string
      (similar to string.rpartition), returning two new arrays of strings.
      *Warning*: This function is experimental and not guaranteed to work.

      :param delimiter: The separator where the split will occur
      :type delimiter: Union[bytes, str_scalars]
      :param times: The number of times the delimiter is sought, i.e. skip over
                    the last (times-1) delimiters
      :type times: Union[int, np.int64]
      :param includeDelimiter: If true, prepend the delimiter to the start of the first return
                               array. By default, it is appended to the end of the
                               second return array.
      :type includeDelimiter: bool
      :param keepPartial: If true, a string that does not contain <times> instances of
                          the delimiter will be returned in the second array. By default,
                          such strings are returned in the first array.
      :type keepPartial: bool
      :param regex: Indicates whether delimiter is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns:

                left: Strings
                    The remainder of the string after peeling
                right: Strings
                    The field(s) that were peeled from the right of each string
      :rtype: Tuple[Strings, Strings]

      :raises TypeError: Raised if the delimiter parameter is not bytes or str_scalars or
          if times is not int64
      :raises ValueError: Raised if times is < 1 or if delimiter is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`peel`, :obj:`stick`, :obj:`lstick`

      .. rubric:: Examples

      >>> s = ak.array(['a.b', 'c.d', 'e.f.g'])
      >>> s.rpeel('.')
      (array(['a', 'c', 'e.f']), array(['b', 'd', 'g']))
      # Compared against peel
      >>> s.peel('.')
      (array(['a', 'c', 'e']), array(['b', 'd', 'f.g']))



   .. py:method:: save(prefix_path: str, dataset: str = 'strings_array', mode: str = 'truncate', save_offsets: bool = True, compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the Strings object to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. HDF5 support single files, in which case the file name will
      only be that provided. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: The name of the Strings dataset to be written, defaults to strings_array
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Strings dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param save_offsets: Defaults to True which will instruct the server to save the offsets array to HDF5
                           If False the offsets array will not be save and will be derived from the string values
                           upon load/read. This is not supported for Parquet files.
      :type save_offsets: bool
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str
      :param file_type: Default: Distribute
                        Distribute the dataset over a file per locale.
                        Single file will save the dataset to one file
      :type file_type: str ("single" | "distribute")

      :rtype: String message indicating result of save operation

      .. rubric:: Notes

      Important implementation notes: (1) Strings state is saved as two datasets
      within an hdf5 group: one for the string characters and one for the
      segments corresponding to the start of each string, (2) the hdf5 group is named
      via the dataset parameter. (3) Parquet files do not store the segments,
      only the values.



   .. py:method:: search(pattern: Union[bytes, arkouda.dtypes.str_scalars]) -> arkouda.match.Match

      Returns a match object with the first location in each element where pattern produces a match.
      Elements match if any part of the string matches the regular expression pattern

      :param pattern: Regex used to find matches
      :type pattern: str

      :returns: Match object where elements match if any part of the string matches the
                regular expression pattern
      :rtype: Match

      .. rubric:: Examples

      >>> strings = ak.array(['1_2___', '____', '3', '__4___5____6___7', ''])
      >>> strings.search('_+')
      <ak.Match object: matched=True, span=(1, 2); matched=True, span=(0, 4);
      matched=False; matched=True, span=(0, 2); matched=False>



   .. py:method:: split(pattern: Union[bytes, arkouda.dtypes.str_scalars], maxsplit: int = 0, return_segments: bool = False) -> Union[Strings, Tuple]

      Returns a new Strings split by the occurrences of pattern.
      If maxsplit is nonzero, at most maxsplit splits occur

      :param pattern: Regex used to split strings into substrings
      :type pattern: str
      :param maxsplit: The max number of pattern match occurences in each element to split.
                       The default maxsplit=0 splits on all occurences
      :type maxsplit: int
      :param return_segments: If True, return mapping of original strings to first substring
                              in return array.
      :type return_segments: bool

      :returns: * *Strings* -- Substrings with pattern matches removed
                * *pdarray, int64 (optional)* -- For each original string, the index of first corresponding substring
                  in the return array

      .. rubric:: Examples

      >>> strings = ak.array(['1_2___', '____', '3', '__4___5____6___7', ''])
      >>> strings.split('_+', maxsplit=2, return_segments=True)
      (array(['1', '2', '', '', '', '3', '', '4', '5____6___7', '']), array([0 3 5 6 9]))



   .. py:method:: startswith(substr: Union[bytes, arkouda.dtypes.str_scalars], regex: bool = False) -> arkouda.pdarrayclass.pdarray

      Check whether each element starts with the given substring.

      :param substr: The prefix to search for
      :type substr: Union[bytes, str_scalars]
      :param regex: Indicates whether substr is a regular expression
                    Note: only handles regular expressions supported by re2
                    (does not support lookaheads/lookbehinds)
      :type regex: bool

      :returns: True for elements that start with substr, False otherwise
      :rtype: pdarray, bool

      :raises TypeError: Raised if the substr parameter is not a bytes ior str_scalars
      :raises ValueError: Rasied if substr is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.contains`, :obj:`Strings.endswith`

      .. rubric:: Examples

      >>> strings_end = ak.array([f'string {i}' for i in range(1, 6)])
      >>> strings_end
      array(['string 1', 'string 2', 'string 3', 'string 4', 'string 5'])
      >>> strings_end.startswith('string')
      array([True, True, True, True, True])
      >>> strings_start = ak.array([f'{i} string' for i in range(1,6)])
      >>> strings_start
      array(['1 string', '2 string', '3 string', '4 string', '5 string'])
      >>> strings_start.startswith('\d str', regex = True)
      array([True, True, True, True, True])



   .. py:method:: stick(other: Strings, delimiter: Union[bytes, arkouda.dtypes.str_scalars] = '', toLeft: bool = False) -> Strings

      Join the strings from another array onto one end of the strings
      of this array, optionally inserting a delimiter.
      *Warning*: This function is experimental and not guaranteed to work.

      :param other: The strings to join onto self's strings
      :type other: Strings
      :param delimiter: String inserted between self and other
      :type delimiter: str
      :param toLeft: If true, join other strings to the left of self. By default,
                     other is joined to the right of self.
      :type toLeft: bool

      :returns: The array of joined strings
      :rtype: Strings

      :raises TypeError: Raised if the delimiter parameter is not bytes or str_scalars
          or if the other parameter is not a Strings instance
      :raises ValueError: Raised if times is < 1
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`lstick`, :obj:`peel`, :obj:`rpeel`

      .. rubric:: Examples

      >>> s = ak.array(['a', 'c', 'e'])
      >>> t = ak.array(['b', 'd', 'f'])
      >>> s.stick(t, delimiter='.')
      array(['a.b', 'c.d', 'e.f'])



   .. py:method:: strip(chars: Optional[Union[bytes, arkouda.dtypes.str_scalars]] = '') -> Strings

      Returns a new Strings object with all leading and trailing occurrences of characters contained
      in chars removed. The chars argument is a string specifying the set of characters to be removed.
      If omitted, the chars argument defaults to removing whitespace. The chars argument is not a
      prefix or suffix; rather, all combinations of its values are stripped.

      :param chars: the set of characters to be removed

      :returns: Strings object with the leading and trailing characters matching the set of characters in
                the chars argument removed
      :rtype: Strings

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. rubric:: Examples

      >>> strings = ak.array(['Strings ', '  StringS  ', 'StringS   '])
      >>> s = strings.strip()
      >>> s
      array(['Strings', 'StringS', 'StringS'])

      >>> strings = ak.array(['Strings 1', '1 StringS  ', '  1StringS  12 '])
      >>> s = strings.strip(' 12')
      >>> s
      array(['Strings', 'StringS', 'StringS'])



   .. py:method:: sub(pattern: Union[bytes, arkouda.dtypes.str_scalars], repl: Union[bytes, arkouda.dtypes.str_scalars], count: int = 0) -> Strings

      Return new Strings obtained by replacing non-overlapping occurrences of pattern with the
      replacement repl.
      If count is nonzero, at most count substitutions occur

      :param pattern: The regex to substitue
      :type pattern: str_scalars
      :param repl: The substring to replace pattern matches with
      :type repl: str_scalars
      :param count: The max number of pattern match occurences in each element to replace.
                    The default count=0 replaces all occurences of pattern with repl
      :type count: int

      :returns: Strings with pattern matches replaced
      :rtype: Strings

      :raises TypeError: Raised if pattern or repl are not bytes or str_scalars
      :raises ValueError: Raised if pattern is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.subn`

      .. rubric:: Examples

      >>> strings = ak.array(['1_2___', '____', '3', '__4___5____6___7', ''])
      >>> strings.sub(pattern='_+', repl='-', count=2)
      array(['1-2-', '-', '3', '-4-5____6___7', ''])



   .. py:method:: subn(pattern: Union[bytes, arkouda.dtypes.str_scalars], repl: Union[bytes, arkouda.dtypes.str_scalars], count: int = 0) -> Tuple

      Perform the same operation as sub(), but return a tuple (new_Strings, number_of_substitions)

      :param pattern: The regex to substitue
      :type pattern: str_scalars
      :param repl: The substring to replace pattern matches with
      :type repl: str_scalars
      :param count: The max number of pattern match occurences in each element to replace.
                    The default count=0 replaces all occurences of pattern with repl
      :type count: int

      :returns: * *Strings* -- Strings with pattern matches replaced
                * *pdarray, int64* -- The number of substitutions made for each element of Strings

      :raises TypeError: Raised if pattern or repl are not bytes or str_scalars
      :raises ValueError: Raised if pattern is not a valid regex
      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.sub`

      .. rubric:: Examples

      >>> strings = ak.array(['1_2___', '____', '3', '__4___5____6___7', ''])
      >>> strings.subn(pattern='_+', repl='-', count=2)
      (array(['1-2-', '-', '3', '-4-5____6___7', '']), array([2 1 0 2 0]))



   .. py:method:: title() -> Strings

      Returns a new Strings from the original replaced with their titlecase equivalent.

      :returns: Strings from the original replaced with their titlecase equivalent.
      :rtype: Strings

      :raises RuntimeError: Raised if there is a server-side error thrown.

      .. seealso:: :obj:`Strings.lower`, :obj:`String.upper`

      .. rubric:: Examples

      >>> strings = ak.array([f'StrINgS {i}' for i in range(5)])
      >>> strings
      array(['StrINgS 0', 'StrINgS 1', 'StrINgS 2', 'StrINgS 3', 'StrINgS 4'])
      >>> strings.title()
      array(['Strings 0', 'Strings 1', 'Strings 2', 'Strings 3', 'Strings 4'])



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'strings_array', col_delim: str = ',', overwrite: bool = False)

      Write Strings to CSV file(s). File will contain a single column with the Strings data.
      All CSV Files written by Arkouda include a header denoting data types of the columns.
      Unlike other file formats, CSV files store Strings as their UTF-8 format instead of storing
      bytes as uint(8).

      :param prefix_path: The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                          when they are written to disk.
      :type prefix_path: str
      :param dataset: Column name to save the Strings under. Defaults to "strings_array".
      :type dataset: str
      :param col_delim: Defaults to ",". Value to be used to separate columns within the file.
                        Please be sure that the value used DOES NOT appear in your dataset.
      :type col_delim: str
      :param overwrite: Defaults to False. If True, any existing files matching your provided prefix_path will
                        be overwritten. If False, an error will be returned if existing files are found.
      :type overwrite: bool

      :rtype: str reponse message

      :raises ValueError: Raised if all datasets are not present in all parquet files or if one or
          more of the specified files do not exist
      :raises RuntimeError: Raised if one or more of the specified files cannot be opened.
          If `allow_errors` is true this may be raised if no values are returned
          from the server.
      :raises TypeError: Raised if we receive an unknown arkouda_type returned from the server

      .. rubric:: Notes

      - CSV format is not currently supported by load/load_all operations
      - The column delimiter is expected to be the same for column names and data
      - Be sure that column delimiters are not found within your data.
      - All CSV files must delimit rows using newline (``\n``) at this time.



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'strings_array', mode: str = 'truncate', save_offsets: bool = True, file_type: str = 'distribute') -> str

      Save the Strings object to HDF5.
      The object can be saved to a collection of files or single file.

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: The name of the Strings dataset to be written, defaults to strings_array
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', create a new Strings dataset within existing files.
      :type mode: str {'truncate' | 'append'}
      :param save_offsets: Defaults to True which will instruct the server to save the offsets array to HDF5
                           If False the offsets array will not be save and will be derived from the string values
                           upon load/read.
      :type save_offsets: bool
      :param file_type: Default: Distribute
                        Distribute the dataset over a file per locale.
                        Single file will save the dataset to one file
      :type file_type: str ("single" | "distribute")

      :rtype: String message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - Parquet files do not store the segments, only the values.
      - Strings state is saved as two datasets within an hdf5 group:
        one for the string characters and one for the
        segments corresponding to the start of each string
      - the hdf5 group is named via the dataset parameter.
      - The prefix_path must be visible to the arkouda server and the user must
        have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
        ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
        the file name will be `prefix_path`.
      - If any of the output files already exist and
        the mode is 'truncate', they will be overwritten. If the mode is 'append'
        and the number of output files is less than the number of locales or a
        dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
        determine the file format.

      .. seealso:: :obj:`to_hdf`



   .. py:method:: to_list() -> list

      Convert the SegString to a list, transferring data from the
      arkouda server to Python. If the SegString exceeds a built-in size limit,
      a RuntimeError is raised.

      :returns: A list with the same strings as this SegString
      :rtype: list

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> a = ak.array(["hello", "my", "world"])
      >>> a.to_list()
      ['hello', 'my', 'world']
      >>> type(a.to_list())
      list



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from the
      arkouda server to Python. If the array exceeds a built-in size limit,
      a RuntimeError is raised.

      :returns: A numpy ndarray with the same strings as this array
      :rtype: np.ndarray

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``ak.client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting ak.client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> a = ak.array(["hello", "my", "world"])
      >>> a.to_ndarray()
      array(['hello', 'my', 'world'], dtype='<U5')
      >>> type(a.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'strings_array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      Save the Strings object to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.



   .. py:method:: transfer(hostname: str, port: arkouda.dtypes.int_scalars)

      Sends a Strings object to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the Strings object is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister() -> None

      Unregister a Strings object in the arkouda server which was previously
      registered using register() and/or attached to using attach()


      :rtype: None

      :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

      .. seealso:: :obj:`register`, :obj:`attach`

      .. rubric:: Notes

      Registered names/Strings objects in the server are immune to deletion until
      they are unregistered.



   .. py:method:: unregister_strings_by_name(user_defined_name: str) -> None
      :staticmethod:


      Unregister a Strings object in the arkouda server previously registered via register()

      :param user_defined_name: The registered name of the Strings object
      :type user_defined_name: str

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`attach`, :obj:`is_registered`



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'strings_array', save_offsets: bool = True, repack: bool = True)

      Overwrite the dataset with the name provided with this Strings object. If
      the dataset does not exist it is added

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param save_offsets: Defaults to True which will instruct the server to save the offsets array to HDF5
                           If False the offsets array will not be save and will be derived from the string values
                           upon load/read.
      :type save_offsets: bool
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the Strings object

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added



   .. py:method:: upper() -> Strings

      Returns a new Strings with all lowercase characters from the original replaced with
      their uppercase equivalent

      :returns: Strings with all lowercase characters from the original replaced with
                their uppercase equivalent
      :rtype: Strings

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`Strings.lower`

      .. rubric:: Examples

      >>> strings = ak.array([f'StrINgS {i}' for i in range(5)])
      >>> strings
      array(['StrINgS 0', 'StrINgS 1', 'StrINgS 2', 'StrINgS 3', 'StrINgS 4'])
      >>> strings.upper()
      array(['STRINGS 0', 'STRINGS 1', 'STRINGS 2', 'STRINGS 3', 'STRINGS 4'])



.. py:data:: akfloat64

.. py:data:: akint64

.. py:data:: akuint64

.. py:function:: align(*args)

   Map multiple arrays of sparse identifiers to a common 0-up index.

   :param \*args: Arrays to map to dense index
   :type \*args: pdarrays or sequences of pdarrays

   :returns: **aligned** -- Arrays with values replaced by 0-up indices
   :rtype: list of pdarrays


.. py:function:: arange(*args, **kwargs) -> arkouda.pdarrayclass.pdarray

   arange([start,] stop[, stride,] dtype=int64)

   Create a pdarray of consecutive integers within the interval [start, stop).
   If only one arg is given then arg is the stop parameter. If two args are
   given, then the first arg is start and second is stop. If three args are
   given, then the first arg is start, second is stop, third is stride.

   The return value is cast to type dtype

   :param start: Starting value (inclusive)
   :type start: int_scalars, optional
   :param stop: Stopping value (exclusive)
   :type stop: int_scalars
   :param stride: The difference between consecutive elements, the default stride is 1,
                  if stride is specified then start must also be specified.
   :type stride: int_scalars, optional
   :param dtype: The target dtype to cast values to
   :type dtype: np.dtype, type, or str
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Integers from start (inclusive) to stop (exclusive) by stride
   :rtype: pdarray, dtype

   :raises TypeError: Raised if start, stop, or stride is not an int object
   :raises ZeroDivisionError: Raised if stride == 0

   .. seealso:: :obj:`linspace`, :obj:`zeros`, :obj:`ones`, :obj:`randint`

   .. rubric:: Notes

   Negative strides result in decreasing values. Currently, only int64
   pdarrays can be created with this method. For float64 arrays, use
   the linspace method.

   .. rubric:: Examples

   >>> ak.arange(0, 5, 1)
   array([0, 1, 2, 3, 4])

   >>> ak.arange(5, 0, -1)
   array([5, 4, 3, 2, 1])

   >>> ak.arange(0, 10, 2)
   array([0, 2, 4, 6, 8])

   >>> ak.arange(-5, -10, -1)
   array([-5, -6, -7, -8, -9])


.. py:function:: argsort(pda: Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical], algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD, axis: arkouda.dtypes.int_scalars = 0) -> arkouda.pdarrayclass.pdarray

   Return the permutation that sorts the array.

   :param pda: The array to sort (int64, uint64, or float64)
   :type pda: pdarray or Strings or Categorical

   :returns: The indices such that ``pda[indices]`` is sorted
   :rtype: pdarray, int64

   :raises TypeError: Raised if the parameter is other than a pdarray or Strings

   .. seealso:: :obj:`coargsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and
   resilient to non-uniformity in data but communication intensive.

   .. rubric:: Examples

   >>> a = ak.randint(0, 10, 10)
   >>> perm = ak.argsort(a)
   >>> a[perm]
   array([0, 1, 1, 3, 4, 5, 7, 8, 8, 9])


.. py:data:: bigint

.. py:function:: broadcast(segments: pdarray, values: Union[pdarray, Strings], size: Union[int, np.int64, np.uint64] = -1, permutation: Union[pdarray, None] = None)

   Broadcast a dense column vector to the rows of a sparse matrix or grouped array.

   :param segments: Offsets of the start of each row in the sparse matrix or grouped array.
                    Must be sorted in ascending order.
   :type segments: pdarray, int64
   :param values: The values to broadcast, one per row (or group)
   :type values: pdarray, Strings
   :param size: The total number of nonzeros in the matrix. If permutation is given, this
                argument is ignored and the size is inferred from the permutation array.
   :type size: int
   :param permutation: The permutation to go from the original ordering of nonzeros to the ordering
                       grouped by row. To broadcast values back to the original ordering, this
                       permutation will be inverted. If no permutation is supplied, it is assumed
                       that the original nonzeros were already grouped by row. In this case, the
                       size argument must be given.
   :type permutation: pdarray, int64

   :returns: The broadcast values, one per nonzero
   :rtype: pdarray, Strings

   :raises ValueError: - If segments and values are different sizes
       - If segments are empty
       - If number of nonzeros (either user-specified or inferred from permutation)
         is less than one

   .. rubric:: Examples

   >>>
   # Define a sparse matrix with 3 rows and 7 nonzeros
   >>> row_starts = ak.array([0, 2, 5])
   >>> nnz = 7
   # Broadcast the row number to each nonzero element
   >>> row_number = ak.arange(3)
   >>> ak.broadcast(row_starts, row_number, nnz)
   array([0 0 1 1 1 2 2])
   # If the original nonzeros were in reverse order...
   >>> permutation = ak.arange(6, -1, -1)
   >>> ak.broadcast(row_starts, row_number, permutation=permutation)
   array([2 2 1 1 1 0 0])


.. py:function:: coargsort(arrays: Sequence[Union[arkouda.strings.Strings, arkouda.pdarrayclass.pdarray, arkouda.categorical.Categorical]], algorithm: SortingAlgorithm = SortingAlgorithm.RadixSortLSD) -> arkouda.pdarrayclass.pdarray

   Return the permutation that groups the rows (left-to-right), if the
   input arrays are treated as columns. The permutation sorts numeric
   columns, but not strings/Categoricals -- strings/Categoricals are grouped, but not ordered.

   :param arrays: The columns (int64, uint64, float64, Strings, or Categorical) to sort by row
   :type arrays: Sequence[Union[Strings, pdarray, Categorical]]

   :returns: The indices that permute the rows to grouped order
   :rtype: pdarray, int64

   :raises ValueError: Raised if the pdarrays are not of the same size or if the parameter
       is not an Iterable containing pdarrays, Strings, or Categoricals

   .. seealso:: :obj:`argsort`

   .. rubric:: Notes

   Uses a least-significant-digit radix sort, which is stable and resilient
   to non-uniformity in data but communication intensive. Starts with the
   last array and moves forward. This sort operates directly on numeric types,
   but for Strings, it operates on a hash. Thus, while grouping of equivalent
   strings is guaranteed, lexicographic ordering of the groups is not. For Categoricals,
   coargsort sorts based on Categorical.codes which guarantees grouping of equivalent categories
   but not lexicographic ordering of those groups.

   .. rubric:: Examples

   >>> a = ak.array([0, 1, 0, 1])
   >>> b = ak.array([1, 1, 0, 0])
   >>> perm = ak.coargsort([a, b])
   >>> perm
   array([2, 0, 3, 1])
   >>> a[perm]
   array([0, 0, 1, 1])
   >>> b[perm]
   array([0, 1, 0, 1])


.. py:function:: concatenate(arrays: Sequence[Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, Categorical]], ordered: bool = True) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, Categorical]

   Concatenate a list or tuple of ``pdarray`` or ``Strings`` objects into
   one ``pdarray`` or ``Strings`` object, respectively.

   :param arrays: The arrays to concatenate. Must all have same dtype.
   :type arrays: Sequence[Union[pdarray,Strings,Categorical]]
   :param ordered: If True (default), the arrays will be appended in the
                   order given. If False, array data may be interleaved
                   in blocks, which can greatly improve performance but
                   results in non-deterministic ordering of elements.
   :type ordered: bool

   :returns: Single pdarray or Strings object containing all values, returned in
             the original order
   :rtype: Union[pdarray,Strings,Categorical]

   :raises ValueError: Raised if arrays is empty or if 1..n pdarrays have
       differing dtypes
   :raises TypeError: Raised if arrays is not a pdarrays or Strings python Sequence such as a
       list or tuple
   :raises RuntimeError: Raised if 1..n array elements are dtypes for which
       concatenate has not been implemented.

   .. rubric:: Examples

   >>> ak.concatenate([ak.array([1, 2, 3]), ak.array([4, 5, 6])])
   array([1, 2, 3, 4, 5, 6])

   >>> ak.concatenate([ak.array([True,False,True]),ak.array([False,True,True])])
   array([True, False, True, False, True, True])

   >>> ak.concatenate([ak.array(['one','two']),ak.array(['three','four','five'])])
   array(['one', 'two', 'three', 'four', 'five'])


.. py:function:: find(query, space)

   Return indices of query items in a search list of items (-1 if not found).

   :param query: The items to search for. If multiple arrays, each "row" is an item.
   :type query: (sequence of) array-like
   :param space: The set of items in which to search. Must have same shape/dtype as query.
   :type space: (sequence of) array-like

   :returns: **indices** -- For each item in query, its index in space or -1 if not found.
   :rtype: pdarray, int64


.. py:function:: full(size: Union[arkouda.dtypes.int_scalars, str], fill_value: Union[arkouda.dtypes.numeric_scalars, str], dtype: Union[numpy.dtype, type, str, arkouda.dtypes.BigInt] = float64, max_bits: Optional[int] = None) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings]

   Create a pdarray filled with fill_value.

   :param size: Size of the array (only rank-1 arrays supported)
   :type size: int_scalars
   :param fill_value: Value with which the array will be filled
   :type fill_value: int_scalars
   :param dtype: Resulting array type, default float64
   :type dtype: all_scalars
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: array of the requested size and dtype filled with fill_value
   :rtype: pdarray or Strings

   :raises TypeError: Raised if the supplied dtype is not supported or if the size
       parameter is neither an int nor a str that is parseable to an int.

   .. seealso:: :obj:`zeros`, :obj:`ones`

   .. rubric:: Examples

   >>> ak.full(5, 7, dtype=ak.int64)
   array([7, 7, 7, 7, 7])

   >>> ak.full(5, 9, dtype=ak.float64)
   array([9, 9, 9, 9, 9])

   >>> ak.full(5, 5, dtype=ak.bool)
   array([True, True, True, True, True])


.. py:function:: in1d(pda1: arkouda.groupbyclass.groupable, pda2: arkouda.groupbyclass.groupable, assume_unique: bool = False, symmetric: bool = False, invert: bool = False) -> Union[arkouda.pdarrayclass.pdarray, arkouda.groupbyclass.groupable]

   Test whether each element of a 1-D array is also present in a second array.

   Returns a boolean array the same length as `pda1` that is True
   where an element of `pda1` is in `pda2` and False otherwise.

   Support multi-level -- test membership of rows of a in the set of rows of b.

   :param a: Rows are elements for which to test membership in b
   :type a: list of pdarrays, pdarray, Strings, or Categorical
   :param b: Rows are elements of the set in which to test membership
   :type b: list of pdarrays, pdarray, Strings, or Categorical
   :param assume_unique: If true, assume rows of a and b are each unique and sorted.
                         By default, sort and unique them explicitly.
   :type assume_unique: bool
   :param symmetric: Return in1d(pda1, pda2), in1d(pda2, pda1) when pda1 and 2 are single items.
   :type symmetric: bool
   :param invert: If True, the values in the returned array are inverted (that is,
                  False where an element of `pda1` is in `pda2` and True otherwise).
                  Default is False. ``ak.in1d(a, b, invert=True)`` is equivalent
                  to (but is faster than) ``~ak.in1d(a, b)``.
   :type invert: bool, optional

   :returns: * True for each row in a that is contained in b
             * *Return Type*
             * *------------* -- pdarray, bool

   .. rubric:: Notes

   Only works for pdarrays of int64 dtype, float64, Strings, or Categorical


.. py:function:: in1d_intervals(vals, intervals, symmetric=False)

   Test each value for membership in *any* of a set of half-open (pythonic)
   intervals.

   :param vals: Values to test for membership in intervals
   :type vals: pdarray(int, float)
   :param intervals: Non-overlapping, half-open intervals, as a tuple of
                     (lower_bounds_inclusive, upper_bounds_exclusive)
   :type intervals: 2-tuple of pdarrays
   :param symmetric: If True, also return boolean pdarray indicating which intervals
                     contained one or more query values.
   :type symmetric: bool

   :returns: * *pdarray(bool)* -- Array of same length as <vals>, True if corresponding value is
               included in any of the ranges defined by (low[i], high[i]) inclusive.
             * *pdarray(bool) (if symmetric=True)* -- Array of same length as number of intervals, True if corresponding
               interval contains any of the values in <vals>.

   .. rubric:: Notes

   First return array is equivalent to the following:
       ((vals >= intervals[0][0]) & (vals < intervals[1][0])) |
       ((vals >= intervals[0][1]) & (vals < intervals[1][1])) |
       ...
       ((vals >= intervals[0][-1]) & (vals < intervals[1][-1]))
   But much faster when testing many ranges.

   Second (optional) return array is equivalent to:
       ((intervals[0] <= vals[0]) & (intervals[1] > vals[0])) |
       ((intervals[0] <= vals[1]) & (intervals[1] > vals[1])) |
       ...
       ((intervals[0] <= vals[-1]) & (intervals[1] > vals[-1]))
   But much faster when vals is non-trivial size.


.. py:function:: interval_lookup(keys, values, arguments, fillvalue=-1, tiebreak=None, hierarchical=False)

   Apply a function defined over intervals to an array of arguments.

   :param keys: Tuple of closed intervals expressed as (lower_bounds_inclusive, upper_bounds_inclusive).
                Must have same dtype(s) as vals.
   :type keys: 2-tuple of (sequences of) pdarrays
   :param values: Function value to return for each entry in keys.
   :type values: pdarray
   :param arguments: Values to search for in intervals. If multiple arrays, each "row" is an item.
   :type arguments: (sequences of) pdarray
   :param fillvalue: Default value to return when argument is not in any interval.
   :type fillvalue: scalar
   :param tiebreak: When an argument is present in more than one key interval, the interval with the
                    lowest tiebreak value will be chosen. If no tiebreak is given, the
                    first valid key interval will be chosen.
   :type tiebreak: (optional) pdarray, numeric

   :returns: Value of function corresponding to the keys interval
             containing each argument, or fillvalue if argument not
             in any interval.
   :rtype: pdarray


.. py:function:: is_cosorted(arrays)

   Return True iff the arrays are cosorted, i.e., if the arrays were columns in a table
   then the rows are sorted.

   :param arrays: Arrays to check for cosortedness
   :type arrays: list-like of pdarrays

   :returns: True iff arrays are cosorted.
   :rtype: bool

   :raises ValueError: Raised if arrays are not the same length
   :raises TypeError: Raised if arrays is not a list-like of pdarrays


.. py:function:: left_align(left, right)

   Map two arrays of sparse identifiers to the 0-up index set implied by the left array,
   discarding values from right that do not appear in left.


.. py:function:: lookup(keys, values, arguments, fillvalue=-1)

   Apply the function defined by the mapping keys --> values to arguments.

   :param keys: The domain of the function. Entries must be unique (if a sequence of
                arrays is given, each row is treated as a tuple-valued entry).
   :type keys: (sequence of) array-like
   :param values: The range of the function. Must be same length as keys.
   :type values: pdarray
   :param arguments: The arguments on which to evaluate the function. Must have same dtype
                     (or tuple of dtypes, for a sequence) as keys.
   :type arguments: (sequence of) array-like
   :param fillvalue: The default value to return for arguments not in keys.
   :type fillvalue: scalar

   :returns: **evaluated** -- The result of evaluating the function over arguments.
   :rtype: pdarray

   .. rubric:: Notes

   While the values cannot be Strings (or other complex objects), the same
   result can be achieved by passing an arange as the values, then using
   the return as indices into the desired object.

   .. rubric:: Examples

   # Lookup numbers by two-word name
   >>> keys1 = ak.array(['twenty' for _ in range(5)])
   >>> keys2 = ak.array(['one', 'two', 'three', 'four', 'five'])
   >>> values = ak.array([21, 22, 23, 24, 25])
   >>> args1 = ak.array(['twenty', 'thirty', 'twenty'])
   >>> args2 = ak.array(['four', 'two', 'two'])
   >>> aku.lookup([keys1, keys2], values, [args1, args2])
   array([24, -1, 22])

   # Other direction requires an intermediate index
   >>> revkeys = values
   >>> revindices = ak.arange(values.size)
   >>> revargs = ak.array([24, 21, 22])
   >>> idx = aku.lookup(revkeys, revindices, revargs)
   >>> keys1[idx], keys2[idx]
   (array(['twenty', 'twenty', 'twenty']),
   array(['four', 'one', 'two']))


.. py:function:: ones(size: Union[arkouda.dtypes.int_scalars, str], dtype: Union[numpy.dtype, type, str, arkouda.dtypes.BigInt] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with ones.

   :param size: Size of the array (only rank-1 arrays supported)
   :type size: int_scalars
   :param dtype: Resulting array type, default float64
   :type dtype: Union[float64, int64, bool]
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Ones of the requested size and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported or if the size
       parameter is neither an int nor a str that is parseable to an int.

   .. seealso:: :obj:`zeros`, :obj:`ones_like`

   .. rubric:: Examples

   >>> ak.ones(5, dtype=ak.int64)
   array([1, 1, 1, 1, 1])

   >>> ak.ones(5, dtype=ak.float64)
   array([1, 1, 1, 1, 1])

   >>> ak.ones(5, dtype=ak.bool)
   array([True, True, True, True, True])


.. py:class:: pdarray(name: str, mydtype: Union[numpy.dtype, str], size: arkouda.dtypes.int_scalars, ndim: arkouda.dtypes.int_scalars, shape: Sequence[int], itemsize: arkouda.dtypes.int_scalars, max_bits: Optional[int] = None)

   The basic arkouda array class. This class contains only the
   attributies of the array; the data resides on the arkouda
   server. When a server operation results in a new array, arkouda
   will create a pdarray instance that points to the array data on
   the server. As such, the user should not initialize pdarray
   instances directly.

   .. attribute:: name

      The server-side identifier for the array

      :type: str

   .. attribute:: dtype

      The element type of the array

      :type: dtype

   .. attribute:: size

      The number of elements in the array

      :type: int_scalars

   .. attribute:: ndim

      The rank of the array (currently only rank 1 arrays supported)

      :type: int_scalars

   .. attribute:: shape

      A list or tuple containing the sizes of each dimension of the array

      :type: Sequence[int]

   .. attribute:: itemsize

      The size in bytes of each element

      :type: int_scalars


   .. py:attribute:: BinOps


   .. py:attribute:: OpEqOps


   .. py:method:: all() -> numpy.bool_

      Return True iff all elements of the array evaluate to True.



   .. py:method:: any() -> numpy.bool_

      Return True iff any element of the array evaluates to True.



   .. py:method:: argmax() -> Union[numpy.int64, numpy.uint64]

      Return the index of the first occurrence of the array max value.



   .. py:method:: argmaxk(k: arkouda.dtypes.int_scalars) -> pdarray

      Finds the indices corresponding to the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the  maximum `k` values, sorted
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: argmin() -> Union[numpy.int64, numpy.uint64]

      Return the index of the first occurrence of the array min value



   .. py:method:: argmink(k: arkouda.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: Indices corresponding to the maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: astype(dtype) -> pdarray

      Cast values of pdarray to provided dtype

      :param dtype: Dtype to cast to
      :type dtype: np.dtype or str

      :returns: An arkouda pdarray with values converted to the specified data type
      :rtype: ak.pdarray

      .. rubric:: Notes

      This is essentially shorthand for ak.cast(x, '<dtype>') where x is a pdarray.



   .. py:method:: attach(user_defined_name: str) -> pdarray
      :staticmethod:


      class method to return a pdarray attached to the registered name in the arkouda
      server which was registered using register()

      :param user_defined_name: user defined name which array was registered under
      :type user_defined_name: str

      :returns: pdarray which is bound to the corresponding server side component which was registered
                with user_defined_name
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: bigint_to_uint_arrays() -> List[pdarray]

      Creates a list of uint pdarrays from a bigint pdarray.
      The first item in return will be the highest 64 bits of the
      bigint pdarray and the last item will be the lowest 64 bits.

      :returns: A list of uint pdarrays where:
                The first item in return will be the highest 64 bits of the
                bigint pdarray and the last item will be the lowest 64 bits.
      :rtype: List[pdarrays]

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. seealso:: :obj:`pdarraycreation.bigint_from_uint_arrays`

      .. rubric:: Examples

      >>> a = ak.arange(2**64, 2**64 + 5)
      >>> a
      array(["18446744073709551616" "18446744073709551617" "18446744073709551618"
      "18446744073709551619" "18446744073709551620"])

      >>> a.bigint_to_uint_arrays()
      [array([1 1 1 1 1]), array([0 1 2 3 4])]



   .. py:method:: clz() -> pdarray

      Count the number of leading zeros in each element. See `ak.clz`.



   .. py:method:: corr(y: pdarray) -> numpy.float64

      Compute the correlation between self and y using pearson correlation coefficient.

      :param y: Other pdarray used to calculate correlation
      :type y: pdarray

      :returns: The scalar correlation of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: cov(y: pdarray) -> numpy.float64

      Compute the covariance between self and y.

      :param y: Other pdarray used to calculate covariance
      :type y: pdarray

      :returns: The scalar covariance of the two arrays
      :rtype: np.float64

      :raises TypeError: Raised if y is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: ctz() -> pdarray

      Count the number of trailing zeros in each element. See `ak.ctz`.



   .. py:method:: fill(value: arkouda.dtypes.numeric_scalars) -> None

      Fill the array (in place) with a constant value.

      :param value:
      :type value: numeric_scalars

      :raises TypeError: Raised if value is not an int, int64, float, or float64



   .. py:method:: format_other(other) -> str

      Attempt to cast scalar other to the element dtype of this pdarray,
      and print the resulting value to a string (e.g. for sending to a
      server command). The user should not call this function directly.

      :param other: The scalar to be cast to the pdarray.dtype
      :type other: object

      :rtype: string representation of np.dtype corresponding to the other parameter

      :raises TypeError: Raised if the other parameter cannot be converted to
          Numpy dtype



   .. py:method:: info() -> str

      Returns a JSON formatted string containing information about all components of self

      :param None:

      :returns: JSON string containing information about all components of self
      :rtype: str



   .. py:method:: is_registered() -> numpy.bool_

      Return True iff the object is contained in the registry

      :param None:

      :returns: Indicates if the object is contained in the registry
      :rtype: bool

      :raises RuntimeError: Raised if there's a server-side error thrown

      .. note::

         This will return True if the object is registered itself or as a component
         of another object



   .. py:method:: is_sorted() -> numpy.bool_

      Return True iff the array is monotonically non-decreasing.

      :param None:

      :returns: Indicates if the array is monotonically non-decreasing
      :rtype: bool

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: max() -> arkouda.dtypes.numpy_scalars

      Return the maximum value of the array.



   .. py:property:: max_bits


   .. py:method:: maxk(k: arkouda.dtypes.int_scalars) -> pdarray

      Compute the maximum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:method:: mean() -> numpy.float64

      Return the mean of the array.



   .. py:method:: min() -> arkouda.dtypes.numpy_scalars

      Return the minimum value of the array.



   .. py:method:: mink(k: arkouda.dtypes.int_scalars) -> pdarray

      Compute the minimum "k" values.

      :param k: The desired count of maximum values to be returned by the output.
      :type k: int_scalars

      :returns: The maximum `k` values from pda
      :rtype: pdarray, int

      :raises TypeError: Raised if pda is not a pdarray



   .. py:property:: nbytes
      The size of the pdarray in bytes.

      :returns: The size of the pdarray in bytes.
      :rtype: int


   .. py:attribute:: objType
      :value: 'pdarray'



   .. py:method:: opeq(other, op)


   .. py:method:: parity() -> pdarray

      Find the parity (XOR of all bits) in each element. See `ak.parity`.



   .. py:method:: popcount() -> pdarray

      Find the population (number of bits set) in each element. See `ak.popcount`.



   .. py:method:: pretty_print_info() -> None

      Prints information about all components of self in a human readable format

      :param None:

      :rtype: None



   .. py:method:: prod() -> numpy.float64

      Return the product of all elements in the array. Return value is
      always a np.float64 or np.int64.



   .. py:method:: register(user_defined_name: str) -> pdarray

      Register this pdarray with a user defined name in the arkouda server
      so it can be attached to later using pdarray.attach()
      This is an in-place operation, registering a pdarray more than once will
      update the name in the registry and remove the previously registered name.
      A name can only be registered to one pdarray at a time.

      :param user_defined_name: user defined name array is to be registered under
      :type user_defined_name: str

      :returns: The same pdarray which is now registered with the arkouda server and has an updated name.
                This is an in-place modification, the original is returned to support a
                fluid programming style.
                Please note you cannot register two different pdarrays with the same name.
      :rtype: pdarray

      :raises TypeError: Raised if user_defined_name is not a str
      :raises RegistrationError: If the server was unable to register the pdarray with the user_defined_name
          If the user is attempting to register more than one pdarray with the same name,
          the former should be unregistered first to free up the registration name.

      .. seealso:: :obj:`attach`, :obj:`unregister`, :obj:`is_registered`, :obj:`list_registry`, :obj:`unregister_pdarray_by_name`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion
      until they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: reshape(*shape, order='row_major')

      Gives a new shape to an array without changing its data.

      :param shape: The new shape should be compatible with the original shape.
      :type shape: int, tuple of ints, or pdarray
      :param order: Read the elements of the pdarray in this index order
                    By default, read the elements in row_major or C-like order where the last index
                    changes the fastest
                    If 'column_major' or 'F', read the elements in column_major or Fortran-like order where the
                    first index changes the fastest
      :type order: str {'row_major' | 'C' | 'column_major' | 'F'}

      :returns: An arrayview object with the data from the array but with the new shape
      :rtype: ArrayView



   .. py:method:: rotl(other) -> pdarray

      Rotate bits left by <other>.



   .. py:method:: rotr(other) -> pdarray

      Rotate bits right by <other>.



   .. py:method:: save(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None, file_format: str = 'HDF5', file_type: str = 'distribute') -> str

      DEPRECATED
      Save the pdarray to HDF5 or Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. HDF5 support single files, in which case the file name will
      only be that provided. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)
      :param file_format: By default, saved files will be written to the HDF5 file format. If
                          'Parquet', the files will be written to the Parquet file format. This
                          is case insensitive.
      :type file_format: str {'HDF5', 'Parquet'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray
      :raises ValueError: Raised if there is an error in parsing the prefix path pointing to
          file write location or if the mode parameter is neither truncate
          nor append
      :raises TypeError: Raised if any one of the prefix_path, dataset, or mode parameters
          is not a string

      .. seealso:: :obj:`save_all`, :obj:`load`, :obj:`read`, :obj:`to_parquet`, :obj:`to_hdf`

      .. rubric:: Notes

      The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales``. If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      Previously all files saved in Parquet format were saved with a ``.parquet`` file extension.
      This will require you to use load as if you saved the file with the extension. Try this if
      an older file is not being found.
      Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.save('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.save('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving with an extension (Parquet)
      >>> a.save('path/prefix.parquet', dataset='array', file_format='Parquet')
      Saves the array in numLocales Parquet files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:method:: slice_bits(low, high) -> pdarray

      Returns a pdarray containing only bits from low to high of self.

      This is zero indexed and inclusive on both ends, so slicing the bottom 64 bits is
      pda.slice_bits(0, 63)

      :param low: The lowest bit included in the slice (inclusive)
                  zero indexed, so the first bit is 0
      :type low: int
      :param high: The highest bit included in the slice (inclusive)
      :type high: int

      :returns: A new pdarray containing the bits of self from low to high
      :rtype: pdarray

      :raises RuntimeError: Raised if there is a server-side error thrown

      .. rubric:: Examples

      >>> p = ak.array([2**65 + (2**64 - 1)])
      >>> bin(p[0])
      '0b101111111111111111111111111111111111111111111111111111111111111111'

      >>> bin(p.slice_bits(64, 65)[0])
      '0b10'



   .. py:method:: std(ddof: arkouda.dtypes.int_scalars = 0) -> numpy.float64

      Compute the standard deviation. See ``arkouda.std`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating std
      :type ddof: int_scalars

      :returns: The scalar standard deviation of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises RuntimeError: Raised if there's a server-side error thrown



   .. py:method:: sum() -> arkouda.dtypes.numeric_and_bool_scalars

      Return the sum of all elements in the array.



   .. py:method:: to_csv(prefix_path: str, dataset: str = 'array', col_delim: str = ',', overwrite: bool = False)

              Write pdarray to CSV file(s). File will contain a single column with the pdarray data.
              All CSV Files written by Arkouda include a header denoting data types of the columns.

              Parameters
              -----------
              prefix_path: str
                  The filename prefix to be used for saving files. Files will have _LOCALE#### appended
                  when they are written to disk.
              dataset: str
                  Column name to save the pdarray under. Defaults to "array".
              col_delim: str
                  Defaults to ",". Value to be used to separate columns within the file.
                  Please be sure that the value used DOES NOT appear in your dataset.
              overwrite: bool
                  Defaults to False. If True, any existing files matching your provided prefix_path will
                  be overwritten. If False, an error will be returned if existing files are found.

              Returns
              --------
              str reponse message

              Raises
              ------
              ValueError
                  Raised if all datasets are not present in all parquet files or if one or
                  more of the specified files do not exist
              RuntimeError
                  Raised if one or more of the specified files cannot be opened.
                  If `allow_errors` is true this may be raised if no values are returned
                  from the server.
              TypeError
                  Raised if we receive an unknown arkouda_type returned from the server

              Notes
              ------
              - CSV format is not currently supported by load/load_all operations
              - The column delimiter is expected to be the same for column names and data
              - Be sure that column delimiters are not found within your data.
              - All CSV files must delimit rows using newline (`
      `) at this time.




   .. py:method:: to_cuda()

      Convert the array to a Numba DeviceND array, transferring array data from the
      arkouda server to Python via ndarray. If the array exceeds a builtin size limit,
      a RuntimeError is raised.

      :returns: A Numba ndarray with the same attributes and data as the pdarray; on GPU
      :rtype: numba.DeviceNDArray

      :raises ImportError: Raised if CUDA is not available
      :raises ModuleNotFoundError: Raised if Numba is either not installed or not enabled
      :raises RuntimeError: Raised if there is a server-side error thrown in the course of retrieving
          the pdarray.

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_cuda()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_cuda())
      numpy.devicendarray



   .. py:method:: to_hdf(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', file_type: str = 'distribute') -> str

      Save the pdarray to HDF5.
      The object can be saved to a collection of files or single file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param file_type: Default: "distribute"
                        When set to single, dataset is written to a single file.
                        When distribute, dataset is written on a file per locale.
                        This is only supported by HDF5 files and will have no impact of Parquet Files.
      :type file_type: str ("single" | "distribute")

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`. Otherwise,
      the file name will be `prefix_path`.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_hdf('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_hdf('path/prefix.h5', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.h5`` where #### is replaced by each locale number
      >>> # Saving to a single file
      >>> a.to_hdf('path/prefix.hdf5', dataset='array', file_type='single')
      Saves the array in to single hdf5 file on the root node.
      ``cwd/path/name_prefix.hdf5``



   .. py:method:: to_list() -> List

      Convert the array to a list, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A list with the same data as the pdarray
      :rtype: list

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`to_ndarray`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_list()
      [0, 1, 2, 3, 4]

      >>> type(a.to_list())
      list



   .. py:method:: to_ndarray() -> numpy.ndarray

      Convert the array to a np.ndarray, transferring array data from the
      Arkouda server to client-side Python. Note: if the pdarray size exceeds
      client.maxTransferBytes, a RuntimeError is raised.

      :returns: A numpy ndarray with the same attributes and data as the pdarray
      :rtype: np.ndarray

      :raises RuntimeError: Raised if there is a server-side error thrown, if the pdarray size
          exceeds the built-in client.maxTransferBytes size limit, or if the bytes
          received does not match expected number of bytes

      .. rubric:: Notes

      The number of bytes in the array cannot exceed ``client.maxTransferBytes``,
      otherwise a ``RuntimeError`` will be raised. This is to protect the user
      from overflowing the memory of the system on which the Python client
      is running, under the assumption that the server is running on a
      distributed system with much more memory than the client. The user
      may override this limit by setting client.maxTransferBytes to a larger
      value, but proceed with caution.

      .. seealso:: :obj:`array`, :obj:`to_list`

      .. rubric:: Examples

      >>> a = ak.arange(0, 5, 1)
      >>> a.to_ndarray()
      array([0, 1, 2, 3, 4])

      >>> type(a.to_ndarray())
      numpy.ndarray



   .. py:method:: to_parquet(prefix_path: str, dataset: str = 'array', mode: str = 'truncate', compression: Optional[str] = None) -> str

      Save the pdarray to Parquet. The result is a collection of files,
      one file per locale of the arkouda server, where each filename starts
      with prefix_path. Each locale saves its chunk of the array to its
      corresponding file.
      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files (must not already exist)
      :type dataset: str
      :param mode: By default, truncate (overwrite) output files, if they exist.
                   If 'append', attempt to create new dataset in existing files.
      :type mode: str {'truncate' | 'append'}
      :param compression: (None | "snappy" | "gzip" | "brotli" | "zstd" | "lz4")
                          Sets the compression type used with Parquet files
      :type compression: str (Optional)

      :rtype: string message indicating result of save operation

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - The prefix_path must be visible to the arkouda server and the user must
      have write permission.
      - Output files have names of the form ``<prefix_path>_LOCALE<i>``, where ``<i>``
      ranges from 0 to ``numLocales`` for `file_type='distribute'`.
      - 'append' write mode is supported, but is not efficient.
      - If any of the output files already exist and
      the mode is 'truncate', they will be overwritten. If the mode is 'append'
      and the number of output files is less than the number of locales or a
      dataset with the same name already exists, a ``RuntimeError`` will result.
      - Any file extension can be used.The file I/O does not rely on the extension to
      determine the file format.

      .. rubric:: Examples

      >>> a = ak.arange(25)
      >>> # Saving without an extension
      >>> a.to_parquet('path/prefix', dataset='array')
      Saves the array to numLocales HDF5 files with the name ``cwd/path/name_prefix_LOCALE####``
      >>> # Saving with an extension (HDF5)
      >>> a.to_parqet('path/prefix.parquet', dataset='array')
      Saves the array to numLocales HDF5 files with the name
      ``cwd/path/name_prefix_LOCALE####.parquet`` where #### is replaced by each locale number



   .. py:method:: transfer(hostname: str, port: arkouda.dtypes.int_scalars)

      Sends a pdarray to a different Arkouda server

      :param hostname: The hostname where the Arkouda server intended to
                       receive the pdarray is running.
      :type hostname: str
      :param port: The port to send the array over. This needs to be an
                   open port (i.e., not one that the Arkouda server is
                   running on). This will open up `numLocales` ports,
                   each of which in succession, so will use ports of the
                   range {port..(port+numLocales)} (e.g., running an
                   Arkouda server of 4 nodes, port 1234 is passed as
                   `port`, Arkouda will use ports 1234, 1235, 1236,
                   and 1237 to send the array data).
                   This port much match the port passed to the call to
                   `ak.receive_array()`.
      :type port: int_scalars

      :rtype: A message indicating a complete transfer

      :raises ValueError: Raised if the op is not within the pdarray.BinOps set
      :raises TypeError: Raised if other is not a pdarray or the pdarray.dtype is not
          a supported dtype



   .. py:method:: unregister() -> None

      Unregister a pdarray in the arkouda server which was previously
      registered using register() and/or attahced to using attach()


      :rtype: None

      :raises RuntimeError: Raised if the server could not find the internal name/symbol to remove

      .. seealso:: :obj:`register`, :obj:`unregister`, :obj:`is_registered`, :obj:`unregister_pdarray_by_name`, :obj:`list_registry`

      .. rubric:: Notes

      Registered names/pdarrays in the server are immune to deletion until
      they are unregistered.

      .. rubric:: Examples

      >>> a = zeros(100)
      >>> a.register("my_zeros")
      >>> # potentially disconnect from server and reconnect to server
      >>> b = ak.pdarray.attach("my_zeros")
      >>> # ...other work...
      >>> b.unregister()



   .. py:method:: update_hdf(prefix_path: str, dataset: str = 'array', repack: bool = True)

      Overwrite the dataset with the name provided with this pdarray. If
      the dataset does not exist it is added

      :param prefix_path: Directory and filename prefix that all output files share
      :type prefix_path: str
      :param dataset: Name of the dataset to create in files
      :type dataset: str
      :param repack: Default: True
                     HDF5 does not release memory on delete. When True, the inaccessible
                     data (that was overwritten) is removed. When False, the data remains, but is
                     inaccessible. Setting to false will yield better performance, but will cause
                     file sizes to expand.
      :type repack: bool

      :rtype: str - success message if successful

      :raises RuntimeError: Raised if a server-side error is thrown saving the pdarray

      .. rubric:: Notes

      - If file does not contain File_Format attribute to indicate how it was saved,
        the file name is checked for _LOCALE#### to determine if it is distributed.
      - If the dataset provided does not exist, it will be added



   .. py:method:: value_counts()

      Count the occurrences of the unique values of self.

      :returns: * **unique_values** (*pdarray*) -- The unique values, sorted in ascending order
                * **counts** (*pdarray, int64*) -- The number of times the corresponding unique value occurs

      .. rubric:: Examples

      >>> ak.array([2, 0, 2, 4, 0, 0]).value_counts()
      (array([0, 2, 4]), array([3, 2, 1]))



   .. py:method:: var(ddof: arkouda.dtypes.int_scalars = 0) -> numpy.float64

      Compute the variance. See ``arkouda.var`` for details.

      :param ddof: "Delta Degrees of Freedom" used in calculating var
      :type ddof: int_scalars

      :returns: The scalar variance of the array
      :rtype: np.float64

      :raises TypeError: Raised if pda is not a pdarray instance
      :raises ValueError: Raised if the ddof >= pdarray size
      :raises RuntimeError: Raised if there's a server-side error thrown



.. py:function:: right_align(left, right)

   Map two arrays of sparse values to the 0-up index set implied by the right array,
   discarding values from left that do not appear in right.

   :param left: Left-hand identifiers
   :type left: pdarray or a sequence of pdarrays
   :param right: Right-hand identifiers that define the index
   :type right: pdarray or a sequence of pdarrays

   :returns: * **keep** (*pdarray, bool*) -- Logical index of left-hand values that survived
             * **aligned** (*(pdarray, pdarray)*) -- Left and right arrays with values replaced by 0-up indices


.. py:function:: search_intervals(vals, intervals, tiebreak=None, hierarchical=True)

   Given an array of query vals and non-overlapping, closed intervals, return
   the index of the best (see tiebreak) interval containing each query value,
   or -1 if not present in any interval.

   :param vals: Values to search for in intervals. If multiple arrays, each "row" is an item.
   :type vals: (sequence of) pdarray(int, uint, float)
   :param intervals: Non-overlapping, half-open intervals, as a tuple of
                     (lower_bounds_inclusive, upper_bounds_exclusive)
                     Must have same dtype(s) as vals.
   :type intervals: 2-tuple of (sequences of) pdarrays
   :param tiebreak: When a value is present in more than one interval, the interval with the
                    lowest tiebreak value will be chosen. If no tiebreak is given, the
                    first containing interval will be chosen.
   :type tiebreak: (optional) pdarray, numeric
   :param hierarchical: When True, sequences of pdarrays will be treated as components specifying
                        a single dimension (i.e. hierarchical)
                        When False, sequences of pdarrays will be specifying multi-dimensional intervals
   :type hierarchical: boolean

   :returns: **idx** -- Index of interval containing each query value, or -1 if not found
   :rtype: pdarray(int64)

   .. rubric:: Notes

   The return idx satisfies the following condition:
       present = idx > -1
       ((intervals[0][idx[present]] <= vals[present]) &
        (intervals[1][idx[present]] >= vals[present])).all()

   .. rubric:: Examples

   >>> starts = (ak.array([0, 5]), ak.array([0, 11]))
   >>> ends = (ak.array([5, 9]), ak.array([10, 20]))
   >>> vals = (ak.array([0, 0, 2, 5, 5, 6, 6, 9]), ak.array([0, 20, 1, 5, 15, 0, 12, 30]))
   >>> ak.search_intervals(vals, (starts, ends), hierarchical=False)
   array([0 -1 0 0 1 -1 1 -1])
   >>> ak.search_intervals(vals, (starts, ends))
   array([0 0 0 0 1 1 1 -1])
   >>> bi_starts = ak.bigint_from_uint_arrays([ak.cast(a, ak.uint64) for a in starts])
   >>> bi_ends = ak.bigint_from_uint_arrays([ak.cast(a, ak.uint64) for a in ends])
   >>> bi_vals = ak.bigint_from_uint_arrays([ak.cast(a, ak.uint64) for a in vals])
   >>> bi_starts, bi_ends, bi_vals
   (array(["0" "92233720368547758091"]),
   array(["92233720368547758090" "166020696663385964564"]),
   array(["0" "20" "36893488147419103233" "92233720368547758085" "92233720368547758095"
   "110680464442257309696" "110680464442257309708" "166020696663385964574"]))
   >>> ak.search_intervals(bi_vals, (bi_starts, bi_ends))
   array([0 0 0 0 1 1 1 -1])


.. py:function:: unique(pda: groupable, return_groups: bool = False, assume_sorted: bool = False, return_indices: bool = False) -> Union[groupable, Tuple[groupable, pdarray, pdarray, int]]

   Find the unique elements of an array.

   Returns the unique elements of an array, sorted if the values are integers.
   There is an optional output in addition to the unique elements: the number
   of times each unique value comes up in the input array.

   :param pda: Input array.
   :type pda: (list of) pdarray, Strings, or Categorical
   :param return_groups: If True, also return grouping information for the array.
   :type return_groups: bool, optional
   :param return_indices: Only applicable if return_groups is True.
                          If True, return unique key indices along with other groups
   :type return_indices: bool, optional
   :param assume_sorted: If True, assume pda is sorted and skip sorting step
   :type assume_sorted: bool, optional

   :returns: * **unique** (*(list of) pdarray, Strings, or Categorical*) -- The unique values. If input dtype is int64, return values will be sorted.
             * **permutation** (*pdarray, optional*) -- Permutation that groups equivalent values together (only when return_groups=True)
             * **segments** (*pdarray, optional*) -- The offset of each group in the permuted array (only when return_groups=True)

   :raises TypeError: Raised if pda is not a pdarray or Strings object
   :raises RuntimeError: Raised if the pdarray or Strings dtype is unsupported

   .. rubric:: Notes

   For integer arrays, this function checks to see whether `pda` is sorted
   and, if so, whether it is already unique. This step can save considerable
   computation. Otherwise, this function will sort `pda`.

   .. rubric:: Examples

   >>> A = ak.array([3, 2, 1, 1, 2, 3])
   >>> ak.unique(A)
   array([1, 2, 3])


.. py:function:: unsqueeze(p)

.. py:function:: where(condition: arkouda.pdarrayclass.pdarray, A: Union[str, arkouda.dtypes.numeric_scalars, arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical], B: Union[str, arkouda.dtypes.numeric_scalars, arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical]) -> Union[arkouda.pdarrayclass.pdarray, arkouda.strings.Strings, arkouda.categorical.Categorical]

   Returns an array with elements chosen from A and B based upon a
   conditioning array. As is the case with numpy.where, the return array
   consists of values from the first array (A) where the conditioning array
   elements are True and from the second array (B) where the conditioning
   array elements are False.

   :param condition: Used to choose values from A or B
   :type condition: pdarray
   :param A: Value(s) used when condition is True
   :type A: Union[numeric_scalars, str, pdarray, Strings, Categorical]
   :param B: Value(s) used when condition is False
   :type B: Union[numeric_scalars, str, pdarray, Strings, Categorical]

   :returns: Values chosen from A where the condition is True and B where
             the condition is False
   :rtype: pdarray

   :raises TypeError: Raised if the condition object is not a pdarray, if A or B is not
       an int, np.int64, float, np.float64, pdarray, str, Strings, Categorical
       if pdarray dtypes are not supported or do not match, or multiple
       condition clauses (see Notes section) are applied
   :raises ValueError: Raised if the shapes of the condition, A, and B pdarrays are unequal

   .. rubric:: Examples

   >>> a1 = ak.arange(1,10)
   >>> a2 = ak.ones(9, dtype=np.int64)
   >>> cond = a1 < 5
   >>> ak.where(cond,a1,a2)
   array([1, 2, 3, 4, 1, 1, 1, 1, 1])

   >>> a1 = ak.arange(1,10)
   >>> a2 = ak.ones(9, dtype=np.int64)
   >>> cond = a1 == 5
   >>> ak.where(cond,a1,a2)
   array([1, 1, 1, 1, 5, 1, 1, 1, 1])

   >>> a1 = ak.arange(1,10)
   >>> a2 = 10
   >>> cond = a1 < 5
   >>> ak.where(cond,a1,a2)
   array([1, 2, 3, 4, 10, 10, 10, 10, 10])

   >>> s1 = ak.array([f'str {i}' for i in range(10)])
   >>> s2 = 'str 21'
   >>> cond = (ak.arange(10) % 2 == 0)
   >>> ak.where(cond,s1,s2)
   array(['str 0', 'str 21', 'str 2', 'str 21', 'str 4', 'str 21', 'str 6', 'str 21', 'str 8','str 21'])

   >>> c1 = ak.Categorical(ak.array([f'str {i}' for i in range(10)]))
   >>> c2 = ak.Categorical(ak.array([f'str {i}' for i in range(9, -1, -1)]))
   >>> cond = (ak.arange(10) % 2 == 0)
   >>> ak.where(cond,c1,c2)
   array(['str 0', 'str 8', 'str 2', 'str 6', 'str 4', 'str 4', 'str 6', 'str 2', 'str 8', 'str 0'])

   .. rubric:: Notes

   A and B must have the same dtype and only one conditional clause
   is supported e.g., n < 5, n > 1, which is supported in numpy
   is not currently supported in Arkouda


.. py:function:: zero_up(vals)

   Map an array of sparse values to 0-up indices.

   :param vals: Array to map to dense index
   :type vals: pdarray

   :returns: **aligned** -- Array with values replaced by 0-up indices
   :rtype: pdarray


.. py:function:: zeros(size: Union[arkouda.dtypes.int_scalars, str], dtype: Union[numpy.dtype, type, str, arkouda.dtypes.BigInt] = float64, max_bits: Optional[int] = None) -> arkouda.pdarrayclass.pdarray

   Create a pdarray filled with zeros.

   :param size: Size of the array (only rank-1 arrays supported)
   :type size: int_scalars
   :param dtype: Type of resulting array, default float64
   :type dtype: all_scalars
   :param max_bits: Specifies the maximum number of bits; only used for bigint pdarrays
   :type max_bits: int

   :returns: Zeros of the requested size and dtype
   :rtype: pdarray

   :raises TypeError: Raised if the supplied dtype is not supported or if the size
       parameter is neither an int nor a str that is parseable to an int.

   .. seealso:: :obj:`ones`, :obj:`zeros_like`

   .. rubric:: Examples

   >>> ak.zeros(5, dtype=ak.int64)
   array([0, 0, 0, 0, 0])

   >>> ak.zeros(5, dtype=ak.float64)
   array([0, 0, 0, 0, 0])

   >>> ak.zeros(5, dtype=ak.bool)
   array([False, False, False, False, False])


